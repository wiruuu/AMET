\documentclass[a4paper, 12pt,oneside,openany]{book}
\usepackage{import}
\subimport{../MacTex/condiments/}{preamble.tex}
\subimport{../MacTex/condiments/}{letterfonts.tex}
\subimport{../MacTex/condiments/}{macros.tex}
\usepackage{tikz-cd} 
\usepackage[all]{xy}
\usepackage[margin=0.7in, foot=.25in, bottom=1.5in]{geometry}
\usepackage{fancyhdr}
\usepackage{graphicx} 
\usepackage{titlesec}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage[Bjornstrup]{fncychap}
\usetikzlibrary{calc}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headsep}{0.7in}

\definecolor{line}{HTML}{292524}
\definecolor{fillh}{HTML}{f5f5f4}
\titleformat*{\section}{\Large\bfseries\sffamily}
\titleformat*{\subsection}{\large\bfseries\sffamily}
\renewcommand{\headrulewidth}{0pt}
\newcommand \hdheight{1in}

\pagestyle{empty}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\fancyhf{}
\fancyhead[E]{%
\begin{tikzpicture}[overlay, remember picture]%
    \fill[fillh] (current page.north west) rectangle ($(current page.north east)+(0,-\hdheight)$);
    \draw[line] ([yshift=-\hdheight]current page.north west) -- ([yshift=-\hdheight]current page.north east);
    \node[anchor=south west, text width=1cm] (evenpagenum) at ($(current page.north west)+(.5\hdheight,-\hdheight)$) {\thepage};
    \node[anchor=south west, text width=1.5cm, text=darkgray, font=\fontsize{2cm}{5.5cm}\selectfont] (chapter) at (evenpagenum.south east) {\thechapter};
    \node[anchor=south west] at (chapter.south east) {\bfseries\leftmark};
\end{tikzpicture}
}%
\fancyhead[O]{%
\begin{tikzpicture}[overlay, remember picture]%
    \fill[fillh] (current page.north west) rectangle ($(current page.north east)+(0,-\hdheight)$);
    \draw[line] ([yshift=-\hdheight]current page.north west) -- ([yshift=-\hdheight]current page.north east);
    \node[align=right, anchor=south east, text width=1cm] (evenpagenum) at ($(current page.north east)+(-.5\hdheight,-\hdheight)$) {\thepage};
    \node[align=right, anchor=south east, text width=1.5cm, text=darkgray, font=\fontsize{2cm}{5.5cm}\selectfont] (chapter) at (evenpagenum.south west) {\thechapter};
    \node[align=right, anchor=south east] at (chapter.south west) {\bfseries\leftmark};
\end{tikzpicture}
}
\fancyfoot[CE]{}
\fancyfoot[CO]{}
\setlength{\headheight}{22pt}
\setlength{\textheight}{700pt}
  % a nice math font
%\usepackage{newpxtext,newpxmath}   % a nice text font

\begin{document}

\begin{titlepage}
\begin{center}
\includegraphics[width=12cm]{amet.png}

   \end{center}
\end{titlepage}
\dominitoc[n]
\sffamily

\tableofcontents

\clearpage
\pagestyle{fancy}
\chapter{Opening remarks}

This set of notes is a more accessible interpretation of Frank Witte's UCL ECON0006/0010 lecture notes, used for the 2024-2025 academic year.

I have made these notes as a companion guide to the first-year mathematical economics course. This is intended to provide more student-friendly insights to the course, and is meant to assuage the worries and troubles that tend to arise within students when reading the official lecture notes. 

In many cases, the mathematical economics course introduces topics not commonly seen in mathematical economics textbooks, or divulge novel presentations of existing content. Although this set of notes has fortunately received pleasant feedback from quite a substantial number of first-year students, this is not a replacement for the official set of notes.

Thank you to the 100+ of you from economics to statistics to philosophy to geography to mathematics majors who have sent me messages of gratitude, thanked me in person, or have generously pointed out errata or points of clarification.

- Ethan :)

\chapter{Week 1}

\section{Types of equations}

There are many types of equations. These include linear equations, non-linear equations, parametric equations and systems of linear equations.

This chapter is review.

\begin{enumerate}
	\item \textbf{Linear equations} are our best friend. These are equations in the form $$f(x) = a+bx.$$ Note that $f(x)=0$ implies $a+bx=0$, which means its roots are $x=-\frac{a}{b}$. Simple.

	\item For \textbf{non-linear equations} like $x^2+3xy=4$, we can use algebraic manipulation or software like \emph{Mathematica} to make $x$ the subject or $y$ the subject. In many cases there are multiple solutions to non-linear equations. 

	\item \textbf{Parametric equations}. This is when we represent two unknowns with one equation, so we will have a family of solutions. 
	
	Consider the equation $3x-4y=2$. This can be arranged to become $y = \frac{3x-2}{4}.$ Thus, $\{x(t), y(t)\} = \{t, \frac{3t-2}{4}\}$, where $t$ is a parameter. Or, we can write $$\begin{pmatrix} x(t) \\ y(t) \end{pmatrix} = t\begin{pmatrix} 1 \\ \frac{3}{4} \end{pmatrix} + \begin{pmatrix} 0 \\ -\frac{1}{2} \end{pmatrix}.$$
	
	In fact, one can make sensible parameterisations when the number of equations is less than the number of unknowns.
	
	\item \tbf{Systems of linear equations}. This is when we have multiple linear equations in multiple variables. We will return to systems of linear equations when we study matrices albeit us knowing how to solve said systems via elimination or substitution. 

\end{enumerate}

\ex{Examples of these equations}{

}

\section{Single-variable calculus}

We looked at the derivative of a single-variable function. If we have a function $y=f(x)$, then we have the \emph{limit definition of the derivative:} $$\frac{dy}{dx} = \lim\limits_{\Delta x \to 0} \frac{f(x+\Delta x) - f(x)}{\Delta x}.$$

The rules of differentiation include the sum rule, the product rule, the quotient rule, the chain rule etc.

We also mentioned optimisation, which is very instrumental to our study of economics. Consider the utility function $$u(x) = x-\frac{2}{3}x^2.$$ We call any local optima the \tbf{first-order condition (foc)} - i.e. $u'(x)=0$. 

Recall from secondary school that we can classify optima. These are what we call the \tbf{second-order conditions (soc)}. If $\frac{du}{dx}=0$, we have that 

$$\frac{d^2u}{dx^2}: \  \begin{cases}
	<0 \ \text{implies that the optimum is a maximum} \\
	>0 \ \text{implies that the optimum is a minimum} \\
	=0 \ \text{implies that the optimum is undecided}
\end{cases}$$

We also reviewed integration. $\int\limits_a^b f(x) dx$ is the area under the curve $f(x)$ in the range $x \in [a, b]$.

\subsection{Linear and quadratic approximations}

For those who only took A Level Maths, this might be new.

Those who took A Level Further Maths or IB Maths AAHL know that we can use a Maclaurin series to approximate a function $f(x)$ at $0$. In fact, we can use polynomial functions to approximate a function at any point $x_0$. This is called a \textbf{Taylor series}. It uncovers the polynomial function that approximates $f(x)$.

\includegraphics[scale=0.5]{taylor.png} 

Consider the function $f(x)=\sin x$ and its series of approximations about the point $x_0=0$, as shown above. 

We can use a linear function $y=x$ to approximate it (consider the curve labelled $n=0$) which does a decently good job for values of $x$ near 0. We can also use a cubic function $x-\frac{x^3}{6}$ (labelled $n=1$) to approximate function decently well for a larger range of $x$. The idea is that as we introduce terms of larger degree, our polynomial will be able to decently approximate \textbf{more and more of} $\sin x$.

How do we find this polynomial?

\defn{Taylor series}{
	To approximate $f(x)$ at a point $x=x_0$, we consider the \emph{Taylor polynomial} $$f(x) = f(x_0) + \frac{f'(x_0)}{1!}(x-x_0) + \frac{f''(x_0)}{2!}(x-x_0)^2 + \dots = \sum\limits_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!}(x-x_0)$$
}

For the purposes of ECON0006/0010, we consider only the \textbf{linear and quadratic approximations}. These are $$f(x) \approx f(x_0) + (x-x_0) f'(x_0)$$ and $$f(x) \approx f(x_0) + (x-x_0) f'(x_0) + \frac{1}{2} (x-x_0)^2 f''(x_0).$$

\coro{}{ The linear and quadratic approximations will come up again and again in the course in multivariable form. They also appear when there are multiple functions to be approximated simultaneously. 

}

\ex{Taylor series of $e^x$}{
	Let's approximate $f(x)=e^x$ at $x=0$. Note that we have $f(x) = f'(x) = f^{(n)}(x)=1$, so our Taylor polynomial is $$f(x) = 1+x+\frac{x^2}{2} + \frac{x^3}{6}+\dots $$
	
	Our linear and quadratic approximations are $e^x \approx 1+x$ and $e^x \approx 1+x+\frac{x^2}{2}$ respectively. 
	
	The Maclaurin series for $f(x)=e^x$ is $$\sum\limits_{n=0}^\infty \frac{x^n}{n!}.$$
}



\chapter{Week 2}

In this week, we explore vectors and matrices in great depth. We will gain basic intuition regarding working in multiple variables, which will serve us for future explorations of calculus in multiple variables.

\section{Introduction to vectors}

\defn{Vector in two dimensions}{
	A vector is a quantity that has both size and direction. Informally, they are "lists of numbers". Scalars are quantities that only have size.
	
	In two dimensions, we write vectors as $$\vec{a} = \begin{pmatrix} a_1 \\a_2  \end{pmatrix}$$ This is a pair of real numbers $\{a_1, a_2 \}$, where $a_1, a_2 \in \bbR$. 
	
	When we have a list of two real numbers, we write that $\begin{pmatrix} a_1 \\a_2  \end{pmatrix} \in \bbR \times \bbR = \bbR^2.$
}

\ex{Two-dimensional vector}{
	The vector $\begin{pmatrix} 3 \\ 5 \end{pmatrix}$ can be viewed as an arrow starting from $(0,0)$ and ending at $(3, 5)$. In two dimensions, we view the top entry as the shift in the horizontal component, and the bottom entry as the shift in the vertical component. 
}


We can add vectors $\vec{a}$ and $\vec{b}$ by doing $$\vec{a} + \vec{b} = \begin{pmatrix} a_1 \\a_2  \end{pmatrix} + \begin{pmatrix} b_1 \\b_2  \end{pmatrix} = \begin{pmatrix} a_1+b_1 \\a_2+b_2  \end{pmatrix}$$ and multiply vectors by scalars $$k\vec{a}  = k\begin{pmatrix} a_1 \\a_2  \end{pmatrix}  = \begin{pmatrix} ka_1 \\ka_2  \end{pmatrix}.$$

Vectors have a geometric interpretation. The vector $\begin{pmatrix} a_1 \\a_2  \end{pmatrix}$ suggests a line going from $(0, 0)$ in the direction of $(a_1, a_2)$ until the point $(a_1, a_2)$.

\defn{Vector magnitude}{The length of a vector is known as its \textbf{magnitude}, and is written and defined as $$|\vec{x}| = \sqrt{x_1^2+x_2^2+\dots+x_n^2}.$$

}

In $n$ dimensions, we write a vector as $\vec{a} = \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}$. 

\subsection{Vector basis}

\coro{}{\textbf{Key idea:} Basis vectors are \emph{building blocks} for any $n$-dimensional vector. If you add certain quantities of any of the basis vectors, you can construct any vector.}

Let's work in 2 dimensions. We can construct any vector $\vec{a}=\begin{pmatrix} a_1 \\a_2  \end{pmatrix}$ by adding certain quantities of $\begin{pmatrix} 1 \\0  \end{pmatrix}$ and$ \begin{pmatrix} 0 \\1  \end{pmatrix}$. Denote $\hat{e_1}=\begin{pmatrix} 1 \\0  \end{pmatrix}$ and $\hat{e_2}=\begin{pmatrix} 0 \\1  \end{pmatrix}$. 

It is obvious that $\begin{pmatrix} a_1 \\a_2  \end{pmatrix} = a_1 \hat{e_1} + a_2\hat{e_2}.$ We call $\{\hat{e_1}, \hat{e_2}\}$ the \emph{canonical basis}.

We can represent our vectors in bases apart from our canonical basis. Let \begin{align*} \hat{f_1} &= \hat{e_1} + \hat{e_2}\\ \hat{f_2} &= \hat{e_1} - \hat{e_2} .\end{align*} 

We can then write $\hat{e_1} = \frac{\hat{f_1}+\hat{f_2}}{2}$ and $\hat{e_2} = \frac{\hat{f_1}-\hat{f_2}}{2}$. 

Instead of writing $$\begin{pmatrix} a_1 \\a_2  \end{pmatrix} = a_1 \hat{e_1} + a_2\hat{e_2},$$ we can now write \begin{align*} \begin{pmatrix} a_1 \\a_2  \end{pmatrix} &= a_1 \left(\frac{\hat{f_1}+\hat{f_2}}{2} \right) + a_2 \left( \frac{\vec{f_1}-\vec{f_2}}{2} \right)\\ &= \left(\frac{a_1+a_2}{2}\right) \hat{f_1} + \left(\frac{a_1-a_2}{2}\right) \hat{f_2}. \end{align*} This depicts the change of basis from $\{\hat{e_1}, \hat{e_2} \}$ to $\{\hat{f_1}, \hat{f_2} \}$

\ex{One vector, two ways}{
	We can write $$\begin{pmatrix} 15 \\ -5 \end{pmatrix} = 15 \begin{pmatrix} 1 \\ 0 \end{pmatrix} -5\begin{pmatrix} 0 \\ 1 \end{pmatrix}.$$ We can also write $$\begin{pmatrix} 15 \\ -5 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 1 \end{pmatrix} + 4\begin{pmatrix} 3 \\ -2 \end{pmatrix}.$$
	
	This is a change of basis from $\{1, 0\}, \{0, 1\}$ to $\{1, 1\}, \{3, -2\}$.
}

\defn{Linear forms in two dimensions}{
	Linear forms are functions of two variables in which each variable has degree one. Explicitly, we write $$f(x_1, x_2) = f_1x_1 + f_2x_2,$$ with $f_1, f_2 \in \bbR$. 
	
	This is represented by the map $f: \bbR^2 \to \bbR$. This mathematical notation means that the function has two inputs in $\bbR$ and spits out one output in $\bbR$. 
}

These functions satisfy the properties $f(x_1+y_1, x_2+y_2) = f(x_1, x_2) + f(y_1, y_2)$ and $f(kx_1, kx_2) = kf(x_1, x_2)$.

\coro{}{ We can truncate $f(x_1, x_2, \dots)$ to $f(\vec{x})$. To simplify notation, we can also truncate $f_1(\vec{x}), f_2(\vec{x}), \dots$ to $\vec{f}(\vec{x})$. We'll use a mixture of both as using $f(\vec{x})$ all the time can be very mind-boggling and conceptually challenging albeit concise. 

The conceptual challenges are especially apparent when the material is abstract.
}

$f_1x_1+f_2x_2$ is known as the \emph{dot product} of the vectors $\vec{f}=\begin{pmatrix} f_1 \\ f_2 \end{pmatrix}$ and $\vec{x} = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$. 

\defn{Dot product}{
	Let's explore the dot product in $n$ dimensions. We will discuss linear algebra in $n$ dimensions in Week 4. 
	
	Let $\vec{a} = \begin{pmatrix} a_1 & a_2 &\dots & a_n \end{pmatrix}$ and $\vec{b} = \begin{pmatrix} b_1 \\ b_2\\ \vdots \\ b_n \end{pmatrix}$. 
	
	Then, $\vec{a} \cdot \vec{b} = a_1b_1 +a_2b_2+ \dots + a_nb_n.$
	
	There is a \emph{geometric definition} of the dot product. Let the angle between $\vec{a}$ and $\vec{b}$ be $\theta$. Then, $$\vec{a} \cdot \vec{b} = |\vec{a}||\vec{b}| \cos(\theta).$$
}

Note that if the dot product between two vectors is 0, they are perpendicular (or orthogonal).

\ex{}{
	$\begin{pmatrix} 2\\3   \end{pmatrix} \cdot \begin{pmatrix} a \\4a  \end{pmatrix} = 2a+12a=14a$.
}

A vector with magnitude 1 is known as a \tbf{unit vector}. We can turn any vector $\vec{v}$ into a unit vector $\hat{v}$ by $\hat{v} = \frac{\vec{v}}{|\vec{v}|}.$

\tbf{Abuse of notation:} We don't differentiate between row vectors and column vectors, and write them as $\vec{x}$. They should be clear within their context.

\section{Introduction to matrices}

Let's visit our change of basis from $\{1, 0\}, \{0, 1\}$ to $\{1, 1\}, \{3, -2\}$. Suppose we didn't know what coefficients to add to $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ and $\begin{pmatrix} 3 \\ -2 \end{pmatrix}$ to get $\begin{pmatrix} 15 \\ -5 \end{pmatrix}.$ Write $$\begin{pmatrix} 15 \\ -5 \end{pmatrix} = a \begin{pmatrix} 1 \\ 1 \end{pmatrix} + b\begin{pmatrix} 3 \\ -2 \end{pmatrix}.$$

This is the same as \begin{align*} 15&=a+3b \\ -5&=a-2b,\end{align*} or \begin{align*} 15&=\begin{pmatrix} 1 &3 \end{pmatrix} \cdot\begin{pmatrix} a \\ b \end{pmatrix} \\ -5&=\begin{pmatrix} 1 & -2 \end{pmatrix} \cdot\begin{pmatrix} a \\ b \end{pmatrix},\end{align*} or \begin{equation}\begin{pmatrix} 15 \\ -5 \end{pmatrix} = \begin{pmatrix} 1 & 3 \\ 1 & -2 \end{pmatrix}\begin{pmatrix} a \\ b \end{pmatrix}. \end{equation}

\coro{}{\tbf{Key idea:} The matrix $\begin{pmatrix} 1 & 3 \\ 1 & -2 \end{pmatrix}$ describes the \emph{change of basis}. Instead of the canonical basis given by $\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix},$ we have the new basis given by $\begin{pmatrix} 1 & 3 \\ 1 & -2 \end{pmatrix}.$}

We saw that our solution was $\begin{pmatrix} a\\ b \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$. In equation (2.1), can we move our matrix over to the other side so we can evaluate $\begin{pmatrix} a \\ b \end{pmatrix}$ without explicitly solving the system of linear equations? We'll wait until week 4 when we introduce matrix inverses.

We also saw how systems of 2 linear equations in two unknowns could be written in terms of a matrix equation. Whenever we have \begin{align*} ax_1+bx_2=y_1 \\ dx_1+ex_2=y_2,\end{align*} we can write $$\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} y_1 \\ y_2 \end{pmatrix}.$$ In short, this is a matrix equation $$M \cdot \vec{x} = \vec{y}.$$

\defn{Entries of a matrix}{
	Let $M = \begin{pmatrix} 2 & 3& 5 \\ 0 & -4  & -2 \\ -3 & 1 & -4\end{pmatrix}$, and suppose we want to refer to the element $-2$ in row 2, column 3 of $M$. We would write that $$M_{23}=-2,$$ where this subscript notation puts rows before columns.
}

\subsection{Operations on matrices}

We add matrices, and multiply matrices by scalars in the way you would expect. It is a bit more complicated to multiply matrices by matrices and matrices by vectors. Note: $\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} a_{11}v_1+a_{12}v_2 \\ a_{21}v_1+a_{22}v_2. \end{pmatrix}$

$$\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \begin{pmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix} = \begin{pmatrix} a_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22} \\ a_{21}b_{11}+a_{22}b_{21} & a_{21}b_{12}+a_{22}b_{22} \end{pmatrix}.$$

In general, matrices are non-commutative. This means that given matrices $M_1, M_2$, it is generally the case that $M_1 \cdot M_2 \neq M_2 \cdot M_1$.

\defn{Commutator matrix}{
	Let $M_1$ and $M_2$ be two matrices. The commutator of $M_1$ and $M_2$, denoted $C(M_1, M_2)$ or $[M_1, M_2]$ in some texts, is $$C(M_1, M_2) = M_1M_2 - M_2M_1.$$ Simply speaking, it is a measure of how far $M_1$ and $M_2$ are from commuting. If $M_1$ and $M_2$ were commutative, then $C(M_1, M_2) = M_1M_2 - M_2M_1 = M_1M_2 - M_1M_2 =0.$
	
	The commutator is commonly found in abstract algebra and quantum physics. Used here, it makes for interesting problems in matrix algebra.
}



% A store sells commodities c1, c2, c3 in two its branches B1, B2

\chapter{Week 3}

This week focuses on functions of two variables. We apply our knowledge from single-variable calculus to multivariable calculus.

\section{Calculus of two variables}

Consider the Taylor series of a single-variable function about the point $x_0$. We have that $f(x) = f(x_0)+xf'(x_0)+\frac{1}{2}x^2 f''(x_0),$ or $f(x)=a+bx+\frac{1}{2}cx^2$. 

Let's try to emulate this quadratic approximation in two variables. Let $\vec{x} = \{x, y\}$. Instead of $x$, we want a sum of terms in both $x$ and $y$. We can do this by having $\begin{pmatrix} b_1 \\ b_2 \end{pmatrix} \cdot \begin{pmatrix} x & y \end{pmatrix} = b_1x+b_2y$. Instead of $x^2$, we need to find terms in $x^2$, $xy$, and $y^2$.

Note that we cannot perform squares of vectors so we have to find a workaround. We do this by considering the term $\vec{x} \cdot C \cdot \vec{x}$, where $C$ is a matrix. $C$ has to be a matrix - or else it is impossible for us to perform matrix and vector operations.

This is $$f(\vec{x}) = a + \vec{b} \cdot \vec{x} + \frac{1}{2} \vec{x} \cdot C \cdot \vec{x},$$ where $C$ is a matrix, and $\vec{b}, \vec{x}$ are vectors. Writing the two-variable version in full, we express this quadratic expansion as $$f(x, y) = a + \begin{pmatrix} b_1 & b_2 \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} + \frac{1}{2} \begin{pmatrix} x & y \end{pmatrix}  \begin{pmatrix} c_{11} & c_{12} \\ c_{21} & c_{22} \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix}.$$

\coro{}{
	To explicitly write out the $b_i$ and $c_{ij}$ terms, we must use tools from multivariable calculus.
}

Why $\vec{x} \cdot C \cdot \vec{x}$? When expanded, the expression yields quadratic terms in $x$ and $y$. Note that expanding \begin{align*} \vec{x} \cdot C \cdot \vec{x} &= \begin{pmatrix} x & y \end{pmatrix}\begin{pmatrix} c_{11} & c_{12} \\ c_{21}& c_{22} \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \\ &= \begin{pmatrix} x & y \end{pmatrix} \begin{pmatrix} c_{11}x+c_{12}y \\ c_{21}x+c_{22}y \end{pmatrix} \\ &= c_{11}x^2+c_{12}xy+c_{21}yx+c_{22}y^2 . \end{align*} 

So, we have that $$f(x, y) = a + b_1x+b_2y+c_{11}x^2+c_{12}xy+c_{21}yx+c_{22}y^2.$$

The quadratic terms $c_{11}x^2+c_{12}xy+c_{21}yx+c_{22}y^2$ are a \emph{quadratic form} in $x$ and $y$. The combined degree of $x$ and $y$ must be 2.

\defn{Quadratic form}{
	A quadratic form is a polynomial with terms all of degree two. For example, $$2x^2+4xy+3y^2$$ is a quadratic form, but $$2x^2+4xy+3y^2+4y+2$$ is not.
}

\tbf{Remark:} Let $\vec{x} = \begin{pmatrix} x \\ y \end{pmatrix}$. Then, $\vec{b} \cdot \vec{x} = b_1x + b_2y$. This means that the second term in the quadratic approximation corresponds to the \tbf{linear terms in $x$ and $y$.} Similarly, the third term yields $c_{11}x^2+c_{12}xy+c_{21}yx+c_{22}y^2$, which are the \tbf{quadratic terms in $x$ and $y$.}

\tbf{Intuition:} Whenever you see something along the lines of $\vec{x} \cdot C \cdot \vec{x}$, you should think in terms of quadratic terms in $x$ and $y$, or quadratic forms. Whenever you see $\vec{b} \cdot \vec{x}$, think in terms of linear forms.

\subsection{Homogeneous functions}

\defn{Homogeneous function}{Homogeneous functions of \emph{degree $d$} are functions that satisfy $$f(kx_1, kx_2) = k^d f(x_1, x_2)$$
}

Homogeneous functions appear in producer theory. Consider the Cobb-Douglas production function $f(x_1, x_2) = Ax_1^ax_2^b$. Note that $$f(kx_1, kx_2) = Ak^ax_1^ak^bx_2^b = k^{a+b} Ax_1^ax_2^b,$$ making $f(x_1, x_2)$ homogeneous of degree $a+b$.

A function homogeneous of degree 1 has \emph{constant returns to scale}. A function homogeneous of degree greater than 1 has \emph{increasing returns to scale}, or \emph{economies of scale}. Meanwhile, a function homogeneous of degree less than 1 has \emph{diminishing returns to scale}, or \emph{diseconomies of scale}.

Here is an example of a function of two variables.

\ex{Isoquants}{An isoquant tells us which combinations of two inputs provides the same utility. Let our two inputs be $\{x_1, x_2\}$. The isoquant satisfies $f(x_1, x_2)=c$ for a real-valued constant $c$.

Let $f(x_1, x_2)= c_1$ and $f(x_1, x_2)= c_2$ be two isoquants. They are parallel to each other.
}

\subsection{Partial derivatives}

The partial derivative is the derivative's analogue to multivariable functions. We can differentiate $f(x, y)$ in a similar way as we would differentiate $f(x)$. We differentiate $f(x, y)$ with respect to either $x$ or $y$, assuming that all non-$x$ terms or non-$y$ terms are constant terms.

We denote the partial derivative of $f$ with respect to $x$ as $\frac{\partial f}{\partial x}.$ Second partial derivatives are performed similarly to $f''(x)$.

\tbf{Abuse of notation}: We write the partial derivative of a function evaluated at $x_0$, $\frac{\partial f}{\partial x}|_{x=x_0}$ as $\frac{\partial f}{\partial x_0}.$

\ex{Isoquants}{
	Isoquants $f(x_1, x_2)= c_1$ and $f(x_1, x_2)= c_2$ are parallel to one another because their partial derivatives are the same.
	
	Additionally, each isoquant $f(x, y)= c_1$ can be rearranged into a function $y=f(x)$ by rearranging the subject. For example, $x^2+\frac{y}{2}=4$ can be rearranged to become $y=2(4-x^2)$.
}

\ex{}{
	Let $f(x, y)= x^2+2xy+3y^2.$ Then, $$\frac{\partial f}{\partial x} = 2x+2y.$$ Note how we treated $3y^2$ as a constant. Also, $$\frac{\partial f}{\partial y} = 2x+6y,$$ treating $x^2$ as a constant.
}

\ex{}{
	Let $g(x, y) = \sin x+2x\cos y$. Then, $\frac{\partial g}{\partial x} = \cos x+ 2\cos y$, and $\frac{\partial g}{\partial y} = -2x\sin y.$
}

\defn{Gradient}{
	The gradient vector is the vector of partial derivatives of a function. It is defined as $$\nabla f(x, y) = \begin{pmatrix} \frac{\partial f}{\partial x} \\  \frac{\partial f}{\partial y} \end{pmatrix}.$$
	
	Thus, it is the multi-variable analogue to the first derivative for a single-variable function.
}

The gradients point in the direction of where $f(x, y)$ is increasing in both $x$ and $y$. The key point is that the gradient is defined on \textbf{every point of the x-y plane wherein the partial derivatives can be evaluated.}

\coro{}{
	The linear approximation to a multivariable function $f(\vec{x})= a+ \vec{b}\cdot \vec{x}$ can be written as $$f(\vec{x}) = \vec{a} + \nabla f(\vec{x}) \cdot \vec{x}.$$
	
	In two variables, the entries $b_1$ and $b_2$ correspond to $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ respectively.
}

\ex{}{
	Consider the function $g(x, y) = \sin x+2x\cos y$ as above, and perform a linear approximation about the point $(0, 0)$. First note that the constant term is $g(0, 0) = 0$. The gradient vector is $\begin{pmatrix} \cos x+ 2\cos y \\ -2x\sin y \end{pmatrix};$ evaluated at $(0,0)$, this is $\begin{pmatrix} 3 \\ 0 \end{pmatrix}.$ 
	
	Thus, the linear approximation is $g(0, 0) = 0 + \begin{pmatrix} 3 & 0 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = 3x.$
}

Note that the gradient maps a pair $$\begin{pmatrix} x \\ y \end{pmatrix} \in \bbR^2$$ to $$\begin{pmatrix} \frac{\partial f}{\partial x}\\ \frac{\partial f}{\partial y} \end{pmatrix} \in \bbR^2.$$ This is an example of a \tbf{vector field}, where a vector is defined on every point of the x-y plane.

A nice property of gradients is that they are perpendicular to the isoquants $g(x, y)=c$. This is because the gradient vectors point in the direction where the partial derivatives \tbf{change the greatest}, and the isoquant vectors point where the partial derivatives \tbf{change the least} (the gradient is zero along the isoquant), meaning that $\frac{\partial f}{\partial x} =0$ and $\frac{\partial f}{\partial y} =0$.

What is the slope of the isoquant $y(x)$? It is $\frac{dy}{dx}$. Thus, the vector that points along the isoquant is $$\vec{l}(x, y) =  \begin{pmatrix} 1 \\ \frac{dy}{dx} \end{pmatrix},$$ where the slope is $\frac{dy}{dx}$.

In fact, any scalar multiple of $\begin{pmatrix} 1 \\ \frac{dy}{dx} \end{pmatrix}$ works out.

One can also view it this way. Let $g(x, y) = c_1$ and $g(x, y) = c_2$ be isoquants; thus, they are parallel to one another. The gradient points in the direction of greatest change in partial derivatives. This means that it points in the direction where the distance between $g(x, y) = c_1$ and $g(x, y) = c_2$ is the shortest. Since the shortest distance between two parallel curves is perpendicular to both curves, it follows that the isoquants and the gradients are perpendicular to one another.

\defn{Multivariable chain rule}{
	Let $x= g(t)$ and $y=h(t)$ be functions of $t$, and let $z$ be a function $z=f(x, y)$. Then, $z = f(x(t), y(t))$ is a multivariable function of $t$. The derivative of $z$, denoted $\frac{dz}{dt}$, is given by $$\frac{dz}{dt} = \frac{\partial z}{\partial x}\cdot \frac{dx}{dt} + \frac{\partial z}{\partial y}\cdot \frac{dy}{dt} .$$	
}

\ex{}{
	Let $z = 2x^2y^3$. Additionally, let $x = 2t$ and $y=3t^2$. We have that $\frac{\partial z}{\partial x} = 4y^3x, \frac{\partial z}{\partial y} = 6x^2y^2, \frac{dx}{dt}=2, \frac{dy}{dt}=6t.$ Then, \begin{align*} \frac{dz}{dt} &= 4y^3x \cdot 2 + 6x^2y^2 \cdot 6t \\ &= 8(3t^2)^3 (2t)+ 36 (2t^2)(3t^2)t \\ &= 432t^7+216t^5. \end{align*}
}

\subsection{Implicit functions}

Suppose we have that $f(x, y) = c$ for some real-valued $c$. This means that there must be some combination of $x$ and $y$, where one of them is a function of the other, satisfying the condition $f(x, y)=c$. Can we say something about $\frac{dy}{dx}$ without knowing what $f$ is? It turns out that we can.

\thrm{Implicit function theorem}{
	Let $\frac{\partial f}{\partial y} \neq0$. Then, for a function $f(x, y)$, $$\frac{dy}{dx} = \frac{-\frac{\partial f}{\partial x}} {\frac{\partial f}{\partial y}}.$$
}

\coro{}{ Some students mix up whether $\frac{\partial f}{\partial x}$ is on the numerator or denominator. A nice hint would be to consider $\frac{\frac{\partial f}{\partial x}}{\frac{\partial f}{\partial y}} = \frac{\partial f}{\partial x} \cdot \frac{\partial y}{\partial f} = \frac{\partial y}{\partial x}$. Why the negative sign? We will show this later.}

\ex{}{
	Consider the utility function $u(x, y) = x^a y^b.$ Then, $$\frac{dy}{dx} = \frac{-ax^{a-1}y^b}{bx^ay^{b-1}} = -\frac{a}{b}x^{-1}y.$$
	
	This is also known as the \tbf{marginal rate of substitution}, or MRS, of the utility function.
}

\ex{}{
	Use $g(x, y) = \sin x+2x\cos y$ from above. The gradient vector $$\nabla g(x, y) = \begin{pmatrix} \cos x + 2\cos y \\ -2x \sin y \end{pmatrix}.$$
	
	The gradients along the isoquants $g(x, y) = c_i$ for $c_i$ constants is $\begin{pmatrix} 1 \\ \frac{dy}{dx} \end{pmatrix}$. This is $$\vec{l}(x, y) = \begin{pmatrix} 1 \\ \frac{-\cos x-2\cos y}{-2x \sin y} \end{pmatrix}= \begin{pmatrix} 1 \\ \frac{-\frac{\partial f}{\partial x}} {\frac{\partial f}{\partial y}} \end{pmatrix} = \begin{pmatrix} 1 \\ \frac{\cos x+2\cos y}{2x \sin y} \end{pmatrix}.$$
}

We considered a vector field in the case of the isoquant. What about the vector field that points in the direction of the isoquant? This is $$\vec{l}(x, y) = \begin{pmatrix} x'(x) \\ y'(x) \end{pmatrix} = \begin{pmatrix} 1 \\ \frac{-\frac{\partial f}{\partial x}} {\frac{\partial f}{\partial y}} \end{pmatrix}.$$

To show that the isoquant is perpendicular to the gradient, simply show that \begin{align*} \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{pmatrix} \cdot \begin{pmatrix} 1 \\ \frac{-\frac{\partial f}{\partial x}} {\frac{\partial f}{\partial y}} \end{pmatrix} &= \frac{\partial f}{\partial x}-\frac{\partial f}{\partial x} \\&=0.\end{align*}

\defn{Is a given vector field a gradient?} {Consider the vector field $\vec{x}(x, y) = \begin{pmatrix} v_1(x, y) \\ v_2(x, y) \end{pmatrix} = \begin{pmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{pmatrix}$. If the condition $\frac{\partial^2 f}{\partial y\partial x} - \frac{\partial^2 f}{\partial x\partial y} =0$ is satisfied, or

$$\frac{\partial v_1}{\partial y} - \frac{\partial v_2}{\partial x} =0,$$ then the vector field is a gradient.}

What are we doing here? We are checking if the entries in the gradient vector are indeed the partial derivative respect to $x$ and the partial derivative with respect to $y$ of the same function.

\section{Approximations in two variables}

Recall that we can perform a linear approximation for a single-variable function by evaluating its Taylor series until its linear term. Explicitly, we have that $$g(x) = g(x_0)+(x-x_0) g'(x_0)$$

If we have $g(x, y)$ instead and we want to approximate it at $(x_0, y_0)$, we have that 

$$g(x, y) = g(x_0, y_0) + (x-x_0, y-y_0) \cdot \nabla g(x, y) = g(x_0, y_0) + (x-x_0, y-y_0) \cdot \begin{pmatrix} \frac{\partial g}{\partial x_0} \\ \frac{\partial g}{\partial y_0} \end{pmatrix} $$

Notice the last term. This is $$\frac{\partial g}{\partial x_0}(x-x_0) + \frac{\partial g}{\partial y_0}(y-y_0).$$

This is motivation for what we call the \emph{total derivative}.

\defn{Total derivative}{If we make an infinitesimal change to $g(x, y)$, say $g(x+dx, y+dy)$, then we can approximate $$g(x+dx, y+dy) = g(x, y) + dx \frac{\partial g}{\partial x} + dy \frac{\partial g}{\partial y}.$$ The last two terms describe that an infinitesimal change in $g$ - denoted $dg$, would be $dx \frac{\partial g}{\partial x} + dy \frac{\partial g}{\partial y}$. This is the total derivative of $g(x, y).$
}

When $x$ is sufficiently near $x_0$, $x-x_0$ is infinitesimally small, and can be approximated by $dx$. Thus, the last term of the linear approximation $$\frac{\partial g}{\partial x_0}(x-x_0) + \frac{\partial g}{\partial y_0}(y-y_0) = dx \frac{\partial g}{\partial x} + dy \frac{\partial g}{\partial y},$$ giving us the total derivative of $x$. 

\tbf{Conclusion:} This means that the 2-variable analogue to an infinitesimally small length $dx$ in single-variable calculus is the (first) total derivative.

\ex{Revisiting the implicit function theorem}{

Consider the total derivative of a multivariable function in two dimensions. Assume that $f(x_0, y_0)=0$. We have that infinitesimal changes in both $x, y$ around $(x_0, y_0)$ still produce 0. Formally, $$f(x_0 + \Delta x, y_0+\Delta y) = f(x_0, y_0) + \frac{\partial g}{\partial x_0} \Delta x + \frac{\partial g}{\partial y_0} \Delta y = 0$$ which implies that $$\frac{\partial g}{\partial x_0} \Delta x + \frac{\partial g}{\partial y_0} \Delta y = 0,$$ or $$\Delta x = -\frac{\frac{\partial g}{\partial y_0} \Delta y}{\frac{\partial g}{\partial x_0} \Delta x} \Delta y,$$ or $$\frac{\Delta y}{\Delta x} = -\frac{\frac{\partial g}{\partial x_0} \Delta x}{\frac{\partial g}{\partial y_0} \Delta y}.$$

Noting that the $\Delta x$ and $\Delta y$ are infinitesimally small, we move the $\frac{\Delta x}{\Delta y}$ over to obtain $$\frac{dy}{dx}= -\frac{\frac{\partial g}{\partial x_0}}{\frac{\partial g}{\partial y_0}},$$ which is the statement of the implicit function theorem.
}

Let there be a system of functions linearly approximated at $x_0$. Then we would have $$g_1(x, y) = g_1(x_0, y_0) + (\overrightarrow{x-x_0}) \cdot \begin{pmatrix} \frac{\partial g_1}{\partial x_0} \\ \frac{\partial g_1}{\partial y_0} \end{pmatrix}\ \text{and}$$ $$ g_2(x, y) = g_2(x_0, y_0) + (\overrightarrow{x-x_0}) \cdot \begin{pmatrix} \frac{\partial g_2}{\partial x_0} \\ \frac{\partial g_2}{\partial y_0} \end{pmatrix}\ \text{, which produces} $$ 

$$\begin{pmatrix} g_1(x, y) \\ g_2(x, y) \end{pmatrix} = \begin{pmatrix} g_1(x_0, y_0) \\ g_2(x_0, y_0) \end{pmatrix}+ (\overrightarrow{x-x_0}) \cdot \begin{pmatrix} \frac{\partial g_1}{\partial x_0} & \frac{\partial g_1}{\partial y_0} \\ \frac{\partial g_2}{\partial x_0} & \frac{\partial g_2}{\partial y_0}\end{pmatrix}$$

The matrix $$\begin{pmatrix} \frac{\partial g_1}{\partial x_0} & \frac{\partial g_1}{\partial y_0} \\ \frac{\partial g_2}{\partial x_0} & \frac{\partial g_2}{\partial y_0}\end{pmatrix}$$ is named the \tbf{Jacobian matrix}. 

\coro{}{\textbf{Key idea:} The Jacobian matrix is a \tbf{generalisation of the derivative} for one single-variable function towards \tbf{multiple multivariable functions.}}

Let's explore linear approximations of multiple multivariable functions using the Jacobian a la the total derivative.

The list below concludes the way we approach linear approximations for various functions, or lists of functions.

\begin{itemize}
	\item For one single-variable function, we use the derivative.
	\item For multiple single-variable functions (i.e. parametrised functions), we use a vector equation and perform the derivatives.
	\item For one multivariable function, we use the gradient vector.
	\item For multiple multivariable functions, we use the Jacobian matrix.
\end{itemize}

Can we perform quadratic approximations for a multivariable function? Yes. We use the Hessian matrix. Consider the single-variable case $$g(x) = g(x_0) + (x-x_0) \frac{df}{dx_0} + \frac{1}{2} (x-x_0)^2 \frac{d^2f}{dx_0^2}.$$ We find that its multivariable analogue is $$g(x, y) = g(x_0, y_0) + (x-x_0 \ y-y_0) \nabla g(x_0, y_0) + \frac{1}{2} (x-x_0 \ y-y_0) \cdot H(g(x_0, y_0)) \cdot \begin{pmatrix} x-x_0 \\ y-y_0 \end{pmatrix},$$ where the matrix $H(g(x_0, y_0))$ is $$\begin{pmatrix} \frac{\partial^2 g}{\partial x_0^2} & \frac{\partial^2 g}{\partial x_0y_0} \\ \frac{\partial^2 g}{\partial y_0x_0} & \frac{\partial^2 g}{\partial y_0^2}\end{pmatrix}$$

We call this matrix the \emph{Hessian} matrix. Here, the Hessian is evaluated at $(x_0, y_0)$; in generality we write the Hessian as $$\begin{pmatrix} \frac{\partial^2 g}{\partial x^2} & \frac{\partial^2 g}{\partial xy} \\ \frac{\partial^2 g}{\partial yx} & \frac{\partial^2 g}{\partial y^2}\end{pmatrix}$$

\coro{}{
	The terms in this matrix exactly correspond to the $\begin{pmatrix} c_{11} & c_{12} \\ c_{21} & c_{22} \end{pmatrix}$ terms seen in the quadratic approximation! 
	
	So, the Hessian matrix can be seen as the multi-variable analogue to the second derivative.
}

\defn{Hessian matrix}{
	Let $g(x, y)$ be a multivariable function. In its quadratic approximation, the quadratic term could be approximated by $\frac{1}{2} \begin{pmatrix} x-x_0 & y-y_0 \end{pmatrix} H(g(x, y)) \begin{pmatrix} x-x_0 \\ y-y_0 \end{pmatrix},$ where the matrix $H(g(x_0, y_0))$ is given by $$H(g(x, y))= \begin{pmatrix} \frac{\partial^2 g}{\partial x^2} & \frac{\partial^2 g}{\partial xy} \\ \frac{\partial^2 g}{\partial yx} & \frac{\partial^2 g}{\partial y^2} \end{pmatrix}.$$ 
	
	Moreover, let $g(x_1, \dots, x_n)$ be a multivariable function. Its Hessian matrix is $$\begin{pmatrix} \frac{\partial^2 g}{\partial x_1^2} & \frac{\partial^2 g}{\partial x_1x_2} & \dots & \frac{\partial^2 g}{\partial x_1x_n} \\ \frac{\partial^2 g}{\partial x_2x_1} & \frac{\partial^2 g}{\partial x_2^2} & \dots & \frac{\partial^2 g}{\partial x_2x_n} \\ \vdots & \ddots & \ddots & \vdots \\ \frac{\partial^2 g}{\partial x_nx_1} & \frac{\partial^2 g}{\partial x_nx_2} & \dots & \frac{\partial^2 g}{\partial x_n^2}\end{pmatrix}.$$ 
}

We can classify the nature of the optima like we do for single-variable functions. 

Consider the Hessian matrix $$H = \begin{pmatrix} \frac{\partial^2 g}{\partial x^2} & \frac{\partial^2 g}{\partial xy} \\ \frac{\partial^2 g}{\partial yx} & \frac{\partial^2 g}{\partial y^2}\end{pmatrix}$$

\begin{enumerate}
	\item An optima $g(x_0, y_0)$ is a minimum point if for all vectors $\vec{a}$ we have that $\vec{a} \cdot H \cdot \vec{a} >0$. In this case, the Hessian matrix is \emph{positive-definite}. It is a minimum as all deviations within a small range of the optima lead to an increase in $g$.
	\item An optima $g(x_0, y_0)$ is a maximum point if for all vectors $\vec{a}$ we have that $\vec{a} \cdot H \cdot \vec{a} <0$. The Hessian matrix is \emph{negative-definite}.
	\item An optima $g(x_0, y_0)$ is a saddle point if $\vec{a} \cdot H \cdot \vec{a} =0$. This means that the optima is a local minimum along one axis and a local maximum along another axis.
\end{enumerate}

Recall that terms that are quadratic in $x$ and $y$ are obtained when one multiplies $\vec{a} \cdot H \cdot \vec{a}$. 

\defn{Definiteness of a matrix}{
	A positive-definite matrix is a matrix in which the quadratic form obtained from $\vec{a} \cdot H \cdot \vec{a}$ is invariably positive for all vectors $\vec{a} \in \bbR$. Let $H = \begin{pmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \end{pmatrix}$, and let $\vec{a} = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}$.
	
	Explicitly, this means that $$ \begin{pmatrix} a_1 & a_2 \end{pmatrix} \begin{pmatrix} h_{11} & h_{12} \\ h_{21} & h_{22} \end{pmatrix} \begin{pmatrix} a_1 \\ a_2 \end{pmatrix} = h_{11} a_1^2 + h_{12} a_1a_2 + h_{21} a_2a_1 + h_{22} a_2^2 $$ must be positive.
	
	Similarly, a negative-definite matrix is where the quadratic form obtained from $\vec{a} \cdot H \cdot \vec{a}$ is negative. 
}

\ex{}{
	Let $H = \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix}.$ Then, the matrix $\vec{a} \cdot H \cdot \vec{a}$ for non-zero $\vec{a}$ is given by \begin{align*}\begin{pmatrix} a_1 & a_2 \end{pmatrix}  \begin{pmatrix} 2 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} a_1 \\ a_2 \end{pmatrix} &= \begin{pmatrix} a_1 & a_2 \end{pmatrix} \begin{pmatrix} 2a_1 \\ a_2 \end{pmatrix}  \\ &= 2a_1^2+a_2^2,\end{align*} which is invariably positive. Thus, $H$ is positive-definite.
}

% example of positive / negative-(semi)definiteness

\includegraphics[scale=0.3]{saddle.png} 

It might seem difficult to identify when $\vec{a} \cdot H \cdot \vec{a} >0$ for \emph{all vectors} $\vec{a}$. Our job becomes easier once we study eigenvalues and eigenvectors in Week 6.

The hyperbolic paraboloid above is a prototypical example of a saddle point, where it is a maximum along one axis and a minimum along another.

(n.b.) This saddle point allows for easy stacking of potato crisps in a cylindrical can. This should remind you of the shape of a \emph{Pringles crisp.} 

Finally, let's return to our multivariable quadratic approximation $$g(\vec{x}) = g(\vec{x_0}) + (\overrightarrow{x-x_0}) \nabla g + \frac{1}{2} (\overrightarrow{x-x_0}) \cdot H(g) \cdot (\overrightarrow{x-x_0}).$$

Differentiating both sides yields $\nabla g + H(g) \cdot (\overrightarrow{x-x_0}) = 0,$ or $$H(g) \cdot (\overrightarrow{x-x_0}) = -\nabla g.$$ 

Would it be convenient if we could move the Hessian matrix $H(g)$ to the other side to obtain a closed form solution for $\overrightarrow{x-x_0}$? It turns out that we can with inverse matrices, discussed in Week 4. Our solution is

$$(\overrightarrow{x-x_0}) = -(H(g))^{-1} \nabla g.$$

\chapter{Week 4}

In this week, we discuss various properties of matrices - including the matrix trace, determinant, and inverse.

\section{Matrices as linear maps}

We need to build up our intuition of viewing matrices as linear maps. 

\defn{Vector space}{
	Let $V$ be a set of vectors of the same dimension. A \emph{vector space} is a set of vectors where you can perform \tbf{vector addition} and \tbf{scalar multiplication} such that: 
	\begin{itemize}
	\item $\vec{a}, \vec{b} \in V$ implies that $\vec{a}+ \vec{b} \in V$
	\item	$\vec{a} \in V, k \in \bbR$ implies that $k\vec{a} \in V$.
	\end{itemize}
}

Let's explore the relationships between vectors.

\defn{Linear independence}{
	Three vectors are linearly independent if for any combination $$k_{a} \vec{a}+k_{b} \vec{b}+k_{c} \vec{c}=0,$$ the only solution is $\{k_a, k_b, k_c\} = \{0, 0, 0\}.$ 
}

Recall, from Week 2, that we used \emph{linear combinations} of the vectors $\vec{a}= \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ and $\vec{b}=\begin{pmatrix} 0 \\ 1 \end{pmatrix}$ to get $\vec{c}=\begin{pmatrix} 15 \\ -5 \end{pmatrix}$, meaning that we added 15 copies of $\vec{a}$ and -5 copies of $\vec{b}$ to get $\vec{c}$. 

This means that $\vec{c}-15\vec{a}+5\vec{b}=0$. Note this means that the three vectors are not linearly independent.

From this we can formally define a basis.

\defn{Basis vectors}{
	Let $V$ be a vector space. A set of vectors $\{\vec{b_1}, \dots, \vec{b_n}\}$ are a basis of the vector space if:
	
	\begin{enumerate}
		\item All of the basis vectors are \tbf{linearly independent}.
		\item Any vector in the vector space can be formed by a linear combination of the basis vectors. This 	condition is also known as vectors \tbf{spanning} the vector space. 
	\end{enumerate}
	
	An implication of this is that any set in $V$ containing more than $n$ vectors must be linearly dependent.
}

\defn{Vector dimension}{
	The number of vectors in the basis is its dimension.
}

\ex{Canonical basis for $\bbR^3$}{
	The canonical basis for $\bbR^3$ is $$\begin{pmatrix} 1\\0\\0 \end{pmatrix}, \begin{pmatrix} 0\\1\\0 \end{pmatrix},\begin{pmatrix} 0\\0\\1 \end{pmatrix}.$$
}

\tbf{Exercise:} Convince yourself that in the example above, the three basis vectors are linearly independent, and that they span $\bbR^3$ - meaning the set of all triples $(a, b, c)$ where $a, b, c \in \bbR$.

Recall that linear forms are maps from a vector space $V \to \bbR$. 

The set of linear forms form a vector space. This is called the \emph{dual space} of $V$, denoted $V^{*}$. How do we see this? Well, consider two maps $$\mcA, \mcB:V \to \bbR$$ that satisfies the vector space conditions. Then, we can see that $$(\mcA+\mcB)(\vec{a}) = \mcA(\vec{a}) + \mcB(\vec{a})$$ and $$(k \mcA)(\vec{a}) = k\mcA(\vec{a}).$$ 

The key point is that both $(\mcA+\mcB)(\vec{a})$ and $(k \mcA)(\vec{a})$ are linear maps!

\subsection{Moving towards $n$ dimensions}

Consider our standard, canonical basis in $n$ dimensions. This is $\{1, 0, \dots, 0\}, \{0, 1, \dots, 0\}, \dots, \{0, 0, \dots, 1\}$. When we explored change of basis, we went from our old basis to our new basis by performing

$$\begin{pmatrix} (\vec{v}_1)_1 & (\vec{v}_1)_2 \\ (\vec{v}_2)_1 & (\vec{v}_2)_2 \end{pmatrix} = \begin{pmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$$

\coro{}{ The matrix $\bbI = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$ is known as the \tbf{identity matrix}. When acted on by any matrix $M$, it satisfies $$M \cdot \bbI = M.$$}

Let's talk about linear maps from $\bbR^n$ to $\bbR^n$, a generalisation of our basis change from $\bbR^2 \to \bbR^2$ to $n$ dimensions. Consider our representation of $\vec{b}$ as a \tbf{linear combination of vectors in the} canonical basis: $$\vec{b} = \sum\limits_{k=1}^n b_k \hat{e_k},$$ and consider a new set of basis vectors $$\vec{b} = \sum\limits_{k=1}^n \beta_k \vec{v_i}.$$ 

We showed, above, how we could represent a set of vectors in terms of multiplying a "basis change matrix" by the identity matrix, produced using the canonical basis. Replace $\vec{v_i}$ with $\sum\limits_{j=1}^n M_{ij} \hat{e_j}$ to obtain \begin{align*}\vec{b} = \sum\limits_{k=1}^n \beta_k \vec{v_i} &= \sum\limits_{i=1}^n \beta_i \left(\sum\limits_{j=1}^n M_{ij} \hat{e_j}\right)  \\ &= \sum\limits_{j=1}^n\left( \sum\limits_{i=1}^n  \beta_i  M_{ij} \right) \hat{e_j} \end{align*}

Now note that the elements $ \sum\limits_{i=1}^n  \beta_i  M_{ij} $ correspond with $b_j$. Let's paint an intuitive picture of the elements $b_j$. To attain $b_j$, we add a linear combination of the entries in row $j$ of the matrix $M_{ij}$, using the coefficients $\beta_i$. 

A few more definitions before we move on to determinants and inverses.

\defn{Transpose}{
	Let $M$ be a matrix. The transpose of $M$, denoted $M^T$, is the matrix with all its rows and columns swapped.  
	
	For example, let $A= \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}.$ Then $$A^T = \begin{pmatrix} a_{11} & a_{21} & a_{31} \\ a_{12} & a_{22} & a_{32} \\ a_{13} & a_{23} & a_{33} \end{pmatrix}$$
}

Note that for two matrices $A, B$, we have that $(A\times B)^T = A^T \times B^T.$

\defn{Symmetric matrix}{
	A matrix $M$ is symmetric if it equals its transpose, i.e. $M=M^T$.
}

\defn{Trace}{
	Let $M$ be a matrix. The trace of $M$, denoted $\text{Tr}(M)$, is the sum of its diagonal entries. For example, let $A= \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}.$ Then its trace is $$\text{Tr}(A) = a_{11}+a_{22}+a_{33}.$$
}

The trace has some interesting properties. Let $A, B, C$ be matrices, and let $x, y$ be scalars.

\begin{itemize}
	\item $\text{Tr}(ABC)=\text{Tr}(BCA)=\text{Tr}(CAB)$.
	\item $\text{Tr}(xA+yB)=x\text{Tr}(A)+y\text{Tr}(B)$. More notably, $\text{Tr}(A+B)=\text{Tr}(A)+\text{Tr}(B)$.
	\item $\text{Tr}(AA^{T}) = \sum\limits{i=1}^n A_{ii}^2$ - meaning the sums of squares of the diagonal entries.
\end{itemize}

\section{Determinants and inverses}
\defn{Determinant}{
Let the vectors $\vec{a}, \vec{b} \in \bbR^2.$ The determinant is the area spanned by the parallelogram formed by the vectors representing the parallelogram's sides. It is the area of the parallelogram with vertices $(0, 0), (a, b), (c, d)$, and $(a+c, b+d)$. Formally, we have that for a matrix $M=\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}$, $$\det{M} = a_{11}a_{22}-a_{12}a_{21}.$$

}

\includegraphics[scale=0.3]{determinant.png}

The image above demonstrates the parallelogram spanned by the two vectors. It has area $ad-bc$. 

\tbf{Proof:} Let $\vec{u} = (a, b)$ and $\vec{v} = (c, d)$. Using the formula for the area of a triangle, we have that the area is $2 \cdot \frac{1}{2}|\vec{u}||\vec{v}| \sin \theta=|\vec{u}||\vec{v}| \sin \theta$ where the angle between the two vectors is $\theta$. 

Note that the dot product is given by $|\vec{u}||\vec{v}| \cos \theta$. I want to elicit a cosine out of the expression as it is easy to calculate dot products in two dimensions. Note that the vector perpendicular to $\vec{u}$ is $\vec{u}_p = (-b, a)$ (check using $\vec{u} \cdot \vec{u}_p =0$), meaning that the area is $$|\vec{u}_p||v|\cos\theta = \begin{pmatrix}-b \\ a \end{pmatrix} \cdot \begin{pmatrix}c \\ d \end{pmatrix}=ad-bc. \qed$$ 

In three dimensions, the determinant is the volume of the \tbf{parallelepiped} spanned by the three vectors. 

\includegraphics[scale=0.4]{det3.png}

Given a matrix $M= \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}$, the determinant satisfies: $$\det{M} = a_{11} (a_{22}a_{33}-a_{23}a_{32}) - a_{12}(a_{21}a_{33}-a_{23}a_{31}) + a_{13}(a_{21}a_{32}-a_{22}a_{31}).$$ The expressions in brackets should remind you of the determinant of a 2-by-2 matrix.

Computing 4-by-4 matrices is tedious. Note that when calculating the determinant of a 3-by-3 matrix, I computed the determinants of three 2-by-2 matrices ($3!=6$ terms in total). The determinant of a 4-by-4 matrix contains the computations of 4 determinants of 3-by-3 matrices ($4! = 24$ terms in total), which is very tedious. In fact, the determinant of an n-by-n matrix contains $n!$ terms.

I will sketch out how you evaluate the determinant of a 4-by-4 matrix. Given $M= \begin{pmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\ a_{21} & a_{22} & a_{23} & a_{24} \\ a_{31} & a_{32} & a_{33} & a_{34} \\ a_{41} & a_{42} & a_{43} & a_{44}\end{pmatrix},$ we have that $$\det{M} = a_{11} (\dots) - a_{12} (\dots) + a_{13} (\dots) - a{14}(\dots).$$

The 3-by-3 matrices in each $(...)$ consist of matrix elements that \tbf{do not share a row or column with $a_{1j}$.} This would be $\begin{pmatrix} a_{22} & a_{23} & a_{24} \\ a_{32} & a_{33} & a_{34} \\ a_{42} & a_{43} & a_{44} \end{pmatrix}$ for the element $a_{11}$.

At the end of Week 3 I gave a spoiler for how to move the Hessian matrix to the other side to obtain $\overrightarrow{x-x_0}$. This was the matrix $(H(g))^{-1}$. This inverse exists if the \tbf{determinant of the Hessian is not zero.}

Consider the matrix system $A\vec{x} = \vec{b}$, where $A$ is a matrix. Then, $$\vec{x} = A^{-1} \vec{b}.$$

Note that $\vec{x} = A^{-1} \vec{b} = A^{-1} \cdot A\vec{x}$, meaning that $A^{-1} \cdot A = \bbI$. But this was probably obvious. 

What is the inverse of a 2-by-2 matrix? It is $$M^{-1} = \frac{1}{\det{M}} \begin{pmatrix} m_{22} & -m_{12} \\ -m_{21} & m_{11}\end{pmatrix}.$$ To remember this, take your original matrix, flip the top left and bottom right terms, multiply the remaining terms by -1, and divide by its determinant.

How about the inverse of a 3-by-3 matrix? This is much more involved. Consider the matrix $$M= \begin{pmatrix} 3 & 2 & 4 \\ -3 & 5 & 4 \\ 1 & 2 & 3 \end{pmatrix}.$$

We first produce the \emph{matrix of minors}. We first replace each element with the determinant of the matrix composed of elements that do not share a row or column with said element. For example, we replace $M_{11}=3$ with $\det \begin{pmatrix} 5 & 4 \\ 2 & 3 \end{pmatrix}  = 7$, and we replace $M_{22}=5$ with $\det \begin{pmatrix} 3 & 4 \\ 1 & 3 \end{pmatrix}  = 5.$ The matrix of minors is therefore $$M= \begin{pmatrix} 7 & -13 & -11 \\ -2 & 5 & 4 \\ -12 & 24 & 21 \end{pmatrix}.$$ 

Then, we multiply certain elements by $-1$ according to the matrix $\begin{pmatrix} + & - & + \\ - & + & - \\ + & - & + \end{pmatrix}$. We multiply elements corresponding to a negative sign by $-1$. Another way is to consider $M_{ij}$. If $i+j$ is even, multiply $M_{ij}$ by $-1$. This is the \tbf{matrix of cofactors}, and is $\begin{pmatrix} 7 & 13 & -11 \\ 2 & 5 & -4 \\ -12 & -24 & 21 \end{pmatrix}.$

Now we transpose the matrix of cofactors and divide by $\det{M}$ (which is 3) to get our inverse: $$M^{-1}= \begin{pmatrix} \medskip \frac{7}{3} & \frac{2}{3} & -4 \\ \medskip \frac{13}{3} & \frac{5}{3} & -8 \\ \medskip -\frac{11}{3} & -\frac{4}{3} & 7 \end{pmatrix}.$$

One final note on composition of inverse matrices. Note that matrices represent linear maps, so given $A: V_1 \to V_2$ and $B: V_2 \to V_3$, we have the inverse maps $A^{-1}: V_2 \to V_1$ and $B^{-1}: V_3 \to V_2$. Thus, $$(BA)^{-1} = (V_1 \to V_3)^{-1} = V_3 \to V_1,$$ and $$A^{-1}B^{-1} = V_3 \to V_1.$$ They are the same! This yields the identity $(BA)^{-1} = A^{-1}B^{-1}.$

\chapter{Week 5}

In this week, we discuss linear independence in matrices and discuss the Jacobian matrix in $n$ dimensions.

\section{Non-square matrices}

Up to now, we have only considered square matrices in our study of linear algebra. These are matrices with the same number of rows as columns. What if the rows or columns were not linearly independent? Recall that linear independence implies that if the row (or column) vectors satisfy $c_1\vec{m_1} + \dots + c_n \vec{m_n} = \vec{0}$, then $c_i = 0$ for all $i$.

Consider when $\vec{m_2} = 2 \vec{m_1}$. Then the matrix $$M = \begin{pmatrix} \vec{m_1} \\ \vec{m_2} \\ \vdots \\  \vec{m_n} \end{pmatrix} = \begin{pmatrix} \vec{m_1} \\ 2\vec{m_1} \\ \vdots \\ \vec{m_n}, \end{pmatrix}$$ meaning we only have $n-1$ linearly independent components. The vector $\vec{y} = M \cdot \vec{x}$ for any $\vec{x}$ now has a dimension of $n-1$. 

We have a linear combination $c_1\vec{m_1} + c_2\vec{m_2} + \dots + c_n \vec{m_n}$ of vectors. If we have that $\vec{m_2}=2\vec{m_1}$, then our new linear combination is $(c_1+2c_2)\vec{m_1} + c_3\vec{m_3} + \dots + c_n \vec{m_n}$. We see that $\vec{m_2}$ has disappeared. 

We can truncate the second row to obtain a linearly independent set of vectors $M=\begin{pmatrix} \vec{m_1} \\ \vec{m_3} \\ \vdots \\ \vec{m_n} \end{pmatrix}$, and thus obtain the matrix relation $$\begin{pmatrix} y_1 \\ y_3 \\ \vdots \\ y_n \end{pmatrix} = \begin{pmatrix} \vec{m_1} \cdot \vec{x} \\ \vec{m_3} \cdot \vec{x} \\ \vdots \\ \vec{m_n} \cdot \vec{x} \end{pmatrix}.$$ 

Note that the matrix $m$ has $n-1$ rows and $n$ columns. Relabelling our matrix indices, we can write $$M=\begin{pmatrix} M_{11} & M_{12} & \dots & M_{1n} \\ M_{21} & M_{22} & \dots & M_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ M_{(n-1)1} & M_{(n-1)2} & \dots & M_{(n-1)n}\end{pmatrix}.$$

Should there be other rows or columns that are not linearly independent, we shall find the ways in which rows or columns are linear combinations of one another. We then find a way to remove said row or column. This process is known as \emph{Gaussian elimination}, which we will discuss later. Either way, we have produced a \tbf{non-square matrix}, where the number of rows does not equal the number of columns.

The addition of matrices can only be done if both matrices have the same number of rows and columns. However, consider matrices $A$ and $B$ that need not be square. Then, we can calculate $A \cdot B$ only if the \emph{number of rows in $A$ equal the number of rows in $B$.} More specifically, the product of an $m \times n$ matrix and an $n \times k$ matrix is an $m \times k$ matrix.

\ex{}{
	Let $A = \begin{pmatrix} 3 & 4 & 1 \\ 0 & -3 & -2 \end{pmatrix}$ and $B = \begin{pmatrix} 3 & 4 \\ 0 & -3 \\ -2 & 5 \end{pmatrix}$. 
	
	Then, \begin{align*} A \times B &=  \begin{pmatrix} 3\cdot3+4\cdot0+1\cdot-2 & 3\cdot4+4\cdot-3+1\cdot 5 \\ 0\cdot3+-3\cdot0+-2\cdot-2 & 0\cdot4+-3\cdot-3+-2\cdot 5 \end{pmatrix} \\ &= \begin{pmatrix} 7 & 5 \\ 4 & -1 \end{pmatrix}. \end{align*}
}

\subsection{Rank, kernel, row reduction}

\defn{Matrix rank}{
	The row or column rank of a matrix is the total \tbf{number of linearly independent rows or columns} of the matrix. Note that we will demonstrate later that the \tbf{row rank equals the column rank. }
	
	Consequently, any vector acted on by a matrix of rank $m$ underlies a linear map from $\bbR^n \to \bbR^m$. Formally, this is a map $\vec{x} \mapsto \vec{x} \cdot M$ or $\vec{x} \mapsto M \cdot \vec{x}.$
	
	The map under the matrix $M$ is usually termed the \emph{image} of a vector $\vec{x}$, and is a vector space. The dimension of the image is the rank.
}

\ex{}{
	Consider the matrix $\begin{pmatrix} 1 & 3 & 4 \\ 0 & 1 & 1 \\ 1 & 4 & 5 \end{pmatrix}$. Note that $\vec{r_3} = \vec{r_2} + \vec{r_1}$. Thus, the third row (and column) is the sum of the other two rows (and columns), so the row rank (and column rank) is at most 2. 
	
	Also note that the second row is not a scalar multiple of the first row, so the rank is 2.
}

How do we know, for sure, what the rank is? The answer is Gaussian elimination, or representing a matrix in reduced row echelon form (RREF). One performs row operations on a matrix to represent the matrix in RREF. These include:

\begin{enumerate}
	\item \tbf{Multiplying} any row by a constant $k\neq 0$ and making the result the new row. This is notated $R \to kR$, 
	\item \tbf{Replacing} any row with $k$ times another row, added to the original row. This is notated $R \to R+kR'$.
	\item Rearranging any two rows. 
\end{enumerate}

Consider a 3-by-3 matrix $M= \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}.$ 

We can perform the row operation $\{a_{11}, a_{12}, a_{13}\} \to \{ka_{11}, ka_{12}, ka_{13}\}$, and thus replace $\{a_{11}, a_{12}, a_{13}\}$ with $\{ka_{11}, ka_{12}, ka_{13}\}$. We can also perform $\{a_{11}, a_{12}, a_{13}\} \to \{a_{11}-ka_{21}, a_{12}-ka_{22}, a_{13}-ka_{23}\}$, and replace $\{a_{11}, a_{12}, a_{13}\}$ with $\{a_{11}-ka_{21}, a_{12}-ka_{22}, a_{13}-ka_{23}\}$. Finally, we can swap $\{a_{11}, a_{12}, a_{13}\}$ with $\{a_{21}, a_{22}, a_{23}\}$.

\defn{Row-reduced form and reduced-row echelon form}{
	A matrix in row-reduced form is a matrix in which the leading entry, or first non-zero entry, is 1. For example, $$M=\begin{pmatrix} 0 & 1 & 1 & 3 \\ 1 & 3 & -2 & 0 \\ 0 & 0 & 1 & 4 \end{pmatrix}$$ is row-reduced.
	
	A matrix in reduced-row echelon form is a matrix with the zero rows at the bottom of the matrix and the other rows in a staircase-like formation. Matrix $M$, in reduced-row echelon form, is $$M=\begin{pmatrix} 1 & 0 & 0 & 11 \\ 0 & 1 & 0 & -1 \\ 0 & 0 & 1 & 4 \end{pmatrix}.$$ A good way to check for reduced-row echelon form is to see if the leading entry is 1 \tbf{and is the only non-zero entry of its column}.
}

\ex{Row reduction}{
	Let $M=\begin{pmatrix} 0 & 1 & 1 & 3 \\ 1 & 3 & -2 & 0 \\ 0 & 0 & 1 & 4 \end{pmatrix}$. We aim to put $M$ in RREF. We start by swapping $R_1$ and $R_2$: $$M=\begin{pmatrix}  1 & 3 & -2 & 0 \\ 0 & 1 & 1 & 3 \\ 0 & 0 & 1 & 4 \end{pmatrix},$$ then perform the row operation $R_1 = R_1-3R_2$ to produce $$M=\begin{pmatrix}  1 & 0 & -5 & -9 \\ 0 & 1 & 1 & 3 \\ 0 & 0 & 1 & 4 \end{pmatrix},$$ then perform $R_1=R_1+5R_3$ to make $$M=\begin{pmatrix}  1 & 0 & 0 & 11 \\ 0 & 1 & 1 & 3 \\ 0 & 0 & 1 & 4 \end{pmatrix},$$ then produce $R_2=R_2-R_3$ to make $$M=\begin{pmatrix} 1 & 0 & 0 & 11 \\ 0 & 1 & 0 & -1 \\ 0 & 0 & 1 & 4 \end{pmatrix}.$$ The resulting matrix is in reduced-row echelon form. We see that column 4 is a linear combination of the first 3 columns, demonstrating that both the row rank and the column rank is 3.
}

Note that reduced-row echelon form represents all non-basis column vectors in terms of the canonical basis. In the example above, the existence of RREF implies that the \tbf{row rank equals the column rank} - which is 3 in this case.

\defn{Matrix kernel}{
	The kernel of a matrix is the set of vectors $\vec{x}$ where $M \cdot \vec{x}=\vec{0}$. Simply put, they are the set of vectors which, when acted on by the matrix $M$, yields the zero matrix.
	
	The kernel of a matrix is, in itself, a vector space. It therefore has a dimension, just like the image of a matrix.
}

How does the kernel and the image relate?

\thrm{Rank-nullity theorem}{
	Let $M$ be a matrix and let $\text{Ker}(M)$ and $\text{Im}(M)$ be its kernel and image respectively. We have that $$\text{dim}(\text{Ker}(M)) + \text{dim}(\text{Im}(M))= \text{dim}(M).$$ Simply put, the dimension of the kernel is the number of zero rows, and therefore truncated rows, in our reduced-row echelon form matrix.
}

\section{Revisiting the Jacobian}

In week 3, we showed the following result. Assume that $f(x_0, y_0)=0$. We have that infinitesimal changes in both $x, y$ around $(x_0, y_0)$ still produce 0. Formally, $$f(x_0 + \Delta x, y_0+\Delta y) = f(x_0, y_0) + \frac{\partial g}{\partial x_0} \Delta x + \frac{\partial g}{\partial y_0} \Delta y = 0$$ which implies that $$\frac{dy}{dx}= -\frac{\frac{\partial g}{\partial x_0}}{\frac{\partial g}{\partial y_0}}.$$

How does this relate to the Jacobian? Consider we have a set of $n$ matrices in $m$ variables. Consider when $n=m=2$. Then, let $f_1(x, y_1, y_2)=0$ and $f_2(x, y_1, y_2)=0$. The functions $y_1$ and $y_2$ are functions of $x$. This yields 

\begin{align*}
	\frac{\partial f_1}{\partial x}+ \frac{\partial f_1}{\partial y_1} \frac{dy_1}{dx}+ \frac{\partial f_1}{\partial y_2} \frac{dy_2}{dx}&=0\\
	\frac{\partial f_2}{\partial x}+ \frac{\partial f_2}{\partial y_1} \frac{dy_1}{dx}+ \frac{\partial f_2}{\partial y_2} \frac{dy_2}{dx}&=0
\end{align*}

which yields $$\begin{pmatrix} \medskip \dfrac{\partial f_1}{\partial y_1} & \dfrac{\partial f_1}{\partial y_2} \\ \medskip \dfrac{\partial f_2}{\partial y_1} & \dfrac{\partial f_2}{\partial y_2} \end{pmatrix} \begin{pmatrix} \medskip \dfrac{dy_1}{dx} \\ \medskip \dfrac{dy_2}{dx} \end{pmatrix} = -\begin{pmatrix} \medskip \dfrac{\partial f_1}{\partial x} \\ \medskip \dfrac{\partial f_2}{\partial x} \end{pmatrix}.$$

The matrix on the left is the Jacobian. Consider when there are $n$ equations satisfying $f_i(\vec{x}, \vec{y})=0$, where $\vec{x} \in \bbR^m, \vec{y} \in \bbR^n$.

This produces the matrix system $$\begin{pmatrix} \medskip \dfrac{\partial f_1}{\partial y_1} & \dfrac{\partial f_1}{\partial y_2} & \dots & \dfrac{\partial f_1}{\partial y_n} \\ \medskip \dfrac{\partial f_2}{\partial y_1} & \dfrac{\partial f_2}{\partial y_2} & \dots & \dfrac{\partial f_2}{\partial y_n} \\ \medskip \vdots & \vdots & \vdots & \vdots \\ \medskip \dfrac{\partial f_n}{\partial y_1} & \dfrac{\partial f_n}{\partial y_2} & \dots & \dfrac{\partial f_n}{\partial y_n} \end{pmatrix} \begin{pmatrix} \medskip \dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_1}{\partial x_2} & \dots & \dfrac{\partial y_1}{\partial x_m} \\ \medskip \dfrac{\partial y_2}{\partial x_1} & \dfrac{\partial y_2}{\partial x_2} & \dots & \dfrac{\partial y_2}{\partial x_m} \\ \medskip \vdots & \vdots & \vdots & \vdots \\ \medskip \dfrac{\partial y_n}{\partial x_1} & \dfrac{\partial y_n}{\partial x_2} & \dots & \dfrac{\partial y_n}{\partial x_m} \end{pmatrix} = -\begin{pmatrix} \medskip \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} & \dots & \dfrac{\partial f_1}{\partial x_m} \\ \medskip \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} & \dots & \dfrac{\partial f_2}{\partial x_m} \\ \medskip \vdots & \vdots & \vdots & \vdots \\ \medskip \dfrac{\partial f_n}{\partial x_1} & \dfrac{\partial f_n}{\partial x_2} & \dots & \dfrac{\partial f_n}{\partial x_m} \end{pmatrix}.$$

Notice that the leftmost matrix \tbf{must be square} as the number of equations ($n$) equals the dimension of $\vec{y}=n$. Denote it by the Jacobian, $J$. The remaining matrices are \tbf{gradients} in $\vec{y}$ and $\vec{f}$ respectively since the rows all contain partial derivatives of the same function; yet, the number of functions and the number of partial derivatives taken don't match. 

In short, this is $$\nabla \vec{f} + J \cdot \nabla \vec{y} = \mbf{0},$$ where $\mbf{0}$ is a matrix with zeroes for all its components. Rearrange and we get $$\nabla \vec{y} = -J^{-1} \nabla \vec{f}.$$

The Jacobian also underscores a \emph{change of coordinates}. Given a change from $\{x, y\} \to \{u, v\}$, we have a change of coordinates $ \begin{pmatrix} \medskip \dfrac{\partial f}{\partial u} \\ \medskip \dfrac{\partial f}{\partial v} \end{pmatrix} = |J| \cdot \begin{pmatrix} \medskip \dfrac{\partial f}{\partial x} \\ \medskip \dfrac{\partial f}{\partial y} \end{pmatrix}$. The role of the Jacobian is to transform vectors expressed in one coordinate basis into another coordinate basis.

This is confusing. Let's illustrate this with an example. 

\ex{Cartesian coordinates to polar coordinates}{
	Consider Cartesian coordinates in two dimensions, represented by $(x, y)$. Polar coordinates are represented by $(r, \theta)$, given by the relations \begin{align*} x &= r\cos \theta \\ y &= r \sin \theta. \end{align*}
	
	Then, the Jacobian of the coordinate transformation is \begin{align*} \begin{pmatrix} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}\end{pmatrix} = \begin{pmatrix} \cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta \end{pmatrix}, \end{align*} 
	
	which has determinant $r(cos^2\theta+\sin^2\theta)=r.$
}

Let's work through a famous integral, the \emph{Gaussian integral}, using the coordinate change.

\ex{Gaussian integral}{
	We aim to find $$\int\limits_{-\infty}^\infty e^{-x^2}\ dx.$$
	
	This integral cannot be evaluated using the integration methods we learned in secondary school. Let $I=\int\limits_{-\infty}^\infty e^{-x^2}\ dx$. Then 
	
	\begin{align*} I^2&=\int\limits_{-\infty}^\infty e^{-x^2}\ dx \int\limits_{-\infty}^\infty e^{-y^2}\ dy \\ &= \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty e^{-(x^2+y^2)}\ dx\ dy \end{align*}
	
	Noting that $r^2=x^2+y^2$, we are inspired to change the coordinates. The domains of $x$ and $y$ are $(-\infty, \infty)$ and $(-\infty, \infty)$ respectively, while the domains of $r$ and $\theta$ are $(0, \infty)$ and $(0, 2\pi)$ respectively, so we must change our bounds of integration.
	
	We must also multiply by the Jacobian determinant when changing coordinates. Our infinitesimals $dx\ dy$ become $r\ dr\ d\theta$. Our integral becomes $$I^2=\int\limits_0^{2\pi} \int\limits_{0}^\infty e^{-r^2} r \ dr\ d\theta,$$ which we can then use integration by parts to obtain $$I^2=\pi,$$ or $$I=\sqrt{\pi}.$$
}



\chapter{Week 6}

\section{Eigenvalues and eigenvectors}

Let $M$ be a square matrix where its only non-zero entries are along its diagonal. We call this a \tbf{diagonal matrix}. In three dimensions, this is $D=\begin{pmatrix} a&0&0\\0&b&0\\0&0&c \end{pmatrix}$. You can verify that $D^2 = \begin{pmatrix} a^2&0&0\\0&b^2&0\\0&0&c^2 \end{pmatrix}$, and in general, $$D^n = \begin{pmatrix} a^n&0&0\\0&b^n&0\\0&0&c^n \end{pmatrix}$$ for $n\in \bbZ$. 


Each matrix comes with an unique set of eigenvalues and eigenvectors. 

\defn{Eigenvalues and eigenvectors}{Let $M$ be a matrix, and consider a scalar $\lambda$ and a vector $\vec{v}$ such that \begin{equation} M\vec{v_j}=\lambda_j \vec{v_j} \end{equation} We call $\lambda_j$ the \tbf{eigenvalues} of the matrix, and $\vec{v_j}$ the \tbf{eigenvectors.}

	Equation (6.1) is the \emph{eigenvalue equation.}
}

Let's try solving this. We have that $(M-\lambda\bbI) \vec{v_j} = \vec{0}$, so $M-\lambda\bbI=\vec{0}$ or $\vec{v_j}=0$. We obviously cannot guarantee that $\vec{v_j}=0$, so we have that $$\det (M-\lambda\bbI)=0.$$

\ex{Eigenvalues and eigenvectors in two dimensions}{
	Let $M = \begin{pmatrix} 4 & 1 \\ 3 & 2 \end{pmatrix}.$ The required matrix is $$\begin{pmatrix} 4-\lambda & 1 \\ 3 & 2-\lambda \end{pmatrix}$$ which has determinant $(4-\lambda)(2-\lambda)-3=5-6\lambda+\lambda^2 = (\lambda-5)(\lambda-1)=0.$
	
	Thus, $\lambda=5$ or $\lambda=1$. Let $\vec{v_j}= \begin{pmatrix} v_1 \\ v_2 \end{pmatrix}$. Then $$M\vec{v_j} = \begin{pmatrix} 4v_1+v_2 \\ 3v_1+2v_2 \end{pmatrix}.$$ This is equivalent to $\lambda_j \vec{v_j} = \begin{pmatrix} 5v_1 \\ 5v_2 \end{pmatrix}$ or  $\begin{pmatrix} v_1 \\ v_2 \end{pmatrix}$
	
	Consider $\begin{pmatrix} 4v_1+v_2 \\ 3v_1+2v_2 \end{pmatrix} = \begin{pmatrix} 5v_1 \\ 5v_2 \end{pmatrix}$. Its solutions are $v_1=1, v_2=1$, so the eigenvector associated with $\lambda=5$ is $\begin{pmatrix} 1\\1 \end{pmatrix}$.
	
	$\begin{pmatrix} 4v_1+v_2 \\ 3v_1+2v_2 \end{pmatrix} = \begin{pmatrix} v_1 \\ v_2 \end{pmatrix}$ yields $v_1=-\frac{1}{3}, v_2=1$, so the eigenvector associated with $\lambda=1$ is $\begin{pmatrix} -\frac{1}{3}\\1 \end{pmatrix}$.
	
	To conclude, there are two eigenvalue-eigenvector pairs corresponding to $M$: $\{5, \begin{pmatrix} 1\\1 \end{pmatrix} \}$, and $\{-\frac{1}{3}, \begin{pmatrix} -\frac{1}{3}\\1 \end{pmatrix}\}$.
}

For now, we will discuss the eigenvalues and eigenvectors of real symmetric matrices, which are matrices equivalent to its transpose. 

There are two reasons. Firstly, the Spectral Theorem tells us that every real symmetric matrix is diagonalisable. Secondly, the \tbf{eigenvalues} of real symmetric matrices \tbf{are real}, and the \tbf{eigenvectors are orthogonal} to one another.

We refer to the set of all eigenvalues of $M$ as the \emph{spectrum} of $M$.

\thrm{Eigenvector scaling}{
	Let $\vec{v}$ be an eigenvector. Then, $c\vec{v}$ is also an eigenvector for some scalar $c$.
}

\tbf{Proof:} Let $M$ satisfy $M\vec{v} = \lambda\vec{v}$ for eigenvectors $\vec{v}$ and eigenvalues $\lambda$. Then, $M(c\vec{v}) = c(M\vec{v}) = c(\lambda \vec{v}) = \lambda (c\vec{v}). \qed$

This also means that any \tbf{linear combination of eigenvectors produces eigenvectors}, meaning that $$M(a \vec{v}_i+b\vec{v}_j) =m(a \vec{v}_i+b\vec{v}_j).$$ 

This also means that we can carefully choose constants $a$ and $b$ such that the eigenvectors are \tbf{orthogonal}, then scale them such that they all have length 1. These eigenvectors are \tbf{orthonormal}.

\ex{Orthonormal scaling}{
	Suppose that the eigenvectors of a matrix are, up to scale, $\vec{v_1}=\begin{pmatrix} 3\\5 \end{pmatrix}$ and $\vec{v_2}=\begin{pmatrix} -5\\3 \end{pmatrix}$. Observe that both eigenvectors have magnitude $\sqrt{3^2+5^2}=\sqrt{34}$. 
	
	Thus, the \emph{normalised eigenvectors} are $\hat{v_1}=\begin{pmatrix} \frac{3}{\sqrt{34}}\\ \frac{5}{\sqrt{34}} \end{pmatrix}$ and $\hat{v_2}=\begin{pmatrix} -\frac{5}{\sqrt{34}}\\ \frac{3}{\sqrt{34}} \end{pmatrix}$
}

\coro{Notation}{
	Note that we write normalised vectors using the hat symbol $\hat{v_1}$ instead of the arrow symbol $\vec{v_1}$.
}

\tbf{Question:} Can you construct a matrix with eigenvectors $\begin{pmatrix} 3\\5 \end{pmatrix}$ and $\begin{pmatrix} -5\\3 \end{pmatrix}$?

\subsection{The characteristic polynomial}

Let $M = \begin{pmatrix} m_{11} & m_{12} \\ m_{21} & m_{22} \end{pmatrix}.$ The eigenvalues of $M$ are the solutions to the equation $\det (M-\lambda\bbI)=0,$ or \begin{align*}(m_{11}-\lambda)(m_{22}-\lambda)-m_{21}m_{12}&=0 \\ \lambda^2-(m_{11}+m_{22})\lambda + m_{11}m_{22}-m_{12}m_{21} &= 0 \\ \lambda^2 - \text{Tr}(M)\lambda + \det{M}&=0.\end{align*}

This quadratic polynomial is the \tbf{characteristic polynomial} of $M$.

\defn{Characteristic equation of a matrix}{
	In general, the equation $\det (M-\lambda\bbI)=0$ is the \emph{characteristic equation} of $M$, with its solutions being the eigenvalues $\lambda_i$.
}

\ex{Characteristic equation}{
	Let $M= \begin{pmatrix} 4 & -6 \\ 2 & -2 \end{pmatrix}.$ The characteristic polynomial of $M$ is the determinant of the matrix $$\begin{pmatrix} \lambda-4 & -6 \\ 2 & \lambda+2 \end{pmatrix},$$ which is $(\lambda-4)(\lambda+2)+12 = \lambda^2-2\lambda-4$.
	
	Recall that $\lambda^2 - \text{Tr}(M)\lambda + \det{M}=0$. This means that the trace of $M$ is 2, and the determinant of $M$ is -4.
}

Note what this means in terms of \emph{sums and products of roots}. By Vieta's formulae, the trace of $M$ is the sum of the eigenvalues; the determinant of $M$ is the product of the eigenvalues. This is important.

\begin{align*} \text{Tr}(M) &= \lambda_1+\dots+\lambda_n \\ \det(M) &= \lambda_1\dots\lambda_n \end{align*}

Recall that the determinant of an n-by-n matrix contained $n!$ terms if evaluated using the way learned in Week 4. This is what seems to be an immense simplification. It is non-trivial, for large matrices, to construct the characteristic polynomial, let alone solve it.

\section{Eigendecomposition}

Eigendecomposition is the process of rewriting a matrix $M$ such that powers of $M$ can be evaluated more easily. We commonly have two ways of performing eigendecomposition for symmetric matrices - representing $M=SDS^{-1}$, and writing $M$ in terms of eigenvalues and normalised eigenvectors. We will explore the second method here, and briefly discuss the first method in the appendix to this chapter.

Let $M$ be a real-valued symmetric matrix. Recall that, by the Spectral Theorem, the eigenvectors are orthogonal to one another. If we scale them so that each vector has magnitude one, they form an \tbf{orthonormal basis}. Putting those vectors together, we have an \tbf{orthonormal matrix}.

\defn{Orthonormal matrix}{
	An orthonormal matrix $O$ is a real square matrix in which its rows and columns are orthonormal vectors. Furthermore, the matrix satisfies $OO^T=I$, or $O^T = O^{-1}$. 
	
	Therefore, our orthonormal eigendecomposition becomes $ODO^{T}$ instead of $ODO^{-1}$. This is important as \tbf{transposes of matrices are much easier to calculate than inverses,} especially for large matrices.
}



We perform eigendecomposition in terms of the \tbf{tensor product} of eigenvectors and multiplication by its eigenvalues. Note that the method above gives a \tbf{product of 3 matrices} while the method below gives a \tbf{sum of $n$ matrices}, where $n$ is the number of eigenvalues.

The tensor product method of eigendecomposition makes use of the orthonormal basis.

\defn{Tensor product}{
	Let $\vec{a}$ and $\vec{b}$ be vectors, and represent them in terms of the canonical basis $\vec{a} = a_1\hat{e_1}+a_2\hat{e_2}+\dots+a_n\hat{e_n}$ and $\vec{b} = b_1\hat{e_1}+b_2\hat{e_2}+\dots+b_n\hat{e_n}$.
	
	Then, the tensor product of $\vec{a}$ and $\vec{b}$, denoted $\vec{a} \otimes \vec{b}$, is $$\vec{a} \otimes \vec{b} = \begin{pmatrix} a_1b_1 & \dots & a_1b_n \\ \vdots & \ddots & \vdots \\ a_nb_1 & \dots & a_nb_n \end{pmatrix}.$$
}

\ex{Tensor product}{For example, if $\vec{a}=\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ and $\vec{b}=\begin{pmatrix} -2 \\ 3 \end{pmatrix}$, then $$\vec{a} \otimes \vec{b} = \begin{pmatrix} 2\cdot -2 & 2\cdot 3 \\ 1 \cdot -2 & 1 \cdot 3 \end{pmatrix} = \begin{pmatrix} -4 & 6 \\ -2 & 3 \end{pmatrix}.$$

}

\ex{Hessian, gradient, tensor product}{
	Recall that for a multivariable function $g(x_1, \dots, x_n)$, the Hessian matrix is given by $$H = \begin{pmatrix} \frac{\partial^2 g}{\partial x_1^2} & \dots & \frac{\partial^2 g}{\partial x_1x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial^2 g}{\partial x_nx_1} & \dots & \frac{\partial^2 g}{\partial x_n^2}\end{pmatrix}.$$ 
	
	This is actually the tensor product of the gradient's partial derivatives with itself. How? Well, observe that the gradient of $g$ is the matrix $ \nabla_{\vec{x}} g = \begin{pmatrix} \frac{\partial g}{\partial x_1} \\ \vdots \\ \frac{\partial g}{\partial x_n} \end{pmatrix}.$ 
	
	Observe the term $H_{11} = \frac{\partial^2 g}{\partial x_1^2}$. This is equivalent to $\frac{\partial}{\partial x_1} \frac{\partial }{\partial x_1} g$. Similarly, the term $H_{11} = \frac{\partial^2 g}{\partial x_1x_2}$ is actually $\frac{\partial}{\partial x_1} \frac{\partial }{\partial x_2} g$. Each entry of the Hessian matrix is the tensor product of the gradient's corresponding partial derivatives.
	
	Thus, we can conclude that the Hessian $H(g(\vec{x}))$ satisfies $$H(g(\vec{x})) = \nabla_{\vec{x}} \otimes \nabla_{\vec{x}} g.$$
}

\thrm{Spectral representation theorem}{
	Let $M$ have eigenvalues $\lambda_1, \dots, \lambda_n$ and eigenvectors $\vec{v_1}, \dots, \vec{v_n}.$ We have that $$M = \lambda_1(\vec{v_1} \otimes \vec{v_1}) + \dots + \lambda_n(\vec{v_n} \otimes \vec{v_n}) = \sum\limits_{i=1}^n \lambda_i (\vec{v_i} \otimes \vec{v_i}).$$
	
	We also have an analogue to eigendecomposition for finding higher-power matrices, in the form of $$M^k = \sum\limits_{i=1}^n \lambda_i^k (\vec{v_i} \otimes \vec{v_i}).$$
}

\section{Revisiting the Hessian}

Since Hessians are symmetric matrices, they have real eigenvalues. Recall that the quadratic approximation for a multi-variable function $g(\vec{x})$ contained a term $\overrightarrow{x-x_0} \cdot H(g) \cdot \overrightarrow{x-x_0}$, and that we weren't sure if said term was positive or negative. 

Let $\hat{v_i}$ be the eigenvectors' orthonormal basis. Let $\vec{a}=\overrightarrow{x-x_0}$. Then, \begin{align*} \vec{a} \cdot H \cdot \vec{a} &= (\sum\limits_{i=1}^n a_i \hat{v_i})(\sum\limits_{j=1}^n a_j h_j \hat{v_j})\\ &= \sum\limits_{i=1}^n\sum\limits_{j=1}^n a_i a_j h_j (\hat{v_i} \cdot \hat{v_j}) \end{align*}

Using the fact that the eigenvectors are orthogonal, $\hat{v_i} \cdot \hat{v_j}=1.$ Thus we have that $$\vec{a} \cdot H \cdot \vec{a} = \sum\limits_{k=1}^n a_k^2 h_k.$$ In conclusion, the \tbf{sign of the second-order approximation} depends on the \tbf{signs of the eigenvalues.} This is the classification:

\begin{enumerate}
	\item If the eigenvalues are all positive, then the second-order approximation is positive, and the optima is a local minima. 
	\item If the eigenvalues are all negative, then the second-order approximation is negative, and the optima is a local maxima.
	\item If any of the eigenvalues is zero, the result is inconclusive.
	\item If the eigenvalues are of mixed signs, the optima is a saddle point.
\end{enumerate}

\tbf{Exam tip:} Positive-definite or negative-definite matrices \tbf{must be symmetric.} This may be useful in determining the definiteness of a matrix.

\tbf{Remark:} A posititve-definite symmetric matrix $M$, where $\vec{x} \cdot M \cdot \vec{x}>0$, satisfies the condition $\lambda>0$ for all eigenvalues $\lambda$. This follows from the fact that the second-order approximation being positive directly follows from positive eigenvalues; recall that $\vec{x} \cdot M \cdot \vec{x}$ directly corresponds to the terms quadratic in $x_1, x_2$.

Similarly, a negative-definite matrix satisfies $\lambda<0$ for all eigenvalues $\lambda$. If the eigenvalues are of mixed signs, the function is indefinite.

\ex{}{
	Consider the quadratic form $$f(x, y) = kx^2+xy+ky^2.$$ For which values of $k$ is this quadratic form positive-definite, or negative-definite?
	
	Write $kx^2+xy+ky^2$ as $\begin{pmatrix} x & y \end{pmatrix} \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = ax^2+(b+c)xy+dy^2.$
	
	We can quickly see that $a=d=k$. Using the fact that $\begin{pmatrix} a & b \\ c & d \end{pmatrix}$ is symmetric, we have that $b=c$. Given that $b+c=1$, we have that $b=c=\frac{1}{2}$. Therefore, we have that $$kx^2+xy+ky^2 = \begin{pmatrix} x & y \end{pmatrix}\begin{pmatrix} k & \frac{1}{2} \\ \frac{1}{2} & k \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} .$$
	
	Let's start by considering the values of $k$ that make the quadratic form positive-definite. We now find the conditions on $k$ for the eigenvalues to be positive. This is \begin{align*} (\lambda-k)^2-\left(\frac{1}{2}\right)^2 &=0 \\ \left(\lambda-k-\frac{1}{2}\right)\left(\lambda-k+\frac{1}{2}\right) &= 0, \end{align*}
	
	which implies that $\lambda = k+\frac{1}{2}$ or $\lambda=k-\frac{1}{2}$. We then set both our eigenvalues to be greater than zero, giving us $k+\frac{1}{2} >0$ and $k-\frac{1}{2}>0$, or $k>-\frac{1}{2}$ and $k>\frac{1}{2}$. Therefore, the required interval of $k$ is $k>\frac{1}{2}.$
	
	Similarly, the values of $k$ that make the quadratic form negative-definite satisfy $k < \frac{1}{2}$.
}

\section{What is $e^M$?}

Recall from Week 1 that we could express functions continuous on $\bbR$ in terms of a Taylor series. With our way to quickly compute powers of matrices, we can express $f(M)$ as a Taylor series in $M$.

\ex{$e^M$}{
	Recall that $e^x = \sum\limits_{k=0}^\infty \frac{x^k}{k!}$. So, we can write $e^M=1+M+\frac{M^2}{2}+\frac{M^3}{6}+\dots$. More explicitly,  \begin{align*} e^M &= \sum\limits_{k=0}^\infty \frac{M^k}{k!} \\ &= \sum\limits_{k=0}^\infty \frac{\sum\limits_{i=1}^n \lambda_i^k (\hat{v}_i \otimes \hat{v}_i)}{k!}\\ &= \sum\limits_{i=1}^n \left(\sum\limits_{k=0}^\infty \frac{\lambda_i^k}{k!} \right) (\hat{v}_i \otimes \hat{v}_i)  \\ &= \sum\limits_{k=0}^\infty e^{\lambda_i} (\hat{v}_i \otimes \hat{v}_i). \end{align*}
}  

One can perform this procedure for any function that admits a Taylor series. The above example leads to a very nice result. Observe that \begin{align*} \det(e^M) &= \det\left(\sum\limits_{i=1}^n e^{\lambda_i} (\hat{v}_i \otimes \hat{v}_i)\right) \\ &= \sum\limits_i^n  e^{\lambda_i} \\ &= e^{\sum\limits_{i=1}^n \lambda_i} \\ &= e^{\text{Tr}(M)} \end{align*}

This suggests that $$ \det(e^M) = e^{\text{Tr}(M)},$$ which we can write as $$\log(\det(e^M)) = \text{Tr}(M).$$ Writing $A = e^M$ and thus $\log(A) = M$, we have that $$\log(\det(A)) = \text{Tr}(\log A),$$ meaning that the \tbf{log of the determinant is the trace of the log.}

\section{Appendix: $SDS^{-1}$ diagonalisation}

We explored representing a matrix in terms of eigenvalues and normalised eigenvectors in the form $$M = \sum\limits_{i=1}^n \lambda_i (\hat{v_i} \otimes \hat{v}_i,$$ in which we can quickly calculate functions of said matrix by writing it as a Taylor series and writing $M^k = \sum\limits_{i=1}^n \lambda_i^k (\hat{v_i} \otimes \hat{v}_i.$

A common way of eigendecomposition is to represent $M = PDP^{-1}$. Note that by this, we have that $M^2 = (PDP^{-1})(PDP^{-1})=PD^2(PP^{-1})P^{-1}=PD^2P^{-1}$. 

In general, we have that $$M^n = (PDP^{-1})^n = PD^nP^{-1}.$$ As a result, we can quickly find the powers of the matrix $M$ as it is trivial to compute the integer powers of a diagonal matrix. The process of representing $M=PDP^{-1}$ is called \emph{eigendecomposition}, which we will discuss later.

\defn{Diagonalisable matrix}{
	Let $M$ be a matrix. $M$ is diagonalisable if there exists a matrix $P$ such that $P^{-1}MP$ is a diagonal matrix. Note that \tbf{not all matrices} can be factored into this form. 

	To see this, note that $P^{-1}MP = P^{-1} PDP^{-1}P = D$. 
}

Eigendecomposition is the process of taking a diagonalisable matrix $M$ and factoring it into the form $M=SDS^{-1}$, where $D$ is a diagonal matrix. In this section, $M$ is taken to be real symmetric.

Let $M\vec{v_i} = \lambda_i \vec{v_i}$, where $i= 1, \dots, n$. Then, $M=SDS^{-1}$, where $$S = \begin{pmatrix} \vec{v_1} & \vec{v_2} & \dots & \vec{v_n} \end{pmatrix}$$ and $$D = \begin{pmatrix} \lambda_1 & 0 & \dots & 0 \\ 0 & \lambda_2 & \dots & 0 \\ 0 & 0 & \ddots & 0 \\ 0 & 0 & \dots & \lambda_n. \end{pmatrix}$$

\ex{Eigendecomposition}{
	Consider the matrix $M= \begin{pmatrix} 5 & -1 \\ -1 & 3\end{pmatrix}.$ Its eigenvalues are $\lambda_1, \lambda_2= 4-\sqrt{2}$ and $4+\sqrt{2}$, with eigenvectors $\vec{v_1}=\begin{pmatrix} -1+\sqrt{2} \\ 1 \end{pmatrix}$ and $\vec{v_2}=\begin{pmatrix} -1-\sqrt{2} \\ 1 \end{pmatrix}$. Therefore, $$S = \begin{pmatrix} -1+\sqrt{2} & -1-\sqrt{2}\\ 1 & 1\end{pmatrix}$$ and $$S^{-1} = \begin{pmatrix} \frac{\sqrt{2}}{4} & \frac{2+\sqrt{2}}{4} \\ -\frac{\sqrt{2}}{4} & \frac{2-\sqrt{2}}{4} \end{pmatrix}.$$ Evidently $D = \begin{pmatrix} 4+\sqrt{2} & 0 \\ 0 & 4-\sqrt{2} \end{pmatrix}$, so $$M = SDS^{-1} = \begin{pmatrix} -1+\sqrt{2} & -1-\sqrt{2}\\ 1 & 1\end{pmatrix} \begin{pmatrix} 4+\sqrt{2} & 0 \\ 0 & 4-\sqrt{2} \end{pmatrix} \begin{pmatrix} \frac{\sqrt{2}}{4} & \frac{2+\sqrt{2}}{4} \\ -\frac{\sqrt{2}}{4} & \frac{2-\sqrt{2}}{4} \end{pmatrix}.$$
	
	Notably, we can easily compute large powers of $M$. We have that $$M^n= SDS^{-1} = \begin{pmatrix} -1+\sqrt{2} & -1-\sqrt{2}\\ 1 & 1\end{pmatrix} \begin{pmatrix} (4+\sqrt{2})^n & 0 \\ 0 & (4-\sqrt{2})^n \end{pmatrix} \begin{pmatrix} \frac{\sqrt{2}}{4} & \frac{2+\sqrt{2}}{4} \\ -\frac{\sqrt{2}}{4} & \frac{2-\sqrt{2}}{4} \end{pmatrix}.$$
}

To obtain an orthonormal eigenvector basis, simply divide each eigenvector by its magnitude. The orthonormal eigendecomposition of $M$ is given by $$M = OD^nO^T = \begin{pmatrix} \frac{-\sqrt{2}-1}{\sqrt{2(2+\sqrt{2})}} & \frac{\sqrt{2}-1}{\sqrt{4-2\sqrt{2}}}\\ \frac{1}{\sqrt{2(2+\sqrt{2})}} & \frac{1}{\sqrt{4-2\sqrt{2}}}\end{pmatrix} \begin{pmatrix} 4+\sqrt{2} & 0 \\ 0 & 4-\sqrt{2} \end{pmatrix} \begin{pmatrix} \frac{-\sqrt{2}-1}{\sqrt{2(2+\sqrt{2})}} & \frac{1}{\sqrt{2(2+\sqrt{2})}} \\ \frac{\sqrt{2}-1}{\sqrt{4-2\sqrt{2}}}  & \frac{1}{\sqrt{4-2\sqrt{2}}}\end{pmatrix}.$$

Let's try using a set of eigenvalues and orthonormal eigenvectors, putting them through both methods, and seeing if we obtain the same result. 

\tbf{Tensor product method:} Let $\vec{v_1}, \vec{v_2}, \vec{v_3} = \begin{pmatrix} 0 \\ \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}, \begin{pmatrix} \frac{1}{\sqrt{3}} \\ -\frac{1}{\sqrt{3}} \\ -\frac{1}{\sqrt{3}} \end{pmatrix}, \begin{pmatrix} \frac{\sqrt{2}}{\sqrt{3}} \\ \frac{1}{\sqrt{6}} \\ -\frac{1}{\sqrt{6}} \end{pmatrix}$, and let $\lambda_1, \lambda_2, \lambda_3 = 3, 2, 4.$ 

We have that \begin{align*}T_1=\vec{v_1} \otimes \vec{v_1} &= \begin{pmatrix} 0&0&0 \\ 0&\frac{1}{2}&\frac{1}{2} \\ 0&\frac{1}{2}&\frac{1}{2} \end{pmatrix} \\ T_2=\vec{v_2} \otimes \vec{v_2} &= \begin{pmatrix} \frac{1}{3}&-\frac{1}{3}&\frac{1}{3} \\ -\frac{1}{3}&\frac{1}{3}&-\frac{1}{3} \\ \frac{1}{3}&-\frac{1}{3}&\frac{1}{3} \end{pmatrix} \\ T_3=\vec{v_3} \otimes \vec{v_3} &= \begin{pmatrix}\frac{2}{3}&\frac{1}{3}&-\frac{1}{3} \\ \frac{1}{3}&\frac{1}{6}&-\frac{1}{6} \\ -\frac{1}{3}&-\frac{1}{6}&\frac{1}{6} \end{pmatrix}. \end{align*}

It remains for us to add $$3T_1+2T_2+4T_3= \begin{pmatrix} \medskip \frac{10}{3} & \frac{2}{3}&-\frac{2}{3} \\ \medskip \frac{2}{3}&\frac{17}{6}&\frac{1}{6} \\ \medskip -\frac{2}{3}&\frac{1}{6}&\frac{17}{6}\end{pmatrix}.$$

Evidently, $M^k = 3^kT_1+2^kT_2+4^kT_3$.

\tbf{The $ODO^T$ method}: Simply note that $O = \begin{pmatrix} 0 & \frac{1}{\sqrt{3}} & \frac{\sqrt{2}}{\sqrt{3}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{3}}&\frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{3}}&-\frac{1}{\sqrt{6}} \end{pmatrix}$ and $D=\begin{pmatrix} 3 & 0 &0 \\0&2&0\\0&0&4 \end{pmatrix}$. We swiftly obtain that $$ODO^T=\begin{pmatrix} \medskip \frac{10}{3} & \frac{2}{3}&-\frac{2}{3} \\ \medskip \frac{2}{3}&\frac{17}{6}&\frac{1}{6} \\ \medskip -\frac{2}{3}&\frac{1}{6}&\frac{17}{6}\end{pmatrix}.$$


We also have a way of evaluating $e^M$ with $SDS^{-1}$ decomposition. Note 

 \begin{align*} e^M &= \sum\limits_{k=0}^\infty \frac{M^k}{k!} \\ &= \sum\limits_{k=0}^\infty \frac{SD^kS^{-1}}{k!}\\ &= \sum\limits_{k=0}^\infty S \left( \frac{D^k}{k!} \right) S^{-1} \\ &= Se^DS^{-1}. \end{align*}
  
Let $M=SDS^{-1}$. Then, $e^M = e^{SDS^{-1}}$, and $$\det(e^M) = \det(\sum\limits_{i=1}^n e^{\lambda_i}) = \det{S} \cdot \det{e^D} \cdot \det{S^{-1}} = \det(e^D).$$ R secall that the determinant of a diagonal matrix $D$ is the product of its eigenvalues. Therefore, $$\det(e^M)=\det(e^D) = e^{\lambda_1}\dots e^{\lambda_n} = e^{\lambda_1+\dots+\lambda_n} = e^{\text{Tr}(M)}=e^{\text{Tr}(\log(e^M))}. $$ Taking logarithms of the first and last term, we have that $$\log(\det(e^M)) = \text{Tr}(\log(e^M)).$$

The log of the determinant is the trace of the log!

\chapter{Week 7}

This week's focus is on using methods in linear algebra to solve differential and difference equations.

Differential equations are equations that involve the terms $\frac{dy}{dt}, \frac{d^2y}{dt^2}$ etc. Our goal, by solving differential equation, is to obtain the original function $y=f(t)$ from a combination of the derivatives of $f$. An example of a differential equation is $$\frac{dy}{dt}+4y = 5.$$

Difference equations are the discrete-time analogue to differential equations. We replace the time derivative $\frac{dy}{dt}$ with a difference between periods of time $y_{t+1}$ and $y_t$ - so $\Delta y_t = y_{t+1}-y_t$. For example, $\Delta y_t + 4y_t = 5$, or $$y_{t+1} + 3y_t = 5$$ is a difference equation analogous to the differential equation above.

\section{Difference equations}

Let $t = 0, 1, \dots$ be a sequence of numbers $\{y_t \}$. A \emph{first-order difference equation} is an equation with $y_t$, $y_{t+1}$, and some relationship between the two.

\tbf{Key point:} Difference equations are a \tbf{discrete-time} analogue of differential equations. We will explore this relationship later.

\ex{}{
	An example of a difference equation is \begin{equation} y_{t+1} + ky_t=0. \end{equation}
	
	Note this gives $y_{t+1} = -ky_t$. Given that $y_0=k_0$, we have that $y_1 = -k\cdot k_0, y_2=k^2k_0$, and $$y_n = (-k)^n k_0,$$ which is the solution of the difference equation.
}

Equation (7.1) yields the result $y_{t+1}-y_t=-(k+1)y_t$, or $\Delta y_t = -(k+1)y_t$. $\Delta y_t$ is taken to be infinitesimally small when instead of considering $y_{k+1}$ and $y_k$, we considered $y_{k+\tau}$ and $y_k$ for some infinitesimally small $\tau$.

\defn{Stationary state}{
	Let $$y_{t+1}-y_t = f(y_t, t).$$ A \tbf{stationary state} is a value $y_t=y_s$ such that $f(y_s, t)=0$, or more specifically, $$y_{t+1}-y_t = \Delta y_t=0.$$ 
	
	This means that $y_s=y_{s+1}=y_{s+n}$ for all $n>0.$
}

\defn{Asymptotic state}{
	Again, let $$y_{t+1}-y_t = f(y_t, t).$$ If we have that $y_t$ converges to a certain value $y_\infty$, then we call $y_\infty$ an \tbf{asymptotic state}. The asymptotic state can be informally viewed as the "limit" of the difference equation: $$\lim\limits_{t \to\infty} y_t = y_\infty.$$
}

Consider the simplest type of difference equation: $$y_{t+1}-y_t=ay_t+b,$$ where $b$ and $c$ are constants. We can rewrite the equation as $y_{t+1} = (a+1)y_t+b.$ Let's consider some small values of $y_t$.

\begin{itemize}
	\item $y_1 = (a+1)y_0+b$
	\item $y_2= (a+1)y_1+b = (a+1)((a+1)y_0+b)+b = (a+1)^2y_0 + (a+1)b+b$
	\item $y_3 = (a+1)y_2+b = (a+1)((a+1)((a+1)y_0+b)+b)+b = (a+1)^3y_0 + ((a+1)^2+(a+1)+1)b$.
\end{itemize}

In general, the general solution of $y_t$ is \begin{align*}y_t&= (a+1)^t y_0+ b\sum\limits_{k=0}^{t-1} (a+1)^k\\ &= (a+1)^t y_0+ \frac{b((a+1)^t-1)}{a}\end{align*} by the geometric series formula.

What is the stationary state? It is when $ay_t+b=0$, or the value $s$ where $y_s = -\frac{b}{a}$.

What is the asymptotic state? We need values of $a$ and $b$ where $y_\infty$ exists, or where $\{y_t\}$ is \emph{convergent}. It is not hard to notice that for $a>0$, the expression $\frac{b((1+a)^t-1)}{a}$ tends towards $\infty$; for $a<-2$, the expression alternates in sign but tends towards both $\infty$ and $-\infty$. 

If, rather, we have $-2 < a < 0$, we have that the difference $y_{t+1}-y_t$ tends towards 0. 

\ex{}{
	Consider the difference equation $$5y_{t+1}+4y_t=5,$$ where $y_0=1.$ What is its general solution? We can begin by considering some small values of $t$.
	
	Firstly, we write $y_{t+1}$ in terms of $y_t$. This is $$y_{t+1} = \frac{5-4y_t}{5} = 1-\frac{4}{5}y_t,$$ or $$y_{t+1}-y_t = -\frac{9}{5}y_t+1.$$ We also have that \begin{align*} y_1 &= 1-\frac{4}{5} y_0 \\ y_2 &= 1-\frac{4}{5}(1-\frac{4}{5} y_0) \\ &= (1-\frac{4}{5})+(-\frac{4}{5})^2 y_0 \\ y_3 &= 1-\frac{4}{5}(1-\frac{4}{5}+(-\frac{4}{5})^2 y_0) \\ &= 1+(-\frac{4}{5}) + (-\frac{4}{5})^2 + (-\frac{4}{5})^3 y_0.\end{align*}
	
	In general, we see that the general solution is \begin{align*} y_t &= (-\frac{4}{5})^t y_0 + \sum\limits_{k=0}^n (-\frac{4}{5})^k \\ &= (-\frac{4}{5})^t y_0 + \frac{(-\frac{4}{5})^t-1}{-\frac{9}{5}}.\end{align*}
	
	Is there a stationary state? The stationary state is where $y_{t+1}-y_t=0$ since all values of $y_t$ beyond said stationary state $t$ will yield $y_t$. In this case, it is when $-\frac{9}{5}y_t+1=0$, or $y_t = \frac{5}{9}$. 
	
	Is there an asymptotic state? Note that $-2<-\frac{9}{5}<0$, so the difference $y_{t+1}-y_t$ tends towards 0. Thus, there is an asymptotic state. It is \begin{align*} \lim\limits_{t \to \infty} y_t &= \lim\limits_{t\to\infty} (-\frac{4}{5})^t y_0 + \frac{(-\frac{4}{5})^t-1}{-\frac{9}{5}} \\ &= \frac{-1}{-\frac{9}{5}} \\ &= \frac{5}{9}. \end{align*}
}

\section{Systems of difference equations}

Let's consider when we have a system of difference equations $\vec{y}_{t+1}-\vec{y_t}=A\vec{y_t}+\vec{b}$ or $$\vec{y}_{t+1}=(A+\bbI)\vec{y_t}+\vec{b},$$ where $A$ is a matrix. 

How does this yield a system of differential equations? Let $\vec{y_t} = \begin{pmatrix} y_{1, t} \\ y_{2, t} \end{pmatrix}.$ Observe that for two variables, we have \begin{align*} \begin{pmatrix} y_{1, t+1} \\ y_{2, t+1} \end{pmatrix}  &= \begin{pmatrix} a_{11}+1 & a_{12} \\ a_{21} & a_{22}+1 \end{pmatrix} \begin{pmatrix} y_{1, t} \\ y_{2, t} \end{pmatrix} + \begin{pmatrix} b_1 \\ b_2 \end{pmatrix} \\ &=\begin{pmatrix} (a_{11}+1)y_{1, t}+a_{12} y_{2, t}+b_1 \\ a_{21}y_{1, t}+(a_{22}+1)y_{2, t} +b_2\end{pmatrix},\end{align*}

or rather \begin{align*} y_{1, t+1} &= (a_{11}+1)y_{1, t} + a_{12} y_{2, t} + b_1 \\ y_{2, t+1} &= a_{21}y_{1, t} + (a_{22}+1) y_{2, t}+b_2. \end{align*}

Look at the general solutions first. We can translate $$y_t =(a+1)^t y_0+ \frac{b((a+1)^t-1)}{a} = (a+1)^t y_0+ a^{-1}b((a+1)^t-1)$$ into our matrix-vector form. This includes converting constants $a$ into matrices $A$ and converting 1's into the identity matrix.

This yields \begin{align*} \vec{y_t} &= (A+\bbI)^t \vec{y_0} +  \vec{b}\left(\sum\limits_{j=0}^k (A+\bbI)^j \right) \\ &= (A+\bbI)^t \vec{y_0} + A^{-1}\vec{b}((A+\bbI)^t-\bbI) \end{align*}

Since we have to compute high powers of matrices, let's utilise eigendecomposition on $A+\bbI$ to obtain $\vec{y}_{t+1}=(SDS^{-1})\vec{y_t}+\vec{b}$, where the matrix $D$ has eigenvalues $a_i+1$ on its diagonal.

The stationary state is easy to calculate. Let the difference equation be stationary for $t>s.$ The stationary condition satisfies $A \vec{y_s} + \vec{b}=\vec{0}$, or $$\vec{y_s} = -A^{-1} \cdot \vec{b}.$$

The asymptotic state, meanwhile, utilises the fact that the eigenvalues are $a_j+1$; we can replace $A+\bbI$ with $a_j+1$. Explicitly, we have that $$\vec{y_t} = SD^tS^{-1}\vec{y_0} + A^{-1}\vec{b}(SD^tS^{-1}-\bbI)$$ where $D^t = \begin{pmatrix} (a_1+1)^t & \dots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & (a_n+1)^t \end{pmatrix}$ and the asymptotic state is just $$\lim\limits_{t\to\infty} \vec{y_t} = \lim\limits_{t\to\infty} SD^tS^{-1}\vec{y_0} + A^{-1}\vec{b}(SD^tS^{-1}-\bbI).$$

Similarly, we have that deviations from the stationary state will decline towards 0 if $-2<a_i<0$ for all eigenvalues $a_i$. 

Using Frank's tensor-product method, we obtain that $$(A+\bbI)^t = \sum\limits_{j=1}^n (a_j+1)^t (\hat{a_j} \otimes \hat{a_j}),$$ which means that we obtain the same result as with eigendecomposition.

In tensor product notation, the asymptotic states are given by \begin{align*} \lim\limits_{t\to\infty} \vec{y}_t &=\lim\limits_{t\to\infty} (A+\bbI)^t \vec{y_0}+A^{-1}\vec{b}((A+\bbI)^t-\bbI) \\ &= \lim\limits_{t\to\infty} \left(\sum\limits_{j=1}^n (a_j+1)^k (\hat{a_j} \otimes\hat{a_j}) \right) \cdot \vec{y_0}+ \left(\sum\limits_{j=1}^n a_j^{-1} (\hat{a_j} \otimes \hat{a_j})\right) \cdot \vec{b} \left(\sum\limits_{j=1}^n (a_j+1)^t (\hat{a_j} \otimes\hat{a_j}) -\bbI\right). \end{align*}

where the sum $\sum\limits_{j=1}^n$ sums over all $n$ eigenvectors $\hat{a_j}$ and eigenvalues $a_j$. Note that here we used that $$A^{-1}=\sum\limits_{j=1}^n a_j^{-1} (\hat{a_j} \otimes \hat{a_j});$$ here we just replaced $A+\bbI$ with $A^{-1}$ and made suitable adjustments to the eigenvalues. 



\ex{System of difference equations}{
	Consider the system of difference equations $$\begin{pmatrix} x_{t+1} \\ y_{t+1} \end{pmatrix} = \begin{pmatrix} 4 & -1 \\ 2 & 1 \end{pmatrix} \begin{pmatrix}x_t \\ y_t \end{pmatrix}+ \begin{pmatrix} 1 \\ 2 \end{pmatrix}.$$
	
	We first begin by diagonalising the matrix $\begin{pmatrix} 4 & -1 \\ 2 & 1 \end{pmatrix}$. Performing eigendecomposition, we know that $$\begin{pmatrix} 4 & -1 \\ 2 & 1 \end{pmatrix}^t = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}\begin{pmatrix} 3^t & 0 \\ 0 & 2^t \end{pmatrix}\begin{pmatrix} 2 & -1 \\ -1 & 1 \end{pmatrix} = \begin{pmatrix} 2\cdot 3^t -2^t & 2^t-3^t \\ 2\cdot 3^t-2^{t+1} & 2^{t+1}-3^t \end{pmatrix}.$$
	
	Since $A+\bbI = \begin{pmatrix} 4 & -1 \\ 2&1 \end{pmatrix}, A= \begin{pmatrix} 3 & -1 \\ 2&0 \end{pmatrix}$, and $A^{-1} = \begin{pmatrix} 0 & \frac{1}{2} \\ -1 & \frac{3}{2} \end{pmatrix}$. Additionally, $A^{-1}\vec{b} = \begin{pmatrix} 0 & \frac{1}{2} \\ -1 & \frac{3}{2} \end{pmatrix}\begin{pmatrix}1\\2\end{pmatrix}=\begin{pmatrix}1\\2\end{pmatrix}$.
	
	Let $\vec{y_0} = \begin{pmatrix} x_0 \\ y_0 \end{pmatrix}$.  Then, \begin{align*} \vec{y_t} &= \begin{pmatrix} 2\cdot 3^t -2^t & 2^t-3^t \\ 2\cdot 3^t-2^{t+1} & 2^{t+1}-3^t \end{pmatrix} \begin{pmatrix} x_0 \\ y_0 \end{pmatrix} + \begin{pmatrix} 1 \\ 2 \end{pmatrix} \begin{pmatrix} 2\cdot 3^t -2^t-1 & 2^t-3^t \\ 2\cdot 3^t-2^{t+1} & 2^{t+1}-3^t-1 \end{pmatrix} \\ &= \begin{pmatrix} 2\cdot 3^t -2^t & 2^t-3^t \\ 2\cdot 3^t-2^{t+1} & 2^{t+1}-3^t \end{pmatrix} \begin{pmatrix} x_0\\y_0 \end{pmatrix}+ \begin{pmatrix} 2^t-1 \\ 2^{t+1}-2 \end{pmatrix}. \end{align*} 
}

\defn{Periodic states}{
	Let $y_{t+1}-y_t = f(y, t)$ be a difference equation. If the condition $y_t = y_{t+\tau}$ for some $\tau>1$ holds for any $t$, the solution to this equation is periodic of period $\tau$.
}

\ex{}{
	Consider the difference equation $$y_{t+1}=ky_t,$$ where $k=-1$ and $y_0=3$. It is not hard to see that for integer $k$, it holds that $y_{2k} = 3$ and $y_{2k+1}=-3$. Thus, the solution to this equation is periodic of period 2.
	
	Periodicity suggests that the difference equation \tbf{converges to two separate values}.
}

With a series of difference equations, we can explore the possibility of periodic solutions in a more systematic way. Our construction of this systematic way shows that this only holds when $A$ has complex eigenvalues, meaning that $A$ would not have periodic solutions if it is symmetric.

Assume we have complex eigenvalues, and write them all in polar form: $a_j+1 = R_je^{i \omega_j}.$ In our representation $\vec{y_t} = SD^tS^{-1}\vec{y_0} + A^{-1}\vec{b}(SD^tS^{-1}-\bbI)$, we have that $$D^t = \begin{pmatrix} R_1^te^{i\omega_1t} & \dots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & R_n^te^{i\omega_nt} \end{pmatrix}.$$

Note that $e^{it}$ is periodic of period $2\pi$. Therefore, $e^{i\omega_jt}$, for $j = 1, \dots, n$, is periodic of period $\frac{2\pi}{\omega_j}$. Let $\tau=\frac{2\pi}{w_j}$. 

We note that the system $\vec{y_t}$ at time $t$ is equivalent to the system at time $t=t+\tau$ as $e^{iw_jt} = e^{iw_j(t+\frac{2\pi}{w_j})}.$ This creates a periodic state of period $\frac{2\pi}{\omega_j}$.

What if the systems of equations were not linear? We can perform linearisation on the set of first-order difference equations. Recall that in two variables, a set of functions $$f_1(x_0 + \delta x, y_0+\delta y) = f_1(x_0, y_0) + \frac{\partial f_1}{\partial x_0} \delta x + \frac{\partial f_1}{\partial y_0} \Delta y $$ $$f_2(x_0 + \delta x, y_0+\delta y) = f_2(x_0, y_0) + \frac{\partial f_2}{\partial x_0} \delta x + \frac{\partial f_2}{\partial y_0} \delta y $$

produces the Jacobian matrix system $$\vec{f}(\vec{x}+\vec{\delta x}) = \vec{f}(\vec{x}) + J_{\vec{f}} \cdot \nabla \vec{\delta x}. $$

Let's explore this a bit further.

\subsection{Non-linear systems of difference equations}

Suppose that we had a non-linear system of difference equations. This means that $$\vec{y}_{t+1}-\vec{y}_t = \vec{f}(\vec{y}_t).$$ 

We can then perform linearisation around a stationary state $\vec{x}_s$. Let $$\vec{x}_t = \vec{x}_s + \vec{\delta x}_t.$$ Then, \begin{align*} \vec{x}_{t+1}-\vec{x}_t &= (\vec{x}_s + \vec{\delta x}_{t+1}) - (\vec{x}_s + \vec{\delta x}_t)\\&=\vec{\delta x}_{t+1}-\vec{\delta x}_t \\ &= \vec{f}(\vec{x}_t) \\ &= \vec{f}(\vec{x}_s + \vec{\delta x}_t).\end{align*}

Consider the system given by $\vec{\delta x}_{t+1}-\vec{\delta x}_t = \vec{f}(\vec{x}_s + \vec{\delta x}_t)$. We can approximate $ \vec{f}(\vec{x}_s + \vec{\delta x}_t)$ by $\vec{\delta x}_t \cdot J_{\vec{f}}$, where the Jacobian matrix $J_{\vec{f}}$ is given by $$ \begin{pmatrix} \frac{\partial f_1(\vec{x}_s)}{\partial x_{1, s}} & \dots & \frac{\partial f_n(\vec{x}_s)}{\partial x_{1, s}}\\ \vdots & \ddots & \vdots \\ \frac{\partial f_1(\vec{x}_s)}{\partial x_{n, s}} & \dots & \frac{\partial f_n(\vec{x}_s)}{\partial x_{n, s}}\end{pmatrix}$$

Note that $\partial x_{1, s}$ is the partial derivative with respect to the variable $x_1$ evaluated at the stationary state.

Therefore, $$\vec{\delta x}_{t+1}-\vec{\delta x}_t = \vec{\delta x}_t \cdot J_{\vec{f}}.$$ 

This is similar to our explorations of the Jacobian matrix in Week 5, only in the context of difference equations. 

\subsection{Phase portraits and qualitative behaviour}

Suppose we have a system of difference equations $$\vec{y}_{t+1}-\vec{y}_t = A\vec{y}_t + \vec{b}.$$ The stationary state is $\vec{y}_s = -A^{-1} \cdot \vec{b}$. Let's analyse the stability of this general solution as $t\to\infty$. This is an analysis of the \tbf{stability of the stationary solution} in a graphical manner.

\coro{}{
	The stability of the solutions as $t\to\infty$ depends on the eigenvalues of the matrix $A$.
}

In what follows, we consider the case where $A$ is a 2-by-2 matrix. Here is a diagram delineating six separate cases.

\includegraphics[scale=0.6]{eigen1.png}

\tbf{Case 1: Both real eigenvalues positive.} In this case, the vector field features vectors that emanate away from the stationary state.

\tbf{Case 2: Both real eigenvalues negative.} The vector field features vectors that point towards from the stationary state.

\tbf{Case 3: Real eigenvalues of opposite signs.} By now you've seen that positive eigenvalues are associated with arrows going away from the stationary state, and negative eigenvalues are associated with arrows tending towards the stationary state. If the eigenvalues are of opposite signs, a saddle point is created where on one axis, the arrows tend towards the stationary state; on another axis, the arrows tend away from the stationary state.

\tbf{Case 4: Complex eigenvalues, positive real part.} The diagram features vectors that spiral away from the stationary state. Notice that there is no distinction between whether the imaginary part is positive, negative, or zero.

\tbf{Case 5: Complex eigenvalues, negative real part.} Here, we have vectors that spiral towards the stationary state. Notice that again, there is no distinction between whether the imaginary part is positive, negative, or zero.

\tbf{Case 6: Purely imaginary eigenvalues.} If the eigenvalues are purely imaginary, the vector field of vectors that are in concentric circles around the stationary state.

\ex{}{
	Suppose the system of difference equations was $$\vec{y}_{t+1}-\vec{y}_t = \begin{pmatrix} 1 & -8 \\ 2 & 0 \end{pmatrix} \vec{y}_t+ \begin{pmatrix} 3\\-2 \end{pmatrix}.$$
	
	The matrix's characteristic equation is $(\lambda-1)\lambda+16 = \lambda^2-\lambda-16$, which have the roots $\frac{1\pm \sqrt{65}}{2}.$ Note that if $M=\begin{pmatrix} 1 & -8 \\ 2 & 0 \end{pmatrix}$, then $M^{-1}= \begin{pmatrix} 0 & \frac{1}{2} \\ -\frac{1}{8} & \frac{1}{16} \end{pmatrix}$. Thus, the stationary state $$-\begin{pmatrix} 0 & \frac{1}{2} \\ -\frac{1}{8} & \frac{1}{16} \end{pmatrix} \cdot \begin{pmatrix} 3\\-2 \end{pmatrix} = \begin{pmatrix} -1 \\ -\frac{1}{2} \end{pmatrix}$$ is a saddle point, with vectors approaching the stationary point in one axis and repelling away from it in the other axis.
}


\subsection{Difference vs differential equations}

Recall, in the beginning of this section, how we discussed that $\Delta y_t$ is infinitesimally small if we considered $y_{t+\tau}-y_t$ for some tiny $\tau$. Subsequently, we would replace $f(y_t, t)$ with a much tinier $\tau f(y_t, t)$. Explicitly, we have that $$\lim\limits_{\tau \to 0} \frac{y_{t+\tau}-y_t}{\tau} = \frac{dy}{dt} = f(t, y).$$ which shows how difference and differential equations are linked. 

\section{Gradient descent and ascent}

Gradient descent and ascent are ways of calculating the minima and maxima of a multivariate function using an iterative method. In ECON0006/0010, we term this process \emph{gradient learning}.

Woah. Let's slow down. Suppose we want to guess the minima of a multivariate function $f(\vec{x})$. One can start with a guess $\vec{x}_1$. Suppose that our initial guess was wrong. Since the gradient points in the directions of the partial derivative, we know that $f(\vec{x})$ decreases the fastest in the direction of the negative gradient of $\vec{x_1}$, or $-\nabla f(\vec{x_1})$. We make further guesses in the direction of $-\nabla f(\vec{x_1})$.

Consider the sequence $\{\vec{x}_n\}$ given by $$\vec{x}_{n+1} = \vec{x}_n - \alpha_n \nabla f(\vec{x}_n)$$ for $n\geq 1$, and $\alpha_n$ being a set of step sizes dictating how far we increment $\vec{x}_i$ in the direction of $-\nabla f(\vec{x_i})$ for some $1 \leq i \leq n$. Our further guesses are $\vec{x_2}, \vec{x_3}, \dots$.

In the limit as $n \to \infty$, we have that $\{\vec{x}_n\}$ approaches the local minimum. 

This is, evidently, a difference equation of the form $$\vec{x}_{n+1} - \vec{x}_n = - \alpha_n \nabla f(\vec{x}_n).$$

If we wanted to find the local maxima of a multivariate function, we would instead use $$\vec{x}_{n+1} - \vec{x}_n = \alpha_n \nabla f(\vec{x}_n),$$ in which the asymptotic state gives us the local optima as it dictates where $\vec{x}_n$ converges.

Consider a gradient learning problem containing a bivariate function $f(x, y)$. What happens when we try to linearise it along a stationary state $\vec{x}_s$, or to find solutions to $\vec{\delta x}_{t+1}-\vec{\delta x}_t = \vec{f}(\vec{x}_s + \vec{\delta x}_t)$?

The system of difference equations contain two functions $\frac{\partial f(x_s, y_s)}{\partial x_s}$ and $\frac{\partial f(x_s, y_s)}{\partial y_s}$, meaning that the Jacobian matrix is $$\begin{pmatrix} \frac{\partial}{\partial x_s} \frac{\partial}{\partial x_s} f(\vec{x}_s)& \frac{\partial}{\partial x_s} \frac{\partial}{\partial y_s} f(\vec{x}_s) \\ \frac{\partial}{\partial y_s} \frac{\partial}{\partial x_s} f(\vec{x}_s) & \frac{\partial}{\partial y_s} \frac{\partial}{\partial y_s} f(\vec{x}_s)\end{pmatrix} = \begin{pmatrix} \frac{\partial^2}{\partial x_s^2}  f(\vec{x}_s)& \frac{\partial^2}{\partial x_s y_s} f(\vec{x}_s) \\ \frac{\partial^2}{\partial y_sx_s} f(\vec{x}_s) & \frac{\partial^2}{\partial y_s^2} f(\vec{x}_s)\end{pmatrix}, $$ which is a Hessian matrix! This occurs because the Jacobian of the gradient is the Hessian.



\section{Appendix: Differential equations}

\tbf{This is not exam content!}

Frank doesn't cover explicit ways of solving differential equations in his lectures. They're just here for completeness. Additionally, most of you should have studied differential equations in some fashion in high school. 

In this section, we explore the ways of solving \emph{first-order differential equations}, meaning differential equations that only involve the first derivative of a function. 

The following do not outline a surefire way to solve every type of differential equation. It merely outlines some tools to solve various classes of equations. There is no general solution for solving equations of the form $\frac{dy}{dx}=f(x, y).$

\subsection{Variable-separable differential equations}

\defn{Variable-separable differential equation}{
	A variable-separable differential equation is one of the form $$\frac{dy}{dt} = \frac{f(t)}{g(y)}.$$ One solves this by "separating the variables": in other words, obtaining $$\frac{1}{g(y)} \frac{dy}{dt} = f(t),$$ or $$\frac{1}{g(y)} dy = f(t) dt,$$ and integrating both sides.
}

\ex{}{
	Consider the differential equation $$\frac{dy}{dt} = \sin t\ y^2.$$ One rearranges to get $y^{-2}\ dy = \sin t\ dt$, or $-y^{-1} = -\cos t + C$, or $$y=\frac{1}{\cos t} +C,$$ where $C$ is a constant.
}

We are usually given an initial condition $(t=t_0, y=y_0)$. Let's assume that when $t=0$, then $y=2$. This gives $C=1$. 

\ex{}{
	Consider the logistic differential equation $\frac{1}{y}\frac{dy}{dt} = a-by$ with first-order conditions $t=0, y=y_0$. We use the fact that $$\frac{1}{y(a-by)} = \frac{1}{a}\left(\frac{1}{y}+\frac{b}{a-by}\right)$$ to obtain $$\frac{1}{a}\left(\frac{1}{y}+\frac{b}{a-by}\right)\ dy = dt.$$ This gives \begin{align*} \ln(y) -\ln(by-a) &= at+C_0 \\ \frac{y}{by-a} &= C_1e^{at} \\ b-\frac{a}{y} &= C_2e^{-at} \\  y&=\frac{a}{b+C_2e^{-at}}\end{align*}
	
	Applying the first-order conditions $t=0, y=y_0$, we see that $C_2=\frac{a}{y_0-b}.$
}

\subsection{Homogeneous differential equations}

\defn{Homogeneous differential equation}{Homogeneous differential equations are differential equations of the form $$f(x, y) \frac{dy}{dx} + g(x, y)=0,$$ where $f(x, y)$ and $g(x, y)$ are of the same degree.
}

The fact that $f$ and $g$ are of the same degree show that they are homogeneous functions of each other, meaning $f(\lambda x, \lambda y) = \lambda^n f(x, y)$ for some constant $n$. Our next step is to substitute $y=vx$, and thus $\frac{dy}{dx} = v+\frac{dv}{dx} x$.

\ex{}{
	Consider the differential equation $(x^3+3y^2x) \frac{dy}{dx} + 5x^2y-3y^3=0.$ By substituting $y=vx$, we get that $$(x^3+3v^2x^3) \frac{dy}{dx} + 5vx^3-3v^3x^3=0,$$ or \begin{align*} (1+3v^2) \frac{dy}{dx} + 5v-3v^3&=0 \\ v+\frac{dv}{dx} x &= \frac{3v^3-5v}{1+3v^2} \\ x \frac{dv}{dx} &= \frac{3v^3-3v^3-5v-v}{1+3v^2} \\ x \frac{dv}{dx} &= \frac{-6v}{1+3v^2} \\ \frac{1+3v^2}{-6v}\ dv &= \frac{1}{x}\ dx \\ -\frac{1}{6}\ln\left(\frac{y}{x}\right)-\frac{1}{4}\left(\frac{y^2}{x^2} \right)&= \ln(x) + C \\ \end{align*} and one can rearrange and solve.
}

The key point is that we use the substitution to turn the homogeneous differential equation into a variable-separable one, and then use the method outlined above.

\subsection{Exact differential equations}

\defn{Exact differential equation}{Exact differential equations are differential equations of the form $$f(x, y) \frac{dy}{dx} + g(x, y)=0$$ where there exists a function $P(x, y)$ such that $$\frac{\partial P}{\partial x}= f(x, y),$$ and $$\frac{\partial P}{\partial y}=g(x, y).$$
}

Our goal here is to find a function $P(x, y)$ satisfying the above conditions. In this case, we can write $$\frac{\partial P}{\partial x} \left( \frac{dx}{dx} \right)+\frac{\partial P}{\partial y}\frac{dy}{dx}=0.$$ This should remind you of the total derivative. In other words, we have that $$\frac{d}{dx} P(x, y) =0,$$ or $$P(x,y)=c$$ for some constant $c$.

We also have to check if $\frac{\partial^2 P}{\partial x \partial y} = \frac{\partial^2 P}{\partial y \partial x}$.

Another name for the total derivative is the \tbf{exact derivative} - which gives the differential equation its name.

\ex{}{
	Let $P(x, y) = x^3y^2+2y$ with the initial condition $y(1)=2$. Thus, $\frac{\partial P}{\partial x} = 3y^2x^2$, and $\frac{\partial P}{\partial y} = 2x^3y+2.$ We also see that $\frac{\partial^2 P}{\partial x \partial y} = \frac{\partial^2 P}{\partial y \partial x}=6yx^2.$ Therefore, the differential equation $$3y^2x^2 \frac{dy}{dx} + 2x^3y+2=0$$ is exact. 
	
	But what if we're only given the equation $3y^2x^2 \frac{dy}{dx} + 2x^3y+2=0$, and want to find $P(x, y)?$ This is no problem. Since we know that $\frac{\partial f}{\partial y} = \frac{\partial g}{\partial x}$, we can find $P$ either by finding $\int f(x, y)\ dx$ or $\int g(x, y)\ dy$. 
	
	Let's try integrating $\int 3y^2x^2\ dx$. This gives us $y^2x^3 + h(y)$, where $h(y)$ is a function in $y$ analogous to the constant of integration for single-variable calculus. We use $h(y)$ because the partial derivative of any function $h(y)$ with respect to $x$ is 0. 
	
	We find $h(y)$ by differentiating $P(x, y)=y^2x^3+h(y)$ with respect to $y$ and setting it equal to $g(x, y)$. In this case, we have that $\frac{\partial P}{\partial y} = 2x^3y+h'(y)$, meaning that $h'(y)=2$, and $h(y)=2y+C.$ Thus, we recovered the function $$P(x, y)=y^2x^3+2y+C.$$ We can drop the constant $C$ as the solution to this differential equation is $P(x, y)=c$, and $C$ is absorbed into $c$.
	
	Substituting the initial condition yields $c=8$, and so $y^2x^3+2y-8=0$, or $$y=\frac{-2\pm\sqrt{4+32x^3}}{2x^3}.$$ Using $(x, y)=(1, 2)$, we eliminate the negative solution, and conclude that the solution is $$y=\frac{-2+\sqrt{4+32x^3}}{2x^3}.$$
}

\subsection{Integrating factor differential equations}

\defn{Integrating factor differential equation}{
	Integrating factor differential equations are equations of the form $$\frac{dy}{dx}+f(x)y=g(x).$$ Our goal is to find a function $h(x)$ such that $h(x)f(x) = \frac{dh}{dx}$. Multiplying both sides by $h(x)$, we have that $$h(x) \frac{dy}{dx} + h(x)f(x) y = h(x)g(x)$$ and thus $$h(x) \frac{dy}{dx} + \frac{dh}{dx}y = h(x)g(x),$$ or by the product rule, $$(h(x)y)' = h(x)g(x),$$ and one can integrate both sides to obtain our solution.
	
	The function $h(x)$ is called the \tbf{integrating factor}, and is $e^{\int f(x)\ dx}$.
	
	
}

\ex{}{
	Consider $$\frac{dy}{dx} + \frac{3y}{x} = 4x.$$ The integrating factor is $e^{\int \frac{3}{x}\ dx} = e^{3\ln x}=x^3$, so \begin{align*} x^3 \frac{dy}{dx}+x^3\frac{3y}{x} &= 4x^4 \\ (x^3y)' &= 4x^4 \\ x^3y &= \frac{4}{5}x^5+C \\ y&=\frac{4}{5}x^2+Cx^{-3}. \end{align*} 
}

\subsection{Bernoulli differential equation}

\defn{}{A Bernoulli differential equation is an equation of the type $$\frac{dy}{dx} + f(x)y = g(x) y^\alpha,$$ where $\alpha$ is a constant.} Note that if $\alpha=1$, we obtain a variable-separable equation; if $\alpha=0$, we obtain an integrating-factor differential equation. The approach to solving this equation is non-trivial.

Begin by multiplying both sides by $(1-a)y^{-\alpha}$. We get that $$(1-\alpha)y^{-\alpha} \frac{dy}{dx} + (1-\alpha) f(x) y^{1-\alpha} = (1-a)g(x).$$

Let $u=y^{1-\alpha}$. Then, $\frac{du}{dx} = (1-\alpha)y^{-\alpha} \frac{dy}{dx}$, meaning that $$\frac{du}{dx} + (1-\alpha) f(x)u = (1-\alpha)g(x),$$

where this becomes an integrating-factor differential equation and one can solve as per the method above.

\ex{}{
	Let $$\frac{dy}{dx}+x^4y=x^4y^5.$$ Then, we have that \begin{align*} -4y^{-5}(\frac{dy}{dx}+x^4y) &= -4y^{-5}(x^4y^5) \\ -4y^{-5} \frac{dy}{dx} - 4x^4y^{-4} &= -4x^4. \end{align*} Let $u=y^{-4}$. Therefore, $\frac{du}{dx} = -4y^{-5} \frac{dy}{dx}$. The equation above becomes $$\frac{du}{dx} - 4x^4u = -4x^4,$$ or $\frac{du}{dx} = (u-1) 4x^4$. Luckily this is a variable-separable equation (we made $f(x)$ and $g(x)$ the same; otherwise we would use the integrating-factor method), which gives us \begin{align*} \frac{1}{u-1}\ du &= 4x^4\ dx \\ \ln(u-1) &= \frac{4}{5}x^5 \\ u&=1+e^{\frac{4}{5}x^5} \\ y^{-4}&=1+e^{\frac{4}{5}x^5} \\ y&=(1+e^{\frac{4}{5}x^5})^{-\frac{1}{4}}.\end{align*}
}

% solow swan

\chapter{Week 8}

This week's focus is on introducing the theory of constrained optimisation; in particular, the theory of Lagrange multipliers.

Until now, we've dealt with unconstrained optimisation. This is where we maximise or minimise a function on $\bbR^n$. This week will discuss \emph{constrained optimisation}, where functions are optimised subject to a constraint.

When performing constrained optimisation, we aim to find optima of a function $$f(x, y)\ \text{under the condition}\ g(x, y)=0.$$ The main method used in constrained optimisation is the method of Lagrange multipliers, but there are other methods to be discussed later.

Sometimes, our functions $f(x, y)$ and $g(x, y)$ are nice enough that we can directly write $g(x, y)$ in terms of one variable, and substitute it into $f(x, y)$. In many cases, the constraints that we work with are less nice, and direct optimisation becomes more tedious than the method of Lagrange multipliers.

\ex{Direct optimisation}{
	Let's consider $f(x, y) = x^{\frac{1}{2}}y^{\frac{1}{3}}$ subject to the constraint $2x+y=10.$ Our constraint is very simple. Economists might realise that this is akin to the maximisation of a utility function constrained by a certain budget constraint.
	
	Substitute $y=10-2x$ into $f(x, y)$ to yield $$f(x) = x^{\frac{1}{2}}(10-2x)^{\frac{1}{3}},$$ which is maximised at $x=3, y=2.75$. Thus $f(x, y)$ attains its maximum at 2.75.
	
	We can use the second-derivative test to indeed verify that this is a local maximum.
}

\section{Method of Lagrange multipliers}

In the example above, we wrote $g(x, y)=0$ as a function $y=h(x)$. 

Perform total differentiation on $f(x, h(x))$ to obtain $$\frac{df}{dx} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} h'(x).$$ However note that by implicit differentiation, we have that $$h'(x) = \frac{-\frac{\partial g}{\partial x}}{\frac{\partial g}{\partial y}}.$$

Our first-order condition is that $\frac{df}{dx}=0$. Therefore, $$\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\left(-\frac{\partial g}{\partial x}\right)\left(\frac{\partial g}{\partial y}\right)^{-1}=0.$$

Let $$\lambda = \frac{\partial f}{\partial y} / \frac{\partial g}{\partial y}.$$ Then, we have that $$\frac{\partial f}{\partial x}-\lambda \frac{\partial g}{\partial x}=0$$

Similarly, letting $\lambda = \frac{\partial f}{\partial y} / \frac{\partial g}{\partial y},$ we get that $\frac{\partial g}{\partial y} \lambda = \frac{\partial f}{\partial y}$, or $$ \frac{\partial f}{\partial y} - \lambda  \frac{\partial g}{\partial y} =0. $$

\tbf{Key point:} We want a function that yields our first-order conditions when differentiated with respect to both $x$ and $y$. This is the \tbf{Lagrangian function}. 

\defn{Lagrangian}{
	We introduce the function $$\mcL(x, y, \lambda) = f(x, y) - \lambda g(x, y).$$ Call $\mcL$ the Lagrangian function and $\lambda$ the Lagrange multiplier.
	
	Why this function? Our first-order conditions are \begin{equation} \frac{\partial \mcL}{\partial x} = \frac{\partial f}{\partial x}-\lambda \frac{\partial g}{\partial x}=0, \end{equation} and \begin{equation} \frac{\partial \mcL}{\partial y} = \frac{\partial f}{\partial y}-\lambda \frac{\partial g}{\partial y}=0. \end{equation}
	
	We also utilise the fact that \begin{equation} g(x, y)=0, \end{equation} by definition. This is equivalent to $\frac{\partial \mcL}{\partial \lambda}=0$. Here we have 3 equations in 3 variables, $x, y, \lambda$. We solve for all three variables and our work is done.
	
	\tbf{Summary:} Solve $\frac{\partial \mcL}{\partial x}=0, \frac{\partial \mcL}{\partial y}=0$, and $\frac{\partial \mcL}{\partial \lambda}=g(x, y)=0$.
	
	\coro{}{The fact that we want $\frac{\partial \mcL}{\partial x}=0, \frac{\partial \mcL}{\partial y}=0$ implies that we are working with a gradient $$\nabla_{\vec{x}} \mcL = \vec{0}.$$ This is the first-order condition for a multivariable function, with the Hessian being the second-order condition. We will learn later that it is not the Hessian that we know and love that we use to evaluate the second-order condition, but instead a \emph{bordered Hessian}.}
}

This work is abstract, and will be supported by many examples.

\ex{Lagrange multipliers}{
	Let's consider maximising $f(x, y) = x^{\frac{1}{2}}y^{\frac{1}{3}}$ subject to the constraint $2x+y=10.$ Note that our new function $$g(x, y) = 2x+y-10=0.$$ Therefore, our Lagrangian function is $$\mcL(x, y, \lambda) = f(x, y) - \lambda g(x, y) = x^{\frac{1}{2}}y^{\frac{1}{3}} - \lambda(2x+y-10).$$ We must then have $\frac{\partial\mcL}{\partial x}=\frac{\partial\mcL}{\partial y}=0$. We know that $$\frac{\partial\mcL}{\partial x}=\frac{1}{2}x^{-\frac{1}{2}}y^{\frac{1}{3}} -2 \lambda=0,$$ and $$\frac{\partial\mcL}{\partial y} = \frac{1}{3}x^{\frac{1}{2}}y^{-\frac{2}{3}}-\lambda=0.$$
	
	Using the two equations above yields that $$\frac{2}{3} x^{\frac{1}{2}}y^{-\frac{2}{3}} = \frac{1}{2}x^{-\frac{1}{2}}y^{\frac{1}{3}},$$ or $$\frac{4}{3}x=y.$$ We substitute this into $2x+y=10$ to yield $x=3, y=4$. Therefore, $f(x, y) = 3^{\frac{1}{2}}\cdot 4^{\frac{1}{3}}$. 
	
	Here we didn't have to find the value of $\lambda$ to find $f(x, y)$. But note that $\lambda = \frac{1}{3}x^{\frac{1}{2}}y^{-\frac{2}{3}} = 3^{-1}\cdot 3^{\frac{1}{2}} \cdot 4^{-\frac{2}{3}} = 3^{-\frac{1}{2}} \cdot 4^{-\frac{2}{3}}$.
}

Couldn't we have substituted $y=10-2x$ into $f(x, y)$ to obtain a single-variable function to be optimised? Let's try an example where using direct substitution won't be very useful.

\ex{}{
	Maximise $4x+y$ with respect to $x^2+y^2 = 20$. The Lagrangian is $$\mcL(x, y, \lambda) = 4x+y-\lambda(x^2+y^2-20),$$ meaning that $\frac{\partial\mcL}{\partial x} = 4-2x\lambda=0$ and $\frac{\partial\mcL}{\partial y} = 1-2y\lambda=0$. Rearranging, we have that $x=\frac{2}{\lambda}$ and $y=\frac{1}{2\lambda}$. Substituting into the constraint, we have that \begin{align*} \frac{4}{\lambda^2}+\frac{1}{4\lambda^2}&=20 \\ \frac{17}{4\lambda^2} &= 20\\ \lambda &= \pm\sqrt{\frac{17}{80}}. \end{align*} 
	
	When $\lambda=\sqrt{\frac{17}{80}}.$, we have that $x=2 / \sqrt{\frac{17}{80}}, y=1 / 2\sqrt{\frac{17}{80}}$. When $\lambda=-\sqrt{\frac{17}{80}}$, $x=-2 / \sqrt{\frac{17}{80}}, y=-1 / 2\sqrt{\frac{17}{80}}$. 
	
	Therefore, a maximum is obtained when $\lambda = \sqrt{\frac{17}{80}}, x=2 / \sqrt{\frac{17}{80}}, y=1 / 2\sqrt{\frac{17}{80}}$ where $f(x, y) = 17/2\sqrt{\frac{17}{80}}$ and a minimum is obtained when $\lambda = -\sqrt{\frac{17}{80}}, x=-2 / \sqrt{\frac{17}{80}}, y=-1 / 2\sqrt{\frac{17}{80}}$ where $f(x, y) = -17/2\sqrt{\frac{17}{80}}$.
}

Lagrangians may also arise in functions of three variables. 

\ex{}{
	Maximise $f(x, y, z)=xyz$ with respect to $g(x, y, z)=x+y+z=1$. Also assume that $x, y, z \geq 0$. Our Lagrangian is $$xyz - \lambda(x+y+z-1).$$ Here we obtain a system of four equations in four unknowns, i.e. $$\frac{\partial\mcL}{\partial x}=0, \frac{\partial\mcL}{\partial y}=0, \frac{\partial\mcL}{\partial z}=0, \frac{\partial\mcL}{\partial \lambda}=0,$$ or \begin{align} yz-\lambda&=0\\ xz-\lambda&=0\\ xy-\lambda&=0 \\ x+y+z-1&=0. \end{align} Consider the fact that $yz=xz$. Then, $z(y-x)=0$, which means that $z=0$ or $y=x$. 
	
	\tbf{Case 1: $z=0$.} By substitution, this means that $\lambda=0$. The fact that $xy=0$ (by equation 8.6) shows that $x=0$ or $y=0$. Using the fact that $x+y+z=1$, we have that $\mbf{(x, y, z) = (0, 1, 0)}$ or $\mbf{(x, y, z) = (1, 0, 0)}$. 
	
	\tbf{Case 2: $y=x$.} If both $x=y=0$, we substitute this statement into $g(x, y, z)$ to conclude that $z=1$, meaning that we obtain $\mbf{(x, y, z) = (0, 0, 1)}$. 
	
	However if $x=y \neq 0$, then we use the fact that $yz=xy$ to conclude that $y(z-x)=0$. This means that either $y=0$ or $z=x$, meaning that $x=y=z$. The only solution here is $\mbf{(x, y, z) = (\frac13, \frac13, \frac13).}$
	
	We have obtained four solutions. Three of them are minima - i.e. $(0, 0, 1), (0, 1, 0), (1, 0, 0)$, and one of them is a maxima $(\frac13, \frac13, \frac13)$ that yields $f(x, y, z)=\frac{1}{27}.$
}

Let's observe Lagrangian functions in $n$ variables. Let $f(\vec{x})$ be constrained with respect to the function $g(\vec{x})=0$, where $\vec{x} = \{x_1, \dots, x_n\}$. Then, the Lagrangian function is $$\mcL(\vec{x}, \lambda),$$ with first-order conditions $$\frac{\partial \mcL}{\partial x_1}=0, \dots, \frac{\partial \mcL}{\partial x_n}=0\ \text{and}\ \frac{\partial \mcL}{\partial \lambda}=0.$$

This can be represented as $$\nabla_{\vec{x}}\ \mcL = \vec{0} \ \text{and}\ \frac{\partial \mcL}{\partial \lambda}=0,$$ where $\nabla$ denotes the gradient of $\mcL$ with regards to $\vec{x}$.

But note that $\frac{\partial \mcL}{\partial \lambda}=0$ is equivalent to the condition $g(\vec{x})=0$.

\ex{Two constraints}{Let's finish by observing a Lagrangian function with three variables and two constraints. We attempt to maximise $f(x, y, z) = x^2+y^2+z^2$ with respect to the two constraints $g_1(x, y, z)=-x^2-y^2+z^2=0$ and $g_2(x, y, z) = -x+y-1=0$.

Our Lagrangian is $$\mcL(x, y, z, \lambda_1, \lambda_2) = x^2+y^2+z^2-\lambda_1(-x^2-y^2+z^2)-\lambda_2(-x+y-1).$$ This yields us a system of five equations: 

	$$\begin{cases}
		\frac{\partial \mcL}{\partial x} = 2x+2\lambda_1 x +\lambda_2 \\
		\frac{\partial \mcL}{\partial y} = 2y-2\lambda_1 y -\lambda_2 \\
		\frac{\partial \mcL}{\partial z} = 2z - 2\lambda_1 z  \\
		\frac{\partial \mcL}{\partial \lambda_1} = x^2+y^2-z^2 \\
		\frac{\partial \mcL}{\partial \lambda_2} = x-y+1
	\end{cases}$$
	
	The expression for $\frac{\partial \mcL}{\partial z}$ tells us that $2z(1-\lambda_1)=0$, meaning that $z=0$ or $\lambda=1$. 
	
	\tbf{Case 1:} Consider when $\lambda_1=1$. This yields 
	
	$$\begin{cases}
		0 = 2x+2x +\lambda_2 \\
		0 = 2y-2y -\lambda_2 \\
		0= x^2+y^2-z^2 \\
		0= x-y+1
	\end{cases}$$
	which implies that $\lambda_2=0$ by the second equation. Substitution into the first equation yields $x=0$; this yields $y^2-z^2=0, -y-1=0$. Solving this system yields $\{y, z\} = \{1, 1\}.$
	
	Therefore, $\{x, y, z, \lambda_1, \lambda_2\} = \{0, 1, 1, 1, 0\}.$
	
	\tbf{Case 2:} Consider when $z=0$. Therefore, 
	
	$$\begin{cases}
		0 = 2x+2x +\lambda_2 \\
		0 = 2y-2y -\lambda_2 \\
		0= x^2+y^2 \\
		0= x-y+1
	\end{cases}$$
	
	From the third equation, we obtain that $x=0, y=0.$ This violates the fourth equation's condition that $x-y+1=0$, so we must reject this case.  It follows that $\{x, y, z, \lambda_1, \lambda_2\} = \{0, 1, 1, 1, 0\}$ is our only solution.
}

\subsection{Testing for maxima and minima}

We computed the first-order conditions for the Lagrangian function to obtain the minima and maxima of a function $f$ subject to a constraint $g=0$. Recall that by calculating $\frac{\partial \mcL}{\partial x}=0, \frac{\partial \mcL}{\partial y}=0$, it is implied that we are working with a gradient $$\nabla_{\vec{x}} \mcL = \vec{0}.$$ What about the second-order conditions? Can we just use the Hessian matrix? Yes, but in an \emph{altered} way - we call this the \tbf{Bordered Hessian.}

\defn{Bordered Hessian}{
	Let $f(x, y)$ be subject to a condition $g(x, y)=0$. Let $(x_0, y_0)$ be the optima wherein which type we want to further clarify. Then, the bordered Hessian is defined as $$H(x_0, y_0, \lambda_0)=\begin{pmatrix} \medskip 0 & \dfrac{\partial g}{\partial x_0} & \dfrac{\partial g}{\partial y_0} \\ \medskip \dfrac{\partial g}{\partial x_0} & \dfrac{\partial^2 \mcL}{\partial x_0^2} & \dfrac{\partial^2 \mcL}{\partial x_0\partial y_0} \\ \dfrac{\partial g}{\partial y_0} & \dfrac{\partial^2 \mcL}{\partial y_0\partial x_0} & \dfrac{\partial^2 \mcL}{\partial y_0^2}\end{pmatrix}$$
	
	The type of optima corresponds to the determinant of this matrix.
	
	Note that $$\begin{pmatrix}\medskip \dfrac{\partial^2 \mcL}{\partial x_0^2} & \dfrac{\partial^2 \mcL}{\partial x_0y_0} \\ \dfrac{\partial^2 \mcL}{\partial y_0x_0} & \dfrac{\partial^2 \mcL}{\partial y_0^2}\end{pmatrix}$$ is a Hessian in itself. 
}

Let's work in $n$ dimensions. The bordered Hessian is merely a simplification of the Hessian of $\mcL(\lambda, \vec{x})$, given by $$\begin{pmatrix} \frac{\partial^2 \mcL}{\partial \lambda^2} & \frac{\partial^2 \mcL}{\partial \lambda \partial x_1} & \dots & \frac{\partial^2 \mcL}{\partial \lambda \partial x_n} \\ \frac{\partial^2 \mcL}{ \partial x_1 \partial \lambda} &  \frac{\partial^2 \mcL}{ \partial x_1^2} & \dots & \frac{\partial^2 \mcL}{ \partial x_1 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 \mcL}{ \partial x_n \partial \lambda} & \frac{\partial^2 \mcL}{ \partial x_n \partial x_1} & \dots & \frac{\partial^2 \mcL}{ \partial x_n^2}\end{pmatrix}.$$

Note that $\frac{\partial}{\partial \lambda} \mcL = g$, and therefore $\frac{\partial^2}{\partial \lambda^2} \mcL = 0$ since the constraint $g(\vec{x})$ does not contain $\lambda$ in its argument. We can simplify the terms in the first row and column to obtain $$\begin{pmatrix} 0 & \frac{\partial g}{ \partial x_1} & \dots & \frac{\partial g}{ \partial x_n} \\ \frac{\partial g}{ \partial x_1 } &  \frac{\partial^2 \mcL}{ \partial x_1^2} & \dots & \frac{\partial^2 \mcL}{ \partial x_1 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial g}{ \partial x_n } & \frac{\partial^2 \mcL}{ \partial x_n \partial x_1} & \dots & \frac{\partial^2 \mcL}{ \partial x_n^2}\end{pmatrix}.$$

\defn{Rewriting the Bordered Hessian}{
	The bordered Hessian is a $n \times n$ matrix, where $n$ is the number of variables in the Lagrangian $\mcL(\vec{x}, \lambda)$. Observe the first column. Note that it contains a zero and terms corresponding to the gradient of the condition $g$. The first row also contains a zero and terms corresponding to the gradient of $g$. Write it as $\nabla_{\vec{x}}\ g$.
	
	What about the Hessian in the bottom right? Let's write it concisely using tensor products as $\nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ \mcL$. 
	
	Thus, the bordered Hessian is now $$H_B(\mcL(\vec{x}, \lambda)) = \begin{pmatrix} 0 & (\nabla_{\vec{x}}\ g)^T \\ \nabla_{\vec{x}}\ g & \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ \mcL \end{pmatrix}.$$
	
	In further explorations of the bordered Hessian and the \emph{projected bordered Hessian}, we will employ the concise form of the bordered Hessian. 
}

Let's reuse Example 8.3 to calculate the bordered Hessian. 

\ex{}{In Example 8.3, we had $f(x, y) = 4x+y$ subject to the constraint $g(x, y)=x^2+y^2=20.$ Our second-order conditions for $\mcL$ are: 

\begin{itemize}
	\item $\frac{\partial^2 \mcL}{\partial x^2} = 2\lambda$
	\item $\frac{\partial^2 \mcL}{\partial xy} = 0$
	\item $\frac{\partial^2 \mcL}{\partial yx} = 0$
	\item $\frac{\partial^2 \mcL}{\partial y^2} = 2\lambda$
\end{itemize}

Thus our bordered Hessian is $$\begin{pmatrix} 0 & 2x & 2y \\ 2x & 2\lambda & 0 \\ 2y & 0 & 2\lambda \end{pmatrix}.$$ This matrix has determinant \begin{align*} -2x(4x\lambda)+2y(-4\lambda y) &= -8x^2\lambda - 8y^2\lambda \\ &=-8\lambda(x^2+y^2). \end{align*}

We first consider when $\lambda=\sqrt{\frac{17}{80}}, x=-2 / \sqrt{\frac{17}{80}}, y=-1 / 2\sqrt{\frac{17}{80}}$. Notice that $\lambda >0$ and $x^2+y^2>0$, so the determinant is negative in this case, and we have a local minima. 

 In the case where $\lambda=-\sqrt{\frac{17}{80}}, x=2 / \sqrt{\frac{17}{80}}, y=1 / 2\sqrt{\frac{17}{80}}$, notice that $\lambda <0$ and so the determinant is positive, giving us a local maxima.
}

\coro{}{
	In the usual Hessian matrix, unlike the bordered Hessian, we cannot use the determinant to find whether we have a local maxima, minima, or a saddle point. Instead, we have to defer to the definiteness of the matrix, or the nature of the matrix's eigenvalues.
	
	This is because the nature of the bordered Hessian \tbf{forces it to be indefinite.} To prove this, consider that the bordered Hessian takes on the form $$\begin{pmatrix} 0 & C^T \\ C & H \end{pmatrix},$$ where $C = \nabla_{\vec{x}}\ g$ is the vector of constraints and $ \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ \mcL$ is the Hessian matrix without the constraint, or the Hessian matrix when solely considering $\vec{x}$. 
	
	Let's work in two dimensions $\{x, y\}$, for this can be generalised quite easily. Construct the quadratic form $$\begin{pmatrix} x^T & y^T \end{pmatrix} \begin{pmatrix} 0 & C^T \\ C & H \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = x^T Hx + x^TC^T y + y^T Cx.$$ Note that \begin{align*} x^T Hx + x^TC^T y + y^T Cx &= x^T Hx + (Cx)^T y + y^T Cx \\ &= x^T Hx + y^T Cx + y^T Cx \\ &= x^T Hx + 2y^T Cx.\end{align*}
	
	We aim to show that this quadratic form can be both positive and negative for carefully chosen values of $x$ and $y$. With $y=0$ and $x \neq0$, we have that $x^THx$ is positive if the constraints lead to a maxima, and $x^THx$ being negative if a minima is obtained. 
	
	Evidently, since $C$ is not degenerate (the existence of a constraint implies that $x$ and $y$ cannot be both zeroes), then $2y^TC=0$ if and only if $y=0$. So now assume that $y\neq0$, and thus $y^TC\neq0$. Our job is to pick an $x$ such that $y^T Cx \neq 0$. We can pick $x=(y^T C)^T$ such that $y^TCx = x^T x$, which obviously $\neq 0$.
	
	Consider the function $$f(ky)= ky^T Cx.$$ Since we demonstrated that $y^T Cx \neq 0$, we can scale $y$ to $ky$ by choosing an appropriate $k$ such that $y^T Cx$ is sufficiently tiny to make $x^T Hx + 2y^T Cx$ negative. In fact, all we have to do is make $$2(ky^T)Cx < x^THx,$$ and we have thus demonstrated ways to make $x^T Hx + y^T Cx + y^T Cx$ admit both positive and negative values. 
}

\subsection{Barrier functions}

The lecture notes briefly mention the method of \emph{barrier functions}. Consider Example 8.2, where we maximised $f(x, y) = x^{\frac{1}{2}}y^{\frac{1}{3}}$ subject to $2x+y=10$. Our goal is to consider $$\mcL = x^{\frac{1}{2}}y^{\frac{1}{3}} - h(x, y),$$ where the function $h(x, y)$ would be very large should $2x+y-10 \neq 0$. A possible example is to set $h(x, y) = \lambda(2x+y-10)^{10}$, which yields us $$f(x, y) = x^{\frac{1}{2}}y^{\frac{1}{3}} - \lambda(2x+y-10)^{10}.$$ Note that the larger we make $\lambda$, the more detrimental a deviation away from the condition $2x+y-10=0$ would be. This is an ordinary two-variable optimisation problem: we want $$\nabla_{x, y} f(x, y) = 0.$$

\ex{Quadratic barrier function}{
	Suppose our barrier function was quadratic. Thus, our new Lagrangian would be $$\mcL(x, y)=x^{\frac{1}{2}}y^{\frac{1}{3}} - \lambda(2x+y-10)^2.$$
	
	Notice how the Lagrangian function does not have the parameter $\lambda$. This is, in essence, an optimisation problem in two variables. We set the parameter $\lambda$ to be a comfortably large value, and rely on the might of the barrier function to penalise deviations from optimal conditions. 
	
	This admits the first order conditions $$\frac{1}{2}x^{-\frac{1}{2}}y^{\frac{1}{3}} - 4\lambda (2x+y-10)=0$$ and $$\frac{1}{3}x^{\frac{1}{2}}y^{-\frac{1}{3}} - 2\lambda (2x+y-10)=0.$$
	
	We can set $\lambda=10$ (an arbitrary value - Frank set it to 10 in his lecture notes so we'll do it here) and obtain $x=3.003, y=4.004$ which is close enough to our solution of $x=3, y=4$.
	
	Evidently this is a bit unsatisfying. However we can choose a larger value of $\lambda$ and a more punishing barrier function to obtain closer estimates to our actual solution.
}

\subsection{Approximating the Lagrangian}

Consider the Lagrangian function $\mcL(\vec{x}, \lambda) = f(\vec{x}) - \lambda g(\vec{x}).$ Let $\vec{x}_*, \lambda_*$ be a solution to the first order conditions $\nabla_{\vec{x}} \mcL =0$, or the set \begin{align*} \frac{\partial f}{\partial x_1} - \lambda \frac{\partial g}{\partial x_1} = 0 \\ \vdots \\ \frac{\partial f}{\partial x_n} - \lambda \frac{\partial g}{\partial x_n} = 0.\end{align*}

Consider when we study the Lagrangian function for values close to the solution. We can write $\vec{x} = \vec{x}_* + \vec{\delta x}$ and $\lambda = \lambda_* + \delta \lambda$, where $\delta$ is some infinitesimally small value.

The linear term in the Taylor approximation of the Lagrangian is just the first total derivative of $\mcL(\vec{x_*}+\vec{\delta x}, \lambda_*+\delta\lambda)$ - that being $$\frac{\partial \mcL}{\partial \lambda_*} \delta\lambda + \frac{\partial \mcL}{\partial x_{1*}} \delta x_1 + \dots + \frac{\partial \mcL}{\partial x_{n*}} \delta x_n  = \delta \lambda \frac{\partial \mcL}{\partial \lambda_*} + \vec{\delta x} \nabla_{\vec{x}_*} \mcL.$$

Note that the first-order conditions satisfy $\nabla_{\vec{x}} \mcL =0$. Automatically this means that the first-order conditions vanish, meaning that the linear term in the approximation is zero. 

The quadratic term is similar to the Taylor approximation of a multivariate function. However, we use the bordered Hessian instead of the Hessian. This is \begin{align*}  H_B(\mcL) &= \frac{1}{2} \begin{pmatrix} \delta \lambda & \vec{\delta x}  \end{pmatrix} \cdot  \begin{pmatrix} 0 & \nabla_{\vec{x}}\ g \\ \nabla_{\vec{x}}\ g & \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ \mcL \end{pmatrix} \cdot \begin{pmatrix} \delta \lambda \\ \vec{\delta x} \end{pmatrix} \\ &= \frac{1}{2} \begin{pmatrix} \delta \lambda & \vec{\delta x} \end{pmatrix} \cdot \begin{pmatrix} \nabla_{\vec{x}}\ g \cdot \vec{\delta x} \\ \nabla_{\vec{x}}\ g \cdot \delta \lambda + \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ \mcL \cdot \vec{\delta x} \end{pmatrix} \\ &= \frac{1}{2} \left( \delta \lambda \nabla_{\vec{x}}\ g \cdot \vec{\delta x} +   \vec{\delta x} \nabla_{\vec{x}}\ g \cdot \delta \lambda + \vec{\delta x} \nabla_{\vec{x}} \otimes \nabla_{\vec{x}} \mcL \ \vec{\delta x} \right) \\ &= \frac{1}{2} \left(2\cdot  \delta \lambda \nabla_{\vec{x}}\ g\ \vec{\delta x} +\vec{\delta x} \nabla_{\vec{x}} \otimes \nabla_{\vec{x}} \mcL \ \vec{\delta x} \right) \end{align*}

Therefore, the quadratic approximation of the Lagrangian function is \begin{align*} \mcL(\vec{x}, \lambda) &= \mcL(\vec{x_*} + \vec{\delta x}, \lambda_* + \delta\lambda) \\ &= \mcL(\vec{x_*}, \lambda_*) + \frac{1}{2} \left(2\cdot  \delta \lambda \nabla_{\vec{x}}\ g\ \vec{\delta x} +\vec{\delta x} \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ \vec{\delta x} \right) \end{align*}

What happens when we have an infinitesimally small change in the Lagrangian, or $\delta \mcL$? This is the error term $$\delta \mcL = \frac{1}{2} \left(2\cdot  \delta \lambda \nabla_{\vec{x}}\ g\ \vec{\delta x} +\vec{\delta x} \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ \vec{\delta x} \right).$$

This error term is our second-order approximation. Let's analyse each term and see what happens when we compute them explicitly. 

Consider when there are no variations in $\delta\lambda$; this means that the variations only occur in $\vec{\delta x}$. Then, the first term of the quadratic approximation vanishes, and we are left with the second term $$\frac{1}{2} \left( \vec{\delta x} \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\mcL \ \vec{\delta x} \right),$$ or, if we expand $\mcL = f(\vec{x}) - \lambda g(\vec{x})$, $$\frac{1}{2} \left(\vec{\delta x} \cdot \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ f(\vec{x}) \cdot \vec{\delta x} -\lambda \cdot \vec{\delta x} \cdot \nabla_{\vec{x}} \otimes \nabla_{\vec{x}}\ g(\vec{x}) \cdot \vec{\delta x} \right)$$

If the constraint is linear in $\vec{x}$ - e.g. we maximise a utility function given a linear budget constraint $p_1x_1 + \dots + p_nx_n = B$, the second term becomes zero. In this case, the sign of $\delta \mcL$ depends on the definiteness of the quadratic form $\frac{1}{2} \vec{\delta x} \cdot \vec{x} \otimes \nabla_{\vec{x}}\ f(\vec{x}) \cdot \vec{\delta x}$. We can use the eigenvalue method that we all use and love to find whether this local optimum is a maximum.

What about the first term $\delta \lambda \nabla_{\vec{x}}\ g\ \vec{\delta x}$? Since the constraint is dictated by $\vec{\delta x}$, the direction of $\vec{\delta x}$, or the infinitesimally small changes in $\vec{x}$, should follow the direction of the constraint $g(\vec{x})$. Therefore, the direction of $\vec{\delta x}$ is thus perpendicular to the direction of the gradient of the constraint. Therefore, we have the extra condition that $$ \nabla_{\vec{x}}\ g \cdot \vec{\delta x}=0.$$

How do we satisfy this condition? We need to somehow "project" the $\vec{\delta x}$ along the direction of $g(\vec{x})$. To do this, we introduce the \emph{projected bordered Hessian}. 

\section{The projected bordered Hessian}

How would we create a bordered Hessian for the directions along the constraint? By projecting it along the constraints.

\coro{}{This section is very mechanical and features a very computation-heavy process of calculation. Our goal is to build the bordered Hessian that is projected along the directions perpendicular to $\vec{\delta x}$ using the \emph{projector matrix}.}

\defn{Projector matrix}{
	A projector matrix $P_{\perp \vec{x}}$ is a matrix that satisfies $$(P_{\perp \vec{x}})^2 = P_{\perp \vec{x}} \ \text{and} \ P_{\vec{x}} \cdot \vec{x} = \vec{0}.$$ 
	
	How can we produce a projector matrix from a vector $\vec{x}$? We define $P_{\perp \vec{x}} = \bbI - \vec{x} \otimes \vec{x}$.
}

What do we choose for $\vec{x}$? We choose and \emph{normalise} the first row / first column of the bordered Hessian, given by $$\hat{\nabla} g = \begin{pmatrix} 0 \\ \nabla_{\vec{x}} g(\vec{x}) \end{pmatrix}.$$

The projector matrix is thus $$P_{\perp \hat{\nabla} g} = \bbI - \hat{\nabla} g \otimes \hat{\nabla} g.$$

\defn{Projected bordered Hessian}{Under the previous definitions of the bordered Hessian and the projector matrix, the projected bordered Hessian $H_{\perp B}$ is defined as $$H_{\perp B} = P_{\perp \hat{\nabla} g} \cdot H_B \cdot P_{\perp \hat{\nabla} g}.$$

}

Here are some properties of the projected bordered Hessian. Firstly, the eigenvalues of the projected, bordered Hessian contains at least two zeroes. This is because by definition, the \tbf{direction of the projected bordered Hessian is perpendicular to to the direction of the constraint.} Explicitly, we have that $P_{\hat{\nabla} g} \cdot \hat{\nabla} g =0$, so naturally $$H_{\perp B} \cdot \hat{\nabla} g  = P_{\perp \hat{\nabla} g} \cdot H_B \cdot P_{\perp \hat{\nabla} g} \cdot \hat{\nabla} g = \vec{0}.$$

The eigenvector $\begin{pmatrix}1 \\ 0 \\ \vdots \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ \vec{0} \end{pmatrix}$ also yields zero when multiplied by the projected Hessian. Thus, every projected bordered Hessian has the eigenvalue-eigenvector pair $0, \begin{pmatrix} 1 \\ \vec{0} \end{pmatrix}$.

The values of the non-zero eigenvalues can be used to determine whether the infinitesimal change $\delta \mcL$ is positive or not. This is akin to the way we determine the definiteness of a quadratic form.

The eigenvectors of the projected bordered Hessian are well-defined. They evidently contain $\begin{pmatrix} 1 \\ \vec{0} \end{pmatrix}$. They also contain the gradient of the constraint $\begin{pmatrix} 0 \\ \nabla_{\vec{x}}\ g \end{pmatrix}$, and vectors $\begin{pmatrix} 0 \\ \nabla_{\vec{x}}\ g_p \end{pmatrix}$ perpendicular to the gradient of the constraint.

\ex{Projected bordered Hessian}{
	Consider Example 8.2, where we maximised $f(x, y)=x^{\frac{1}{2}}y^{\frac{1}{3}}$ with respect to $g(x, y)= 2x+y-10=0.$ The optimal conditions are $x=3, y=4$.
	
	The normalised gradient is $\begin{pmatrix} 0 \\ \frac{2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} \end{pmatrix}$, and thus the projector matrix becomes $$P_{\perp \hat{\nabla}g} = \begin{pmatrix} 1 & 0 & 0 \\ 0 & \frac{1}{5} & -\frac{2}{5} \\ 0 & -\frac{2}{5} & \frac{4}{5} \end{pmatrix}.$$
	
	We now need the bordered Hessian. Note that $\mcL(x, y, \lambda) = x^{\frac{1}{2}}y^{\frac{1}{3}} - \lambda(2x+y-10)$, so the second-order conditions are $$\frac{\partial^2 \mcL}{\partial x^2} = -\frac{y^{\frac{1}{3}}}{4x^{\frac{3}{2}}}$$ $$\frac{\partial^2 \mcL}{\partial x\partial y} = \frac{1}{6x^{\frac{1}{2}}y^{\frac{2}{3}}}$$ $$\frac{\partial^2 \mcL}{\partial y^2} = -\frac{2x^{\frac{1}{2}}}{9y^{\frac{5}{3}}}$$
	
	and thus the bordered Hessian is $$H_B = \begin{pmatrix} 0 & 2 & 1 \\ 2 & -\frac{y^{\frac{1}{3}}}{4x^{\frac{3}{2}}} & \frac{1}{6x^{\frac{1}{2}}y^{\frac{2}{3}}} \\ 1 & \frac{1}{6x^{\frac{1}{2}}y^{\frac{2}{3}}} & -\frac{2x^{\frac{1}{2}}}{9y^{\frac{5}{3}}} \end{pmatrix}.$$ Substituting the initial conditions yields us $$H_B = \begin{pmatrix} 0 & 2 & 1 \\ 2 & -\frac{4^{\frac{1}{3}}}{4\cdot 3^{\frac{3}{2}}} & \frac{1}{6\cdot 3^{\frac{1}{2}}4^{\frac{2}{3}}} \\ 1 & \frac{1}{6\cdot 3^{\frac{1}{2}}4^{\frac{2}{3}}} & -\frac{2\cdot 3^{\frac{1}{2}}}{9\cdot 4^{\frac{5}{3}}} \end{pmatrix}.$$
	
	To obtain our projected bordered Hessian, we multiply $P_{\perp \hat{\nabla}g} \cdot H_B \cdot P_{\perp \hat{\nabla}g}$ to obtain $$H_{\perp B} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & -\frac{1}{30\cdot 2^{\frac{1}{3}} 3^{\frac{1}{2}}} & \frac{1}{25 \cdot 2^{\frac{1}{3}} 3^{\frac{1}{2}}} + \frac{2^{\frac{2}{3}}}{75\cdot 3^{\frac{1}{3}}} \\ 0 & \frac{1}{25 \cdot 2^{\frac{1}{3}} 3^{\frac{1}{2}}} + \frac{2^{\frac{2}{3}}}{75\cdot 3^{\frac{1}{3}}}& -\frac{2^{\frac{2}{3}}}{15 \cdot 3^{\frac{1}{2}}} \end{pmatrix}$$
	
	which yields the eigenvalues $$\{0, 0, -\frac{1}{6 \cdot 2^{\frac{1}{3}} \cdot 3^{\frac{1}{2}}} \}$$ and the eigenvectors $$\begin{pmatrix} 1 \\0\\0 \end{pmatrix},\begin{pmatrix} 0 \\2\\1 \end{pmatrix},\begin{pmatrix} 0 \\-1\\2 \end{pmatrix}.$$
	
	The negative sign of the non-zero eigenvalue shows that the optimal conditions $x=3, y=4$ is indeed a maxima. The eigenvectors $\begin{pmatrix} 0 \\2\\1 \end{pmatrix},\begin{pmatrix} 0 \\-1\\2 \end{pmatrix}$ also tell us the direction of the constraint and the direction of the gradient to the constraint.
}

\chapter{Week 9}

This week's content is not like the others. It has a great focus on probability and statistics. This chapter applies constrained optimisation of multiple variables to probability and statistics, placing a great focus on \emph{information theory}. We consider discrete probability distributions before exploring continuous probability distributions.

\section{Review of probability theory}

\subsection{Discrete probability theory}

This is review of elementary probability theory. It is included for completeness' sake. If you take ECON0005, this should be revision.

Let $X$ be a discrete random variable. $X$ admits a sample space.

\ex{Fair die}{
	Let $X$ denote the possible outcomes from the rolls of a fair die. $X$ admits a sample space $$X=\{1, 2, 3, 4, 5, 6 \}$$ with probabilities $$P(X=x_i) = \left\{\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right\}$$ where $x_i = 1, 2, 3, 4, 5, 6.$
}

Here are some properties of probabilities.

\begin{enumerate}
	\item The sum of probabilities $\sum\limits_{i=1}^{|X|} P(X=x_i) =1$, where $|X|$ is the number of elements of the sample space. Written succinctly, this means that $P(X)=1$.
	\item Let $S$ be a set of events such that $S \subset X$. Then, $P(S) \geq 0$.
	\item Let $s, t \in X$ with $s \cap t = \emptyset$. Then, $$P(s \cup t) = P(s) +P(t).$$ This means that the outcomes $s$ and $t$ are mutually exclusive.
\end{enumerate}

\defn{Complement of a set}{
	Let $S \subset X$ be a subset of outcomes. Then, the complement $\bar{S}$ is defined as $X - S$, or the set of all elements in $X$ that are not in $S$.
}

\defn{Union of two sets}{
	Let $S$ and $T$ be two subsets of $X$. Then, the probability $P(S \cup T)$ is $$P(S \cup T) = P(S)+P(T)-P(S \cap T).$$
}

Let's discuss discrete probability distributions. A probability distribution maps each element of the sample space $X$ to a real number in $[0, 1]$, obtaining a series $\{x_i\} = \{x_1, x_2, \dots, x_n \}$, where $n = |X|$.

This is a function $P: X \to [0, 1]$ satisfying $$\sum\limits_{i=1}^{|X|} P(x_i) =1.$$

Let's use the fair die as an example. The fair die maps the sample space containing $\{1, 2, 3, 4, 5, 6 \}$ to the set of probabilities $\left\{\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right\}$.

Instead of probabilities, we can also assign a set of statistical weights $W(x_i)$. To convert from statistical weights to probabilities, use the formula $$P(x_i) = \frac{W(x_i)}{\sum\limits_{i=1}^{|X|} W(x_i)}.$$ 

\tbf{Intuition:} This should give you the intuition of probabilities being the proportion of each element's weight with regards to the sum of all the elements' weights

\ex{Unfair die}{
	Consider a die with a sample space of $X=\{1, 2, 3, 4, 5, 6 \}$ but with a relative frequency of the die's value. For example, a four appears four times as often as a one; a six appears three times as often as a two. The set of statistical weights is then $$W(x_i)=1, 2, 3, 4, 5, 6,$$ where $i \in \{1, 2, 3, 4, 5, 6\}$. The sum of all weights is $21$.

	Let's convert from $W(x_i)$ to $P(x_i)$. This becomes \begin{align*} P(x_i) = \left\{ \frac{1}{21},  \frac{2}{21}, \frac{1}{7}, \frac{4}{21}, \frac{5}{21}, \frac{2}{7}\right\}.\end{align*}
}

The set of all probabilities of outcomes can be described using a \emph{probability vector} $$\vec{P} = \{P(x_1), \dots, P(x_n) \} = \{p_1, \dots, p_n \}.$$ Observe that the sum of all weights is the dot product $$w_1+\dots+w_n =\begin{pmatrix} 1 & \dots & 1 \end{pmatrix} \cdot \begin{pmatrix} w_1 \\ \vdots \\ w_n \end{pmatrix}.$$ Denote the matrix $\begin{pmatrix} 1 & \dots & 1 \end{pmatrix}$ by $\vec{\bbI}$. Thus, the relationship between the probability vector $\vec{P}$ and the weight vector $\vec{W}$ is given by $$\vec{P} = \frac{\vec{W}}{\vec{\bbI} \cdot \vec{W}}.$$

\defn{Expected value for a discrete distribution}{
	Let $X = \{x_1, \dots, x_n \}$ be the set of outcomes of the sample space $X$, and let $\{p_1, \dots, p_n \}$ be their associated probabilities. The expected value is defined as: $$E(X) = \sum\limits_{i=1}^n x_ip_i.$$
}

\defn{Variance for a discrete distribution}{
	There are two equivalent definitions of the variance $Var(X)$.
	
	\begin{enumerate}
		\item $Var(X) = \sum\limits_{i=1}^n p_i(x_i - E(X))^2$
		\item $Var(X) = E(X^2) - E(X)^2$.
	\end{enumerate}
}

\subsection{Continuous probability theory}

\coro{}{\tbf{Key idea:} Instead of $X$ being a set $X= \{x_1, \dots, x_n \}$, it is now a subset of $\bbR$. More specifically, a probability density (note we use the term probability density instead of probability function) on a set $X \subset \bbR$ is a function $P: X \to \bbR^+$ that satisfies $$\int_{X} P(x) dx=1,$$ where $\int_X$ indicates that the bounds of integration is the domain of $X$.       }

Note that when we refer to a probability $P(x)$ of a continuous distribution, we refer to the probability of an outcome being infinitesimally close to $P(x)$. \tbf{The probabilities of individual outcomes in a continuous distribution are always zero.}

Our relationship between probabilities and weights (as described in the discrete case) copy over here. Let $(a, b) \subset X$. We have that $$\int_a^b P(x) dx = \frac{\int_a^b W(x) dx}{\int_X W(x) dx}.$$

More generally, we also have that $$P(x) = \frac{W(x)}{\int_X W(x) dx}.$$

\ex{Continuous weights and probabilities}{
	Let $X = [-1, 1]$, and let the density of statistical weights be $W(x) = x^2$. Note that $$\int_X W(x) dx = \int_{-1}^1 x^2 dx = \frac{2}{3}.$$ This means that the corresponding probability distribution is $$P(x)=\frac{x^2}{\frac{2}{3}} = \frac{3}{2}x^2.$$
	
	\tbf{Exercise:} Check that $\int_{-1}^1 P(x)=1$, thus meaning that $P(x)$, when defined on the sample space $[-1, 1]$, admits a valid probability distribution.
}

\defn{Expected value for a discrete distribution}{
	Let $X \subset \bbR$ have associated probability density $P(x)$ for each value of $x$. The expected value is defined as: $$E(X) = \int_{X} xP(x)dx.$$
}

\defn{Variance for a discrete distribution}{
	There are two equivalent definitions of the variance $Var(X)$.
	
	\begin{enumerate}
		\item $Var(X) = \int_X (x-E(X))^2 P(x)dx$.
		\item $Var(X) = E(X^2) - E(X)^2 = \int_{X} x^2P(x)dx - \left(\int_{X} xP(x)dx \right)^2$.
	\end{enumerate}
}

Note the similarities in the formulas for the expected value and variance of discrete and continuous distributions. 

\section{Entropy of discrete probability distributions}

Consider the throwing of a weighted (and therefore biased) coin. Let $X$ be its sample space: $\{$heads, tails$\}$. Suppose that the probability that it lands heads is $p$, and the probability that it lands tails is $1-p$. 

Let $p$ be very small - e.g. 0.01. Then, there is not much uncertainty in the probability distribution of $X$ as one can certainly assume, most of the time, that the coin will land on tails. 

What if $p$ is 0.5, or somewhere close to 0.5? Then, the probability of heads and tails is near 0.5. There is great uncertainty in the probability distribution of $X$; there is great uncertainty in determining whether the coin will land on heads or tails.

\defn{Entropy}{
	Suppose you wanted a function that described the uncertainty, or the surprise you attained upon knowing any outcome of a random variable $X=\{x_1, \dots, x_n\}$. Let that function be $S(X)$. It is defined as $$S(X)= - \sum\limits_{i=1}^n  p_i \log{p_i},$$ where $\{p_i\}$ denotes the set of probabilities corresponding to $\{x_i\}$.
	
	When referring to entropy as a function of probabilities, we usually write $S(\vec{p})$, where $\vec{p}$ denotes the probability vector.
}

\ex{Entropies of fair and unfair coins}{
	Consider the fair coin, where $p=0.5$. Then, $p_1=0.5, p_2=0.5$. This means that $$S(X) = -(0.5\log 0.5 + 0.5 \log 0.5) = 0.693.$$
	
	The unfair coin has $p=0.01$, meaning that $p_1=0.01, p_2=0.99$. This means that $$S(X) = -(0.01 \log 0.01+ 0.99 \log 0.99)=0.056.$$
	
	We see that there is more surprise, or uncertainty, for the fair coin than the unfair coin. 
}

What is the entropy for general $p$? It is $$S(p) = -(p\log p + (1-p) \log(1-p)).$$ When is $S(X)$ maximised? We have that \begin{align*} \frac{d}{dp} \left(-(p\log p + (1-p) \log(1-p) \right)&= 0 \\ -(\log p + 1 - \log(1-p)-1) &= 0 \\  -\log p + \log(1-p) &= 0 \\ \log\left(\frac{1-p}{p}\right)&=0 \\ p&=\frac{1}{2} \end{align*}

This makes sense as at $p=\frac{1}{2}$, we have the most uncertainty about the outcome of $X$.

\subsection{Entropy (constrained) maximisation}

We explored the entropy function $S(\vec{p})= - \sum\limits_{i=1}^n  p_i \log{p_i}$ as a function of uncertainty, or surprise inherent within the random variable.

This is a constrained optimisation problem in $n$ variables. More specifically, we maximise \begin{align*} S(\vec{p}) &= S(p_1, \dots, p_n) \\ &= -\begin{pmatrix} p_1 \\ \vdots \\ p_n \end{pmatrix} \cdot \begin{pmatrix} \log p_1 \\ \vdots \\ \log p_n \end{pmatrix} \\ &= -\vec{p} \cdot \log{\vec{p}} \end{align*} with respect to $p_1+\dots+p_n=1$, or $$\begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} \cdot \begin{pmatrix} p_1 \\ \vdots \\ p_n \end{pmatrix} = \vec{\bbI} \cdot \vec{p}.$$

The Lagrangian function is thus $$ \mcL(p_1, \dots, p_n, \lambda) = -(p_1 \log p_1+ \dots + p_n \log p_n) - \lambda(p_1+\dots+p_n) $$ with first-order conditions $$\frac{\partial \mcL}{\partial p_1}= -(1 + \log p_1) - \lambda, \dots, \frac{\partial \mcL}{\partial p_1}= -(1 + \log p_n) - \lambda, \frac{\partial \mcL}{\partial \lambda} = \vec{\bbI} \cdot \vec{p}-1.$$ 

In fact, we have that \begin{align*} \nabla_{\vec{p}}\ \mcL &= -(\vec{\bbI} +\log \vec{p}) - \lambda \cdot \vec{\bbI} \\ &= -\log \vec{p} - (1+\lambda) \vec{\bbI}. \end{align*} Solving $\nabla_{\vec{p}}\ \mcL=\vec{0}$ yields $\log \vec{p}  =-(1+\lambda) \vec{\bbI}$, or $$\vec{p} = e^{-(1+\lambda)} \vec{\bbI}.$$

Note that $\vec{p} \cdot \vec{\bbI} = \vec{1}$. Thus, the $p_i$ collectively satisfy \begin{align*} (e^{-(1+\lambda)} \vec{\bbI}) \cdot \vec{\bbI} &=1 \\ (e^{-(1+\lambda)})n&=1 \\ (e^{-(1+\lambda)})&= \frac{1}{n}, \end{align*} meaning that the entropy-maximising set of probabilities is $$\vec{p} = \frac{1}{n} \vec{\bbI}.$$

\ex{}{
	Consider rolling a fair die, where there are $n=6$ outcomes. The entropy-maximising set of probabilities is $\vec{p} = \{\frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}\}.$ Qualitatively, this set of probabilities yields the highest expected uncertainty, thus maximising entropy. 
}

Suppose that apart from constraining $\sum\limits_{i=1}^n p_i=1$, we also had to constrain the expected value. This is the condition $E(X)=\sum \limits_{i=1}^n x_i p_i = \mu$.

Let's attempt to express the expected value as a product of matrices and vectors. We consider the not-so-obvious fact that \begin{align*}\begin{pmatrix} 1 & 1 & \dots & 1 \end{pmatrix} \begin{pmatrix} x_1 & 0  & \dots & 0 \\ 0 & x_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & x_n \end{pmatrix} \begin{pmatrix} p_1 \\ p_2 \\ \dots \\ p_n \end{pmatrix} &= \begin{pmatrix} 1 & 1 & \dots & 1 \end{pmatrix} \cdot \begin{pmatrix} x_1p_1 \\ x_2p_2 \\ \dots \\ x_np_n \end{pmatrix} \\ &= x_1p_1+x_2p_2+\dots+x_np_n \\ &= \sum\limits_{i=1}^n x_ip_i. \end{align*}

Let $X=\begin{pmatrix} x_1 & 0  & \dots & 0 \\ 0 & x_2 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & x_n \end{pmatrix}$ be the matrix with $x_1, x_2, \dots$ on its diagonal. Then, $$E(X)= \vec{\bbI} \cdot X \cdot \vec{p}.$$

Thus, our Lagrangian function is \begin{align*} \mcL(\vec{p}, \lambda_1, \lambda_2) &= -(p_1\log p_1+\dots + p_n \log p_n) - \lambda_1(p_1+\dots+p_n-1) - \lambda_2(x_1p_1 + x_2p_2 + \dots + x_np_n-\mu)\\ &= -\left(\sum\limits_{i=1}^n p_i \log p_i\right) - \lambda_1\left(\left(\sum\limits_{i=1}^n p_i\right)-1\right) - \lambda_2\left(\left(\sum\limits_{i=1}^n x_ip_i\right)-\mu\right). \end{align*} In vector form, this is $$\mcL(\vec{p}, \lambda_1, \lambda_2) = -\vec{p} \cdot \log\vec{p} - \lambda_1 (\vec{\bbI}\cdot \vec{p} -\vec{\bbI}) - \lambda_2 (\vec{\bbI} \cdot X \cdot \vec{p}-\mu\vec{\bbI})$$ 

The first-order conditions are:

$$\begin{cases}
	\frac{\partial \mcL}{\partial p_1} = -\log(p_1)-1-\lambda_1-\lambda_2x_1=0 \\
	\frac{\partial \mcL}{\partial p_2} = -\log(p_2)-1-\lambda_1-\lambda_2x_2=0 \\
	\ \ \vdots \\
	\frac{\partial \mcL}{\partial p_n} = -\log(p_n)-1-\lambda_1-\lambda_2x_n=0 \\
	\frac{\partial \mcL}{\partial \lambda_1} = p_1+p_2+\dots+p_n-1=0 \\
	\frac{\partial \mcL}{\partial \lambda_2} = x_1p_1+x_2p_2+\dots+x_np_n-\mu=0
\end{cases}$$

Consider the first $n$ equations. These satisfy $$\nabla_{\vec{p}}\ \mcL = -\log{\vec{p}}-(1+\lambda_1) \bbI - \lambda_2 \vec{x}=0,$$ where $\vec{x} = \{x_1, x_2, \dots, x_n\}$. The solution for $\vec{p}$ is \begin{align*} -\log \vec{p} &= (1+\lambda_1) \bbI + \lambda_2\vec{x} \\ \vec{p} &= e^{-((1+\lambda_1)\bbI + \lambda_2\vec{x})}.\end{align*} 

Let's look at an individual component $p_i$. It satisfies \begin{equation} p_i = e^{-(1+\lambda_1+\lambda_2 x_i)}. \end{equation} Substitute this formula into our two constraints $\sum\limits_{i=1}^n p_i=1$ and $\sum\limits_{i=1}^n x_ip_i=\mu$ to yield \begin{align} \sum\limits_{i=1}^n e^{-(1+\lambda_1+\lambda_2 x_i)}&=1 \\ \sum\limits_{i=1}^n x_i e^{-(1+\lambda_1+\lambda_2 x_i)}&=\mu. \end{align} Extract $e^{-(1+\lambda_1)}$ to yield \begin{align} e^{-(1+\lambda_1)} \sum\limits_{i=1}^n e^{-\lambda_2 x_i}&=1 \\ e^{-(1+\lambda_1)} \sum\limits_{i=1}^n x_i e^{-\lambda_2 x_i} &=\mu. \end{align} 

Dividing equation (9.5) by equation (9.4) yields $$\mu = \dfrac{\sum\limits_{i=1}^n x_i e^{-\lambda_2 x_i}}{\sum\limits_{i=1}^n e^{-\lambda_2 x_i}}.$$

Also, from (9.4), we have that \begin{align} \sum\limits_{i=1}^n e^{-\lambda_2 x_i}=e^{1+\lambda_1}. \end{align}

Reconsider the fact that $p_i = e^{-(1+\lambda_1+\lambda_2 x_i)} = e^{-(1+\lambda_1)} e^{-\lambda_2x_i}.$ Using equation (9.6), we have that \begin{align*} p_i &= e^{-(1+\lambda_1)} e^{-\lambda_2x_i}\\ &= \left(\sum\limits_{i=1}^n e^{-\lambda_2 x_i}\right)^{-1} e^{-\lambda_2x_i} \\ &= \frac{e^{-\lambda_2x_i}}{\sum\limits_{i=1}^n e^{-\lambda_2 x_i}}. \end{align*}

This is not, and cannot be, explicitly solved. However, we can tinker around with the values of $\lambda_2$. We get that (steps omitted) $$\lim\limits_{\lambda_2 \to 0} W_i = \frac{1}{n}.$$ We also have that $$\lim\limits_{\lambda_2 \to \infty} W_i=1$$ for $i=n$ ($\lim\limits_{\lambda_2 \to \infty} W_i=0$ otherwise), and that $$\lim\limits_{\lambda_2 \to -\infty} W_i=1$$ for $i=1$ ($\lim\limits_{\lambda_2 \to -\infty} W_i=0$ otherwise).

Note that we could not obtain explicit solutions as in our expression for expected value, we introduced two new variables $\lambda_2$ and $\mu$. Yet we only introduced one new equation $\frac{\partial \mcL}{\partial \lambda_2} = x_1p_1+x_2p_2+\dots+x_np_n-\mu=0.$

This is dissatisfying. However we will show later that there is a solution if we use continuous probability distributions instead of discrete probability distributions.

\section{Entropy of continuous probability distributions}

\defn{Continuous entropy}{
	Let $X$ be a continuous random variable defined on some range $(a, b)$ with probability density function $P(X)$. The entropy for $X$ is defined as $$S(X) = -\int_a^b P(x) \log (P(x)) dx.$$ 
}

\ex{Uniform distribution}{
	Consider the uniform distribution defined on $(a, b).$ It has probability density function $$f(x) = \begin{cases} \frac{1}{b-a} & x \in (a, b) \\ 0 & x \not\in (a, b) \end{cases}$$
	
	Let the random variable $X$ have a uniform distribution. The entropy of $X$ is given by \begin{align*} S(X) &= -\int_a^b \frac{1}{b-a} (-\log(b-a)) dx \\ &= \int_a^b \frac{\log(b-a)}{b-a} dx \\ &= \frac{\log(b-a)}{b-a}(b-a) \\ &= \log(b-a).\end{align*}
	
	This means that the entropy of the uniform distribution is equal to the log of the width of the interval. This makes sense. As the size of the interval increases, $\log(b-a)$ increases too as there is a wider range of values which $X$ can take on.
}

\thrm{}{
	The exponential distribution with mean $\mu$ and probability density function $$f(x)=\begin{cases} \frac{1}{\mu} e^{-\frac{1}{\mu}x} & x\geq0 \\ 0 & x<0\end{cases}$$ is the entropy-maximising probability density function.
}

\tbf{Proof:} Let's try to apply our constrained optimisation from the discrete to the continuous case. Instead of the condition $\sum\limits_{i=1}^n p_i = 1$, we have that $$\int_X P(x) dx = 1.$$ Instead of the condition $\sum\limits_{i=1}^n x_ip_i = \mu$, we have that $$\int_X xP(x) dx = \mu.$$

It implies that rather than our Lagrangian being $$\mcL =\left(\sum\limits_{i=1}^n p_i \log p_i\right) - \lambda_1\left(\left(\sum\limits_{i=1}^n p_i\right)-1\right) - \lambda_2\left(\left(\sum\limits_{i=1}^n x_ip_i\right)-\mu\right)$$ in the discrete case, we have that $$\mcL = -\int_X P(x) \log (P(x))dx - \lambda_1\left(\int_X P(x) dx-1\right) - \lambda_2\left(\int_X xP(x) dx-\mu\right).$$

In the discrete case we would find partial derivatives for each of the $p_i$. However there are infinitely many $p_i$ here. To fix this, we have one first-order condition that satisfies $$\frac{d\mcL}{d P(x)} =0.$$

What are our first-order conditions? Take inspiration from the discrete case. Observe the partial derivatives of $\mcL$ with respect to the $p_i$. They are 
$$	\begin{cases}
	\frac{\partial \mcL}{\partial p_1} = -\log(p_1)-1-\lambda_1-\lambda_2x_1=0 \\
	\frac{\partial \mcL}{\partial p_2} = -\log(p_2)-1-\lambda_1-\lambda_2x_2=0 \\
	\ \ \vdots \\
	\frac{\partial \mcL}{\partial p_n} = -\log(p_n)-1-\lambda_1-\lambda_2x_n=0 \\
	\end{cases}
$$

In general, this is $$\frac{\partial \mcL}{\partial p_i} = -\log(p_i)-1-\lambda_1-\lambda_2x_i=0.$$

In the continuous case, this is \begin{align*} \frac{\partial \mcL}{\partial P(x)}=-\log (P(x)) - 1 - \lambda_1-\lambda_2 x&=0 \\ -\log(P(x)) &= 1+\lambda_1+\lambda_2x  \\ P(x) &= e^{-1-\lambda_1-\lambda_2x}. \end{align*}

We must not forget the conditions for both $\lambda_1$ and $\lambda_2$. Also, the sample space of $X$ is $[0, \infty)$.

Thus, we have that \begin{align*} \int_X P(x) dx &= -\left(\frac{e^{-1-\lambda_1-\lambda_2 x}}{\lambda_2}\right) \Big|^\infty_0 \\ &= 0-(-\frac{e^{-1-\lambda_1}}{\lambda_2})\\&=\frac{e^{-1-\lambda_1}}{\lambda_2}=1. \end{align*}

Using integration by parts, we have that \begin{align*} \int_X xP(x) dx &= -\left(\frac{(1+\lambda_2 x)e^{-1-\lambda_1-\lambda_2 x}}{\lambda_2^2}\right) \Big|^\infty_0 \\ &= \frac{e^{-1-\lambda_1}}{\lambda^2_2} \ \  \left(\text{using the fact that}\ \int_X P(x) dx = \frac{e^{-1-\lambda_1}}{\lambda_2}=1\right) \\ &= \frac{1}{\lambda_2} = \mu\end{align*}

Thus, we have that $\lambda_2=\frac{1}{\mu}$. Substitution into $\frac{e^{-1-\lambda_1}}{\lambda_2}=1$ yields $\lambda_1 = \log(\mu)-1$. 

The entropy-maximising function is therefore \begin{align*}P(x) &= e^{-1-(\log(\mu)-1)-\frac{1}{\mu}x} \\&= e^{-\log(\mu) - \frac{1}{\mu}x} \\&= \frac{1}{\mu} e^{-\frac{1}{\mu}x}, \end{align*}

which is an \tbf{exponential distribution}. 

\chapter{Week 10}

This week is review week, meaning there is no new content. This chapter discusses the most important or challenging aspects in weeks 1-9.

\section{Review of weeks 1-9}

By now, you should have found out that the Taylor approximation used in Week 1 was employed frequently in the past nine weeks. The linear Taylor approximation $f(x_0) + (x-x_0) f'(x_0)$ and quadratic Taylor approximation $f(x_0) + (x-x_0) f'(x_0) + \frac{1}{2} (x-x_0)^2 f''(x_0)$ appears as follows:

\begin{enumerate}
	\item (Week 3) In multiple variables, the quadratic approximation is $g(x, y) = g(x_0, y_0) + (x-x_0 \ y-y_0) \nabla g(x_0, y_0) + \frac{1}{2} (x-x_0 \ y-y_0) \cdot H(g(x_0, y_0)) \cdot \begin{pmatrix} x-x_0 \\ y-y_0 \end{pmatrix}$, where $H(g(x_0, y_0))$ is the Hessian matrix $H(g(x, y))$ evaluated at $(x_0, y_0)$.
	\item (Week 6) We used eigenvalues to classify the definiteness of the Hessian matrix, which allowed us to determine the sign of the second-order approximation. We also learned that the Hessian matrix is the tensor product of the gradient with itself.
	\item (Week 7) In a non-linear system of difference equations, we could approximate $\vec{x_t} = \vec{x_s}+\vec{\delta x_t}$, where $x_s$ is the stationary state, and give a linear approximation that involved the Jacobian matrix $\vec{\delta x_{t+1}} - \vec{\delta x_t} = \vec{\delta x_t} \cdot J_{\vec{f}}.$
	\item (Week 8) When performing a quadratic approximation of the Lagrangian function, we use the bordered Hessian instead of the Hessian. 
\end{enumerate}

Some of the constructions presented in these nine weeks were allusions to quantum physics and machine learning. These include the projector matrix of a vector $\vec{a}$ given by $\bbI - \vec{a} \otimes \vec{a}$, and the commutator $C(A, B)=AB-BA$ of two matrices $A$ and $B$. The tensor product of two vectors $\vec{a} \otimes \vec{b}$ is also used in quantum physics. 

The method of gradient learning (gradient descent and ascent) is used in machine learning. Gradient learning problems are applications of difference equations. One can perform linearisation along the stationary states to discover that the the Jacobian in the linearised difference equation is not only a Jacobian, but also a Hessian.

The orthonormal basis was used in various places this semester. These included calculations of the spectral representation of a matrix. This spectral representation contained sums of matrices, each containing the product of  an eigenvalue and the tensor product of its associated eigenvector with itself.

A lot of the work in the past nine weeks was very computationally heavy. These included the spectral theorem for symmetric matrices and the calculation of the projected bordered Hessian.

\chapter{Week 11}

This week's focus is on enhancing our understanding of constrained optimisation. In particular, we will discuss and explore the Envelope theorem.

\section{Marginal interpretation of the Lagrange multiplier}

Consider a standard Lagrange optimisation problem in two variables. Our goal is to maximise a two-variable function $f(x, y)$ with respect to the constraint $g(x, y) = b$ for some constant $b$. 

Let the maximum value of $f(x, y)$ with respect to $g(x, y)=b$ be $v(b)$. This function only takes in one parameter $b$. This section aims to show the following result:

\thrm{Marginal interpretation of the Lagrange multiplier}{
	Let $v(b)$ be the maximal value of $f(x, y)$ with respect to $g(x, y)$. Let $\lambda$ be the Lagrange multiplier at said maximum value as seen in the Lagrangian function $\mcL(x, y, \lambda) = f(x, y) - \lambda g(x, y).$ Then, $$\lambda = v'(b).$$
	
	This theorem interprets the Lagrange multiplier $\lambda$ as being the \tbf{rate of change of the maximal value of $f$ subject to its constraint}, evaluated at the value $b$ which produces said maximal value.
}

To show this, let's first consider a more simple problem: $$\text{to maximise}\ f(x, y)\ \text{with regards to}\ g(x, y)=0.$$ Let the values of $x$ and $y$ wherein $f(x, y)$ is maximised be $x^*, y^*$. Then, by definition, $$f(x^*, y^*) = v(0).$$

Consider the function $$H(x, y)=f(x, y) - v(g(x, y)).$$ Note that we use $v$ instead of $\lambda$, where $v$ is the maximal value of $f(x, y)$ subject to the constraint. Such a function satisfies the condition that $H$ attains its maximum at $(x^*, y^*)$. 

Why? Consider any point $(x, y)$, and let $k = g(x, y)$. Then, $f(x, y) \leq v(k)$ by definition. Equality is attained at the optimal value $(x^*, y^*)$ with $g(x^*, y^*)=k=0$. This means that \begin{align*} H(x^*, y^*) &= f(x^*, y^*) - v(g(x^*, y^*))\\ &= v(0)-v(0) \\ &= 0 \end{align*} which shows that $H(x, y)$ obtains its maximum at the constrained maximum value $(x^*, y^*)$. 

Perform partial differentiation on the function $H(x, y)$. We have that $$\frac{\partial H}{\partial x} = \frac{\partial f}{\partial x} - v'(g(x, y)) \frac{\partial g}{\partial x}.$$ Since $H(x, y)$ is maximised at $(x^*, y^*)$, we have that $\frac{\partial H}{\partial x}=0$, and thus $$\frac{\partial f}{\partial x} - v'(0) \frac{\partial g}{\partial x}=0;$$ similarly, $$\frac{\partial f}{\partial y} - v'(0) \frac{\partial g}{\partial y}=0.$$

These equations above are the first-order conditions of the Lagrangian equation $(\frac{\partial f}{\partial x} - \lambda \frac{\partial g}{\partial x}=0, \frac{\partial f}{\partial y} - \lambda \frac{\partial g}{\partial y}=0)$ with the substitution $\lambda = v'(0)$. 

We can use an analogous argument for $g(x, y)=b$ to indeed show that $$\lambda= v'(b).$$

\ex{Utility function and budget constraint}{Let's apply our newfound knowledge in the economic context of maximising a two-variable utility function $U(x_1, x_2)$ subject to the budget constraint $p_1x_1+p_2x_2=m$, where $m$ is a constant. 

But first, let us consider the Lagrangian function of our economic context. Our Lagrangian is $$\mcL(x_1, x_2, \lambda) = U(x_1, x_2)-\lambda(p_1x_1+p_2x_2-m).$$ The first-order conditions satisfy $\frac{\partial U}{\partial x_1} = \lambda p_1$ and $\frac{\partial U}{\partial x_2} = \lambda p_2.$ 

We use these two equations and the budget constraint $p_1x_1+p_2x_2-m=0$ to obtain solutions for $x_1, x_2$, and $\lambda$. Thus, the optimal values of $x_1$ and $x_2$ are a function of $p_1, p_2$, and $m$. We write $$x_1^* = f_1(p_1, p_2, m), \ \ x_2^*= f_2(p_1, p_2, m).$$

Thus, the utility function that produces maximum utility is indeed $U(x_1^*, x_2^*)$, which we can rewrite as $U(f_1(p_1, p_2, m), f_2(p_1, p_2, m))$, or an entirely new function $$V(p_1, p_2, m).$$

By the marginal interpretation of the Lagrange multiplier, we have that $$\lambda = \frac{\partial V}{\partial m}.$$

n.b. The functions $f_1(p_1, p_2, m), f_2(p_1, p_2, m)$ are known as the consumer's \tbf{demand functions}. The function $V(p_1, p_2, m)$ is the \tbf{indirect utility function}. 
}

Let's now generalise to $n$ variables. Here, we have a utility function $U(\vec{x})$ subject to the budget constraint $\vec{p} \cdot \vec{x} =m$. 

Consider the demand functions $f_1(\vec{p}, m), \dots, f_n(\vec{p}, m)$. By performing the chain rule on the function $U(f_1(\vec{p}, m), \dots, f_n(\vec{p}, m))$ with respect to the variable $m$, we obtain that \begin{align*}\frac{\partial}{\partial m} (U(f_1(\vec{p}, m), \dots, f_n(\vec{p}, m))) &= \frac{\partial U}{\partial x_1} \frac{\partial f_1(\vec{p}, m)}{\partial m}+\dots+\frac{\partial U}{\partial x_n} \frac{\partial f_n(\vec{p}, m)}{\partial m} \\ &= \lambda p_1 \frac{\partial}{\partial m} x_1 + \dots + \lambda p_n \frac{\partial}{\partial m} x_n  \\ &= \lambda(p_1 \frac{\partial}{\partial m}x_1+\dots + p_n \frac{\partial}{\partial m}x_n) \\ &= \lambda(\frac{\partial}{\partial m} (p_1x_1 + \dots + p_nx_n))  \\ &=\lambda  \end{align*}

where we used the first-order condition that $\frac{\partial U}{\partial x_i} = \lambda p_i$ for $i= 1, \dots n$. This result is identical to the fact that $\lambda = \frac{\partial V}{\partial m}$ - only that we used the language of demand functions instead of reformulating it in terms of the indirect utility function.

Also note that $\frac{\partial\mcL}{\partial m}=\lambda$. Thus, we have that \begin{equation}\frac{\partial U}{\partial m}=\lambda = \frac{\partial \mcL}{\partial m}. \end{equation}

This equation above is crucial in demonstrating the might of the envelope theorem.

\section{The envelope theorem}

The marginal interpretation of the Lagrange multiplier is an example of an envelope theorem. On the left-hand side of equation 11.1, we had a rather tedious total derivative of the function $$U(x_1(\vec{p}, m), \dots, x_n(\vec{p}, m)),$$ which involved knowledge of all the first-order conditions of the constrained optimisation problem. The right-hand side only involved the partial derivative with respect to one variable. This is evidently much simpler.

\thrm{Envelope theorem}{
	Consider the function $f(\vec{x}, \vec{t})$, where $\vec{x}$ contains $n$ variables and $\vec{t}$ contains $r$ variables. In this case, the variables $t_i\ (i=1, \dots, r)$ are fixed. 
	
	Let $v(\vec{t})$ be the maximal value of $f(\vec{x}, \vec{t})$ with respect to $\vec{x}$. The envelope theorem states that $$\frac{\partial v(\vec{t})}{\partial t_i} = \frac{\partial f(\vec{x}, \vec{t})}{\partial t_i}$$ for $i=1, \dots, r$.
}

\coro{}{ The key when employing the envelope theorem is to see which variables are being kept fixed, and which variables remain variable. The function $v$ is a function of all the variables kept fixed, and the function $f$ is a function of all the variables - no matter if they are fixed or variable. }

\ex{Envelope theorem applied to constrained maximisation}{
	Let's explore how this relates to the Lagrange multiplier and our function $v(b)$ in the beginning of the last section. Note that our function $v(b)$ was defined such that it is the maximum value of $f(x, y)$ with respect to $g(x, y)-b=0$. 

The function to be minimised here is $$\mcL(x, y, \lambda, b) = f(x, y) - \lambda(g(x, y)-b).$$ Note that we keep the variable $b$ fixed; the variables $x, y$, and $\lambda$ are variable. 

Let $v(b)$ denote the constrained maximum value of $f$, meaning the maximum value of $f$ with respect to $x, y$, and $\lambda$. Then, $$v'(b) = \frac{\partial \mcL}{\partial b} = \lambda.$$ This shows that the marginal interpretation of the Lagrange multiplier is an application of the envelope theorem.
}

Let's generalise the envelope theorem in the context of Lagrange optimisation. Let $f(\vec{x}, \vec{t})$ be a function of $n+r$ variables, where $\vec{x}$ contains $n$ variables and $\vec{t}$ contains $r$ variables. Take $\vec{t}$ as given and maximise the function $f(\vec{x}, \vec{t})$ subject to the constraints $g(\vec{x}, \vec{t})=0$. The Lagrangian is $$\mcL(\vec{x}, \lambda, \vec{t}) = f(\vec{x}, \vec{t})-\lambda g(\vec{x}, \vec{t}).$$ 

Let the maximal value of the function $f$ subject to the constraints be a function in $r$ variables $v(\vec{t})$. Then, the envelope theorem states that $$\frac{\partial v}{\partial t_i} = \frac{\partial \mcL}{\partial t_i},$$ where $i=1, \dots, r$. 

\coro{}{ Recall that in the section above, we established that $\frac{dU}{dm} = \lambda = \frac{\partial \mcL}{\partial m}$ where $\frac{dU}{dm}$ denotes total differentiation with respect to the variable $m$. Performing total differentiation with all the $t_i$ yields $$\frac{df}{dt_i} = \nabla_{\vec{t}} \ v = \nabla_{\vec{t}}\ \mcL.$$ This is what we see in Frank's lecture notes - but in practice it is much more common to consider $\frac{\partial v}{\partial t_i} = \frac{\partial \mcL}{\partial t_i}.$}

What about when there are multiple constraints at play? Consider a Lagrangian function with constraints $g_1(\vec{x}, \vec{t})=0$ and $g_2(\vec{x}, \vec{t})=0$. Then, the Lagrangian would be $$\mcL(\vec{x}, \lambda_1, \lambda_2, \vec{t}) = f(\vec{x}, \vec{t}) - \lambda_1 g_1(\vec{x}, \vec{t}) - \lambda_2 g_2(\vec{x}, \vec{t}),$$ and we could apply the envelope theorem as usual.

In general, we would have a Lagrangian function $$\mcL(\vec{x}, \vec{\lambda}, \vec{t}).$$  

Are there restrictions on the number of variables in $\vec{x}, \vec{\lambda}$, and $\vec{t}$? It turns out that there are. The Lagrangian holds for the number of distinct constraints (number of variables in $\vec{\lambda}$) \tbf{not exceeding the number of variables in $\vec{x}$}. This result owes to an argument about existence of unique solutions to a set of equations; the relevant theory can be found in discussions of matrix rank in Week 5.

In fact, consider the situation where there are $n$ variables and $n$ constraints. Assume the constraints are all distinct. This would lead to situations where there are two constraints that yield one unique solution (the constraints fix the unique solution) or no unique solutions. In both situations, optimisation is meaningless. Therefore, the number of distinct constraints \tbf{must be less than the number of variables in $\vec{x}$.}


\chapter{Week 12}

In this chapter, we explore more applications of Lagrange optimisation and the envelope theorem. 

This week starts with an exploration of inequality constraints. We then discuss the Kuhn-Tucker conditions. We then note how the discussion of the Kuhn-Tucker conditions plays out in the context of the Legendre transform, a means of variable transformation extensively employed in physics and the natural sciences but also in economics. 

\section{Inequality constraints and the Kuhn-Tucker conditions}

A common constraint in economic applications of Lagrange multipliers is the positivity constraint. They are the condition where certain economic variables must have non-negative values.

An example is our well-known two-variable utility function subject to a budget constraint. Consider when we maximise $U(x_1, x_2)$ subject to the budget constraint $p_1x_1+p_2x_2=b$. Now we add the positivity constraints $x_1 \geq0, x_2 \geq 0$. 

In this case, the Lagrangian is $$\mcL(x_1, x_2, \lambda)=U(x_1, x_2)-\lambda(p_1x_1+p_2x_2-m).$$ We have three cases.

\tbf{Case 1:} $x_1>0, x_2>0$, $\frac{\partial \mcL}{\partial x_1}=\frac{\partial \mcL}{\partial x_2}=0$. We are familiar with this case.

\tbf{Case 2:} $x_1>0, x_2=0$. This means that $\frac{\partial \mcL}{\partial x_1}=0$; however, $\frac{\partial \mcL}{\partial x_2}\leq 0$. 

\tbf{Case 3:} $x_1=0, x_2\geq0$. This means that $\frac{\partial \mcL}{\partial x_1}\leq0$; however, $\frac{\partial \mcL}{\partial x_2}= 0$. 

\defn{Active and inactive constraints}{There are two types of constraints - active and inactive constraints. Given an optimisation problem subject to the inequality constraint $g(x_1, x_2)\leq 0$, active constraints are where the constraint $g(x_1^*, x_2^*)=0$, and inactive constraints are where $g(x_1^*, x_2^*)<0$.

}

\coro{}{ In some texts, active constraints are called tight constraints, and inactive constraints are slack constraints.}

The Kuhn-Tucker conditions demonstrate the conditions that the Lagrange multipliers need to satisfy. 

\thrm{Kuhn-Tucker conditions for two variables}{
	Consider the Lagrangian function $\mcL(x, y, \lambda) = f(x, y) - \lambda g(x, y)$ and let $(x, y) = (x^*, y^*)$ be a solution to the problem. Let $x=x^*$ and $y=y^*$ be an optimum. Then, there exists a number $\lambda^*$ with the following properties: $$\frac{\partial \mcL}{\partial x}=\frac{\partial \mcL}{\partial y}=0\ \text{at}\ x=x^*, y=y^*$$ and $$\lambda^* \geq0, g(x^*, y^*) \leq 0, \lambda^* g(x^*, y^*) =0.$$
	
	What this means is that either when $g(x^*, y^*)=0$, then $\lambda^* \geq 0$; when $g(x^*, y^*) <0$, then $\lambda^* =0$. 
	
	\coro{}{These conditions were named after Harold W. Kuhn and Albert W. Tucker, from a 1951 publication. The conditions were separately stated by William Karush in 1939 in his master's thesis. These conditions in nonlinear optimisation are thus commonly named the Karush-Kuhn-Tucker conditions. }
}

\tbf{Intuition:} I'd like to shed some light on the intuition behind the names \emph{active} and \emph{inactive} constraints. Inactive constraints occur when $g(x_1^*, x_2^*)<0$. By the Kuhn-Tucker theorem, this is equivalent to $\lambda=0$; thus, the $\lambda g(x, y)$ term vanishes. Intuitively, this is akin to the Lagrange multiplier being "inactive", as the fact that $\lambda=0$ suggests that it holds no bearing on Lagrangian optimisation. Conversely, if $g(x_1^*, x_2^*)<0$, the constraint is active as $\lambda \geq0$.

The focus of this section is on a inconvenience that occurs when solving inequality constraints. The Kuhn-Tucker conditions for $n$ variables and $m$ constraints, where $m<n$, is very similar to the case for two variables and one constraint. I leave the generalised formulation of these conditions as an exercise to the reader.

Here is an example where our usual Lagrangian optimisation method violates the Kuhn-Tucker conditions.

\ex{Pemberton chapter 18.4}{
	Consider the function $$8\sqrt{x}+2\sqrt{y}$$ subject to the constraints $$2x+y \leq 3, 2y+x \leq 3, x\geq0, y\geq0.$$ Solving this optimisation problem using our usual methods, we have that $$\mcL(x, y, \lambda, \mu) = 8\sqrt{x}+2\sqrt{y} - \lambda(2x+y-3) - \mu(2y+x-3),$$ where $\lambda$ and $\mu$ are our Lagrange multipliers.
	
	Our first-order conditions obtained by taking $\frac{\partial \mcL}{\partial x}=0, \frac{\partial \mcL}{\partial y}=0$ are given by the equations (12.1) and (12.2) below. The Kuhn-Tucker conditions are equations (12.3) and (12.4). These equations are: \begin{align} \frac{4}{\sqrt{x}} &= 2\lambda + \mu \\ \frac{1}{\sqrt{y}} &= \lambda + 2\mu \\ \lambda &\geq 0 \ \text{since}\ 2x+y \leq 3 \\ \mu &\geq 0 \ \text{since}\ 2y+x \leq 3 \end{align} 
	
	Note that conditions (12.3) and (12.4) are equivalent to \begin{align} \lambda (2x+y-3)&=0, \lambda \geq0 \\ \mu(2y+x-3)&=0, \mu\geq0\end{align}
	
	Our first guess is to find a solution where both constraints are active, meaning that $2x+y=3$ and $2y+x=3$. A solution to this set of equations is $(x, y)= (1, 1)$, which yields $\lambda= \frac{7}{3}, \mu=-\frac{2}{3}$. This violates the Kuhn-Tucker conditions; more specifically, the fact that $\mu \geq0$.
	
	In this case, what we do is try to set $\mu=0$ and leave $\lambda >0$ and hope to obtain an optimum that satisfies the Kuhn-Tucker conditions. 
	
	When substituting $\mu=0$ into the first-order conditions (12.1) and (12.2), we have that $x=4\lambda^{-2}$ and $y=\lambda^{-2}$. Let's substitute back into (12.3) to obtain $2x+y=3$, or $9\lambda^{-2}=3$. This occurs if and only if $\lambda=\sqrt{3}$. 
	
	Therefore, we have values $\lambda=\sqrt{3}, \mu=0, x=\frac{4}{3}, y=\frac{1}{3}$. Note that $\frac{4}{3}+2\cdot \frac{1}{3}=2$, so we have found a set of values that satisfy the Kuhn-Tucker conditions. Our desired solution is $x=\frac{4}{3}, y=\frac{1}{3}$, yielding us a constrained maximum of $6\sqrt{3}$ which exceeds the value 10 obtained using $x=1, y=1$.
	
	This demonstrates that taking equality in a constrained optimisation problem may not yield us said optima! Sometimes we have to cut ourselves some slack (pun intended) and find out that inactive, or slack constraints, are what is required to yield us our optima.
	
	In fact, the fact that optimality may not be achieved at equality shows that ordinary Lagrange optimisation using equality constraints may lead to a Pareto suboptimal outcome. When one or more constraints are inactive, one can employ less resources while simultaneously achieving a greater amount of utility.
}	

When dealing with inequality constraints, we follow these procedures:
\begin{enumerate}
	\item Attempt to solve constrained optimisation problem assuming that the inequality conditions are active; we replace the $\leq$ or the $\geq$ with an equal sign.
	\item After obtaining an optimum, check to see if the Kuhn-Tucker conditions are satisfied. If they are, we are done.
	\item If the Kuhn-Tucker conditions are not satisfied, replace the value that violates the Kuhn-Tucker conditions with the closest value from it that still satisfies the condition. For example, if the Kuhn-Tucker conditions state that $\lambda \geq0$ but our active conditions yield $\lambda = -\frac{4}{5}$, we take $\lambda=0$ and continue working with our first-order conditions. 
	\item We then obtain a new set of optima that fulfils the Kuhn-Tucker conditions wherein one or more of the conditions are inactive, or slack.
\end{enumerate}

Aren't $x\geq 0, y\geq 0$ also conditions that we need to account for? In the example above, why have we included Lagrange multipliers for $2x+y\leq 3$ and $2y+x\leq 3$ but not for $x\geq0$ and $y\geq0$? Notice that the natural domain of $\sqrt{x}$ and $\sqrt{y}$ are $x, y\geq0$, so the last two constraints are somewhat redundant. 

\section{Changing the Lagrangian function}

Consider a function $f(x,y)$ subject to the constraints $x\geq0$ and $y\geq0$. Our Lagrangian function, taking into account these two constraints, is $$\mcL(x, y, \lambda, \mu) = f(x, y )-\lambda x - \mu y.$$ This might raise some red flags. Towards the end of section 2 of last week's notes, I wrote that if the number of constraints equals the number of variables, then optimisation would be meaningless due to the constraints fixing one unique solution, or there being no unique solutions. 

These red flags only occur when equality constraints are at play. When considering inequality constraints, the Kuhn-Tucker conditions allow us to consider more constraint variables $\lambda_i$ than variables $\vec{x}$ as the conditions produce additional constraints out of the $\lambda_i$ and $\vec{x}$.

Our first-order conditions are \begin{align*} \frac{\partial \mcL}{\partial x} &= \frac{\partial f}{\partial x} - \lambda =0 \\ \frac{\partial \mcL}{\partial y} &= \frac{\partial f}{\partial y} - \mu = 0  \\ \frac{\partial \mcL}{\partial \lambda} &= -x=0 \\ \frac{\partial \mcL}{\partial \mu} &= -y=0\end{align*}
This set of conditions yields $x=0$ and $y=0$. Due to the fact that we have 2 variables and two constraints, that we are forced to adopt our active (equality) constraints. To work with inactive constraints, where $x>0$ and / or $y>0$, we have to set $\lambda=0$ and / or $\mu=0$ and proceed.

What if I don't want to fix the fact that $x=0$ and $y=0$? Consider a \emph{modified} Lagrangian $$\mcL = f(x, y) - \lambda^2x-\mu^2y$$ where instead of taking $\lambda$ and $\mu$ to be the Lagrange multipliers, we take $\lambda^2$ and $\mu^2$ instead. Our new first-order conditions are \begin{align*} \frac{\partial \mcL}{\partial x} &= \frac{\partial f}{\partial x} - \lambda^2 =0 \\ \frac{\partial \mcL}{\partial y} &= \frac{\partial f}{\partial y} - \mu^2 = 0  \\ \frac{\partial \mcL}{\partial \lambda} &= -2x\lambda=0 \\ \frac{\partial \mcL}{\partial \mu} &= -2y\mu=0\end{align*}

In this instance, we don't have to set $\lambda=0$ and $\mu=0$ by hand. They occur as solutions to the first-order conditions. Notice how in solving optimisation problems with inequality constraints, we had to first assume that the constraints were active and check with the Kuhn-Tucker conditions to determine if the optimum was attained when the constraints were active, some active and some inactive, or both inactive. 

\ex{Modified Lagrangian function}{
	Consider the Lagrangian function in our previous example. Let's make it easier for our own sake - that we need to maximise $U(x, y)=8\sqrt{x}+2\sqrt{y}$ subject to the inequality constraints $2x+y\leq 3, 2y+x\leq 3$. Our modified Lagrangian function is $$\mcL(x, y, \lambda, \mu) = 8\sqrt{x}+2\sqrt{y}-\lambda^2(2x+y-3)-\mu^2(2y+x-3).$$ The first-order conditions are \begin{align*} \frac{\partial \mcL}{\partial x} &= \frac{4}{\sqrt{x}}-2\lambda^2-\mu^2=0 \\ \frac{\partial \mcL}{\partial y} &= \frac{1}{\sqrt{y}}-\lambda^2-2\mu^2 =0 \\ \frac{\partial \mcL}{\partial \lambda} &= -2\lambda(2x+y-3)=0\\ \frac{\partial \mcL}{\partial \mu} &= -2\mu(2y+x-3)=0 \end{align*}
	
	We can classify our solutions by the number of active constraints.
	
	\tbf{Case 1: No active constraints.} In this case, we have that $\lambda=\mu=0$. However this would then entail that $\frac{4}{\sqrt{x}}=0$ and $\frac{1}{\sqrt{y}}=0$, which would be absurd. Indeed, a solution would require one or both constraints to be active.
	
	\tbf{Case 2: One of the constraints are active}. There are two sub-cases: $\lambda=0, \mu\neq0$, and $\lambda\neq 0, \mu=0$. 
	
	\emph{Case 2.1: $\lambda=0, \mu\neq0$.} In this case, our first two first-order conditions simplify to $\mu^2 = \frac{4}{\sqrt{x}}$ and $2\mu^2 = \frac{1}{\sqrt{y}}$. Also, since $\mu\neq0$, we have that $2y+x-3=0$. Simplification yields the solutions $x=\frac{32}{11}, y=\frac{1}{22}$. Under these conditions, $\mu^2=\sqrt{\frac{11}{2}}$, so $\lambda=0$ and $\mu=\pm(\frac{11}{2})^{\frac{1}{4}}$. We have two solutions. However we cannot admit this solution as it violates the constraint $2x+y\leq 3$. 
	
	\emph{Case 2.2: $\mu=0, \lambda\neq0$.} Similarly, we have that $\lambda^2 = \frac{2}{\sqrt{x}}$ and $\lambda^2 = \frac{1}{\sqrt{y}}$. Since $\lambda\neq0$, the constraint $2x+y-3=0$ must hold. Simplification yields the solutions $x=\frac{4}{3}, y=\frac{1}{3}.$ Under these conditions, $\lambda^2=\sqrt{3}$, meaning that the multipliers satisfy $\lambda=-3^{\frac{1}{4}}$ and $\mu=0$. Again, we have two solutions.
	
	\tbf{Case 3: Both constraints active.} In this case, we have that $\lambda \neq 0$ and $\mu \neq 0$. Therefore, $2x+y-3=0$ and $2y+x-3=0$, which implies that $x=1, y=1$. This would mean that $4-2\lambda^2-\mu^2=0$ and $1-\lambda^2-2\mu^2=0$, implying that $\mu^2=-\frac{2}{3}$ and $\lambda^2=\frac{7}{3}$, suggesting that $\mu = \pm\sqrt{\frac{2}{3}}i$ and $\lambda = \pm \sqrt{\frac{7}{3}}$.
	
	Effectively, there are four solutions in this case. However, all four solutions involve $\mu$ to be imaginary. However, note that the Lagrange multipliers in our original method are now replaced by their squares. The existence of imaginary Lagrange multipliers suggest that their squares are negative.
	
	It remains for us to check our three solutions and see which one maximises $8\sqrt{x}+2\sqrt{y}$. We find that $x=\frac{4}{3}, y=\frac{1}{3}$ maximises the utility function. This verifies our result from when we used the Kuhn-Tucker conditions.
}

Since we've done Lagrange optimisation, it makes sense for us to compute our bordered and projected bordered Hessians like we've done in Week 8. Note that since there are two constraints, our projected bordered Hessian utilises two projector matrices $P_\mu$ and $P_\lambda$. Upon constructing the bordered Hessian $H_B$, we compute $$H_{PB} = P_\lambda \cdot P_\mu \cdot H_B \cdot P_\mu \cdot P_\lambda.$$

When one or more of the constraints is active, we must perform variations in the $x$ or $y$ directions. The new projected bordered Hessian becomes $$H_{PB} = P_x \cdot P_\lambda \cdot P_\mu \cdot H_B \cdot P_\mu \cdot P_\lambda \cdot P_x$$ or $$H_{PB} = P_y \cdot P_\lambda \cdot P_\mu \cdot H_B \cdot P_\mu \cdot P_\lambda \cdot P_y,$$ or if both of the constraints are active, $$H_{PB} = P_x \cdot P_y \cdot P_\lambda \cdot P_\mu \cdot H_B \cdot P_\mu \cdot P_\lambda \cdot P_y \cdot P_x.$$

\ex{Second-order conditions of the modified Lagrangian function}{
	Note that the bordered Hessian is $$H_B=\begin{pmatrix}\frac{\partial^2\mcL}{\partial \lambda^2} & \frac{\partial^2\mcL}{\partial \lambda\mu} & \frac{\partial^2\mcL}{\partial \lambda x} & \frac{\partial^2\mcL}{\partial \lambda y} \\ \frac{\partial^2\mcL}{\partial \mu\lambda} & \frac{\partial^2\mcL}{\partial \mu^2} & \frac{\partial^2\mcL}{\partial \mu x} & \frac{\partial^2\mcL}{\partial \mu y} \\ \frac{\partial^2\mcL}{\partial x \lambda} & \frac{\partial^2\mcL}{\partial x \mu} & \frac{\partial^2\mcL}{\partial x^2} & \frac{\partial^2\mcL}{\partial xy} \\ \frac{\partial^2\mcL}{\partial y\lambda} & \frac{\partial^2\mcL}{\partial y\mu} & \frac{\partial^2\mcL}{\partial yx} & \frac{\partial^2\mcL}{\partial y^2} \end{pmatrix}.$$
	
	In the case where the Lagrange multiplier was not replaced with its square, $\frac{\partial^2\mcL}{\partial \lambda^2}$ and $\frac{\partial^2\mcL}{\partial \mu^2}=0$. However, since the second derivative of $\lambda^2$ with respect to $\lambda$ is 2, we would obtain an expression that is twice the constraint. In the scenario above, the bordered Hessian is $$H_B=\begin{pmatrix} -4x-2y+6 & 0 & -4\lambda & -2\lambda \\ 0 & -4y-x+6 & -2\mu & -4\mu \\ -4 \lambda & -2 \mu & -\frac{2}{x^{3/2}} & 0 \\ -2\lambda & -4\mu & 0 & -\frac{1}{2y^{3/2}} \end{pmatrix}$$
	
	Under the optimal conditions $x=\frac{4}{3}, y=\frac{1}{3}, \lambda=-3^{\frac{1}{4}}, \mu=0$, the bordered Hessian is $$H_B=\begin{pmatrix} 0 & 0 & -4\cdot -3^{\frac{1}{4}} & -2\cdot -3^{\frac{1}{4}} \\ 0 & \frac{10}{3} & 0 & 0 \\ -4\cdot -3^{\frac{1}{4}} & 0 & -\frac{2}{(4/3)^{3/2}} & 0 \\ -2\cdot -3^{\frac{1}{4}} & 0 & 0 & -\frac{1}{2(1/3)^{3/2}} \end{pmatrix}$$
	
	To obtain the projected bordered Hessian, note that the normalised gradients for the $\lambda$ and the $\mu$ constraints are $\hat{e_1}=\begin{pmatrix} 0 \\ 0\\  \frac{2}{\sqrt{5}}\\ \frac{1}{\sqrt{5}} \end{pmatrix}$ and $\hat{e_2}= \begin{pmatrix} 0 \\ 0\\ \frac{1}{\sqrt{5}}\\ \frac{2}{\sqrt{5}} \end{pmatrix}$ respectively. 
	
	Thus, the projector matrices for $\lambda$ and $\mu$ are $$P_\lambda = \bbI - \hat{e_1} \otimes \hat{e_1} = \begin{pmatrix} 1 & 0 &0 & 0 \\ 0&1&0&0\\0 &0& \frac{1}{5} & -\frac{2}{5} \\ 0 & 0&-\frac{2}{5} & \frac{4}{5} \end{pmatrix}$$ and $$P_\mu=\bbI - \hat{e_1} \otimes \hat{e_1} = \begin{pmatrix} 1 & 0 &0 & 0 \\ 0&1&0&0\\0 &0& \frac{4}{5} & -\frac{2}{5} \\ 0 & 0&-\frac{2}{5} & \frac{1}{5} \end{pmatrix}$$ respectively, and one can multiply accordingly to obtain the projected bordered Hessian.
}

\section{The Legendre transform}

The Legendre transform is a means of variable transform largely employed in physics and engineering, but it also finds great use in economics and mathematics. The means of conducting the Legendre transform seems non-intuitive until we explore examples using the transform. However, it ties in pretty well with our previous work on the envelope theorem.

Suppose we have a function $P(x, y) = f(x)-xy$. We want to find the optimum of $P(x, y)$ while considering variations in $x$. 

\defn{Legendre transform in one variable}{
	Let $f(x)$ be a function. Its Legendre transform is a function in a \emph{conjugate variable} $y$, given by $$f^*(y) = \max\limits_x (f(x)-xy).$$
	
	Conjugate variables are variables that are inherently related. 
} 

Upon seeing $\max\limits_x$ in the definition of the Legendre transform, you might be reminded of the envelope theorem. You are right! Recall that in the single-variable version of the envelope theorem, we let $v(t)$ be the maximal value of $f(x, t)$ with respect to $x$. The envelope theorem then states that $v'(t) = \frac{\partial f}{\partial t}$. 

In this instance, the envelope theorem implies that \begin{align*}\frac{d}{dy} f^*(y) &= \frac{\partial}{\partial y} (f(x)-xy) \\ &= -x. \end{align*}

Consider the value of $f^*(y)$ at its optimum. The first-order conditions satisfy $$\frac{\partial}{\partial x} (f(x)-xy)=0,$$ or $$f'(x)=y.$$ This suggests that the conjugate variable $y$ has a special meaning - that at the optimum, \tbf{$x$ is a function of $y$.} Therefore, the implication of the envelope theorem is that $$f^*(y) = -x(y);$$ and thus \begin{equation}\frac{d^2}{dy^2}f^*(y) = -\frac{dx}{dy} \end{equation} and from the first-order conditions, \begin{equation}\frac{d^2}{dx^2} f(x) = \frac{dy}{dx}.\end{equation}

Using the two equations above, we conclude that $$\frac{d^2 f^*(y)}{dy^2} = -\left(\frac{d^2 f(x)}{dx^2}\right)^{-1}.$$

We can generalise this to many variables. Consider a multivariable function $f(\vec{x})$ and its Legendre transform $$f^*(y) = \max\limits_{\vec{x}} (f(\vec{x}) - \vec{x} \cdot \vec{y}).$$ As we've done many times in the past, we replace the ordinary second derivative with the Hessian matrix when going from one variable to multiple variables. Therefore, we have that $$\begin{pmatrix} \frac{\partial^2 f^*(\vec{y})}{\partial y_1^2} & \dots &  \frac{\partial^2 f^*(\vec{y})}{\partial y_1y_n} \\ \vdots & \ddots & \vdots \\  \frac{\partial^2 f^*(\vec{y})}{\partial y_ny_1} & \dots &  \frac{\partial^2 f^*(\vec{y})}{\partial y_n^2}\end{pmatrix}=-\begin{pmatrix} \frac{\partial^2 f(\vec{x})}{\partial x_1^2} & \dots &  \frac{\partial^2 f(\vec{x})}{\partial x_1x_n} \\ \vdots & \ddots & \vdots \\  \frac{\partial^2 f^*(\vec{x})}{\partial x_nx_1} & \dots &  \frac{\partial^2 f^*(\vec{x})}{\partial x_n^2}\end{pmatrix}  $$

\coro{}{ Frank uses the notation $g(y)$ instead of $f^*(y).$ I stick with the notation $f^*(y)$ as it makes sense of some of the properties of the Legendre transform. More specifically, $$f^{**}(y)=f(x). \ \text{(check this yourself)}$$}

Additionally, the Legendre transform is defined as $$f^*(y) = \max\limits_x (xy-f(x))$$ in many physics texts. This is not a major issue as the upshots of the Legendre transform with regards to its derivatives remain virtually indistinguishable under both definitions. The first-order conditions don't change. Under this revised definition, there is no minus sign in the relationship between the second derivatives and between the Hessian matrices.

Evidently, the definition $f^*(y) = \max\limits_x (xy-f(x))$ is made under the presupposition that $f(x)$ and $f^*(y)$ are both convex functions. Conversely, $f^*(y) = \max\limits_x (f(x)-xy)$ assumes that both functions are concave.

\ex{Legendre transform of a single-variable function}{
	Consider the function $f(x)=e^x$. Then, $f'(x)=e^x$, suggesting that the Legendre transform $$f^*(y)=e^x-xy$$ also satisfies $f'(x)=y$, meaning $y=e^x$, or $x = \log y$. The Legendre transform now becomes \begin{align*} f^*(y) &= e^x-xy \\ &= y- y \log y. \end{align*}
	
	You can check to see that the second derivative condition is satisfied.
}

\chapter{Week 13}

This week's content is much more abstract in nature than the previous two weeks. In this week, we explore the linear algebra of function spaces, or maps between spaces of functions. We also discuss functions on subsets of $\bbR$ and how they form vector spaces. In particular, we view the derivative as a linear operator, which can be represented using a matrix. 

\section{Maps between functions: discrete sets}

Last week we saw the Legendre transform as a way of transforming a function $f(x)$ into a function in a conjugate variable $y$ through the relationship $$f^*(y) = \max\limits_x (f(x)-xy).$$ This is a transformation between two spaces of functions - $f(x)$ for functions with $x$ as their input, and $f^*(y)$ for functions with $y$ as their input. Explicitly, we represent this relationship as a mapping $$f \mapsto f^*.$$

Before exploring mappings between functions defined on vector spaces such as $\bbR$, it might make more sense for me to define functions on \emph{discrete sets}. 

\coro{}{ This content may appear abstract. It gets less abstract and more approachable when we discuss applications of function spaces in Week 14 in difference and differential equations.}

\defn{Discrete set}{
	A discrete set $\mcI$ is a set of objects wherein each object is isolated from the others and each object in the set can be labelled with a number. Explicitly, there is a map $$z: \bbN \to \mcI$$ that maps each natural number to an element within the set.
}

This means that for each $j\in \bbN$, we can associate $j$ with an element $z(j) \in \mcI$. This is what "labelling the discrete set" means.

\ex{Discrete set}{
	The set $\mcI = \{ n \mid n \in \bbZ\}$ is a discrete set as one can label each element of $\mcI$ using the natural numbers. But in fact one can label any finite or countably infinite set with elements of the natural numbers.
}

Define the set associated with the labelling of the discrete set $\mcI$ as $\mcI_n = \{1, 2, \dots, n\} \subset \bbN$, and consider the functions $x: \mcI_n \to \bbR$. Though the mappings from each $\mcI_j$ where $j=1, \dots, n$ to $\bbR$ are a list, they possess an underlying vector space structure. 

Let $\vec{a}$ and $\vec{b}$ be vectors, and let $k \in \bbR$ be a scalar. Recall that a vector space $V$ satisfies the following axioms:

\begin{enumerate}
	\item If $\vec{a}, \vec{b} \in V, \vec{a}+\vec{b} \in V.$
	\item If $k\in \bbR$ and $\vec{a} \in V$, then $k\vec{a} \in V$.
\end{enumerate}

The key point here is that a discrete set $\mcI_n$ has an underlying vector space structure. Elements of $\mcI_n$ satisfies the above axioms. Recall that every vector space has basis vectors that, upon certain linear combinations of said vectors, can produce every vector in the vector space. This property is known as \emph{span}, with basis vectors given by $$\hat{(e_j)}_k = \begin{cases}1 & \text{if}\ j=k, \\ 0 & \text{if}\ j \neq k. \end{cases}$$ 

Another property of vectors that you might recognise is the property of linear independence. More specifically, if a linear combination $\sum\limits_{j=1}^n f_j e_j(k)=0$, then $e_j(k)=0$ for all $j$. 

Note that these vectors, instead of basis vectors, are \tbf{basis functions} that act on $\mcI_n$. It'll thus be more appropriate for me to $$e_j(k) = \begin{cases}1 & \text{if}\ j=k, \\ 0 & \text{if}\ j \neq k. \end{cases}$$

Thus every function on the discrete set $\mcI_n$ is a linear combination of the $e_j(k)$. More specifically, $$f(k) = \sum\limits_{j=1}^n f_j e_j(k).$$

\tbf{Abuse of notation:} In our proofs below, we write $e_j(k)$ as $e(j)$.

The concepts of linear algebra that we learned in Term 1 are transferrable to this week's content. However, we use function notation $e_j(k)$ instead of vector notation $(e_j)_k$. The mental exercise at hand here is to treat our new material on function spaces just as you would with any old vector space, e.g. $\bbR$.

To exemplify this analogy between vector spaces and function spaces, observe that for two vectors $\vec{f}$ and $\vec{g}$, their dot product is $$\vec{f} \cdot \vec{g} = f_1g_1+\dots+f_ng_n = \sum\limits_{j=1}^n f_jg_j.$$ Similarly, the dot product on discrete sets is given by $$f \cdot g = \sum\limits_{j \in \mcI_n} f(j) g(j)$$ which equals $\sum\limits_{j=1}^n f(j)g(j)$ since $\mcI_n$ is indexed from $j=1, \dots, n$.

The theory behind basis change can also be exemplified in the theory of function spaces. Suppose that we represent a function $$f(j) = \sum\limits_{i=1}^n \bar{f_j} \bar{e(j)}.$$

We explored a means of basis change when we introduced the tensor product in Week 6. Since $\bbI = \sum\limits_{k=1}^n e(k) \otimes e(k)$, we have that \begin{align*} f \cdot \bbI &= \left(\sum\limits_{j=1}^n \bar{f_j} \bar{e(j)}\right) \cdot \left(\sum\limits_{k=1}^n e(k) \otimes e(k)\right) \\ &= \sum\limits_{j=1}^n \sum\limits_{k=1}^n \bar{f_j} (\bar{e(j)} \cdot (e(k) \otimes e(k))) \\ &= \sum\limits_{j=1}^n \sum\limits_{k=1}^n \bar{f_j} (\bar{e(j)} \cdot e(k))e(k) \\ &= \sum\limits_{k=1}^n \left(\sum\limits_{j=1}^n \bar{f_j} (\bar{e(j)} \cdot e(k)) e(k)\right) \\ &= \sum\limits_{k=1}^n f_k e(k) \\ & =f. \end{align*} Although it is painstakingly obvious that $f \cdot \bbI = f$, note that in our first row, we have that $f =\sum\limits_{j=1}^n \bar{f_j} \bar{e(j)}$. In our last row, we have that $f = \sum\limits_{k=1}^n f_k e(k)$. This demonstrates how any set of basis vectors $\bar{e(j)}$ can be changed to our canonical basis $e(k)$. 

\section{Maps between functions: functions on $\bbR$}

Most of our theory on function spaces when the underlying space is a discrete set is made more concrete when the underlying space is the real numbers $\bbR$. More specifically, we are discussing functions on subsets of the real numbers $\bbR$. Note that the functions on discrete sets played the role of vectors. The linear algebra was very much analogous to the linear algebra we worked with in Term 1. When the underlying space is $\bbR$, the linear algebra is a bit more unfamiliar.

Let $[a, b] \subset \bbR$, and define a set of functions $f: [a, b] \to \bbR$ denoted $V$. Additionally, we must have that the set of functions satisfy the vector space axioms

\begin{enumerate}
	\item If $f, g \in V, f+g \in V.$
	\item If $k\in \bbR$ and $f \in V$, then $kf \in V$.
\end{enumerate}

\ex{Function spaces on subsets of $\bbR$}{Consider the set of functions $$V = \{f: [a, b] \to \bbR \mid f(x) \geq 0\}.$$ Is $V$ a vector space? 

Let $f(x), g(x)\geq 0$. Axiom 1 says that $f(x)+g(x) \geq 0$, which checks out. However, if $k<0$, then $kf(x) <0$ and thus $kf(x) \not\in V$, which violates Axiom 2. Thus, $V$ is not a vector space.

Consider the set of functions that satisfy $$W = \{f: [a, b] \to \bbR \mid f(a+b) = f(a)+f(b)\}.$$ 

Is $W$ a vector space? If $f(a+b) = f(a)+f(b)$ and $g(a+b)=g(a)+g(b)$, then $f(a+b)+g(a+b) = (f(a)+g(a)) + (f(b)+g(b))$ which is in $W$. Under scalar multiplication, we have that $cf(a+b) = cf(a)+cf(b)$, which is in $W$. Therefore, $W$ is a vector space.

\coro{}{ The \emph{functional equation} $f(a+b) = f(a)+f(b)$ is also known as Cauchy's functional equation, named after French mathematician Augustin-Louis Cauchy.}

Consider the set of functions $$X = \{f: [a, b]^{n+1} \to\bbR \mid f(x)=a_0+a_1x+\dots+a_nx^n, a_i \in [a, b] \}.$$ Note that this set of functions takes in $n+1$ copies of $[a+b]$ for each coefficient of $f$, and spits out the value of $f(x)$.

Is $X$ a vector space? We can simply check by letting $f(x)=a_0+a_1x+\dots+a_nx^n$ and $g(x)=b_0+b_1x+\dots+b_nx^n$, where $a_i, b_i \in [a, b]$. Check that $$f(x)+g(x) = (a_0+b_0)+(a_1+b_1)x+\dots+(a_n+b_n)x^n.$$ However we might have that $a_i+b_i \not\in [a, b]$, so the first axiom fails and $X$ is not a vector space.

If we replace the condition that $a_i, b_i \in [a, b]$ with $a_i, b_i \in \bbR$, then you can check both axioms to see that $X$ is a vector space.
}

We briefly discussed linear independence when discussing the linear algebra of discrete sets. It also holds when we discuss maps between spaces of functions on $\bbR$. 

\defn{Dot product on $\bbR$}{
	Let $f(x)$ and $g(x)$ be two functions $f, g: [a, b] \to \bbR$. Their dot product is $$f \cdot g = \int_a^b f(x) g(x)\ dx$$ 
}

This is a generalisation of our dot product on discrete sets. Recall that the dot product is $f \cdot g = \sum\limits_{j \in \mcI_n} f(j) g(j)$ which equals $\sum\limits_{j=1}^n f(j)g(j)$; our result above is the continuous form of the dot product on discrete sets.

With a dot product, we can identify two functions $f$ and $g$ as being orthogonal to each other when $f \cdot g=0$. 

\ex{Orthogonal functions}{
	Let $f(x)=1$ and $g(x)=x$ and suppose that $f, g$ are functions from $[-1, 1]$ to $\bbR$. Then, $$f\cdot g = \int_{-1}^1 x\ dx = 0.$$
}

\ex{Orthogonal basis for quadratic functions on $[-1, 1]$}{
	One can verify that the functions $f(x)=1, g(x)=x, h(x)=1-3x^2$ form an orthogonal basis for the space of quadratic functions on $[-1, 1]$. 
	
	This is because $\int_{-1}^1 f(x)g(x)\ dx = 0, \int_{-1}^1 f(x)h(x)\ dx = 0$, and $\int_{-1}^1 g(x)h(x)\ dx = 0$.
}

\coro{}{ Because the integrals of the products of any two basis functions above is zero, it follows that one can take any scalar multiple of $f(x), g(x)$, and $h(x)$ and also obtain the same orthogonal basis. In short, these basis functions are determined up-to-a-scale. }

We viewed function spaces where the underlying function is a map $f: [a, b] \to \bbR$. What about when $[a, b] = [-\infty, \infty]$, meaning that the map is $f: \bbR\to\bbR?$

The underlying theory is pretty similar. We do have the extra condition that the dot product of a function with itself must converge, i.e. the value $$f \cdot f = \int_{\infty}^\infty (f(x))^2\ dx$$ converges. Evidently we also require the condition that $f(x)$ is continuous and admits a finite value for all values of $x$. Consider the function $f(x) = \frac{1}{\sqrt{x}}$ for example. Although $\int_{\infty}^\infty (f(x))^2= \int_{\infty}^\infty \frac{1}{x}=0$, $f(x)$ is undefined at $x=0$.

\subsection{Viewing the derivative as a matrix}

Differentiation is a linear operation. To motivate this, we can check that the function of differentiation satisfies the conditions for a vector space.

Let $f(x)$ and $g(x)$ be two functions differentiable on $\bbR$. Then, $$\frac{d}{dx}(f(x)+g(x)) = \frac{d}{dx} f(x)+\frac{d}{dx}g(x)$$ and $$\frac{d}{dx} kf(x)= k\frac{d}{dx}f(x)$$ where $k\in\bbR$. This suggests that the derivative operator is a vector space. 

The derivative, or differentiation operator $D$ is a mapping that acts on functions of a space (e.g. $\bbR$) to produce elements of another space which resembles differentiation. It clearly acts on spaces where functions on said space are continuous, or smooth. Thus, the differentiation operator is a \href{https://www.youtube.com/watch?v=4TYv2PhG89A}{smooth operator.}

Therefore, the set of functions $$V = \{f: \bbR \to \bbR \mid f(x) \mapsto f'(x) \ \text{and}\ f\ \text{differentiable}\}$$ is a vector space. We can also write $V$ as $$V=\{D:\bbR \to \bbR \mid f(x) \mapsto f'(x)\}.$$

\ex{Quadratic polynomials}{Let $\bbR_2[X]$ denote the vector space of all quadratic polynomials with coefficients in $\bbR$. Every polynomial of degree 2 can be represented as $f(x)=a+bx+cx^2$. It is determined by its coefficients $a, b$, and $c$; these coefficients form the vector $\begin{pmatrix} a\\ b \\ c \end{pmatrix}$ wherein each element of $\bbR_2[X]$ is determined by the values within the vector. This coefficient relationship works in a similar way to how $\bbR^3$ works.

Consider the canonical basis of $\bbR^3$, given by $\hat{e_1}=\begin{pmatrix} 1\\0\\0 \end{pmatrix}, \hat{e_2}=\begin{pmatrix} 0\\1\\0 \end{pmatrix}, \hat{e_3}=\begin{pmatrix} 0\\0\\1 \end{pmatrix}.$ 

In $\bbR_2[X]$, the corresponding basis elements are $\hat{e_1}=\begin{pmatrix} 1\\0\\0 \end{pmatrix}=1, \hat{e_2}=\begin{pmatrix} 0\\1\\0 \end{pmatrix}=x$, and $\hat{e_3}=\begin{pmatrix} 0\\0\\1 \end{pmatrix}=x^2$.

Just like how every element of $\bbR^3$ is determined by a linear combination of the three basis vectors, every element of $\bbR_2[X]$ is determined by a linear combination of $1, x$, and $x^2$. 

Since differentiation maps 1 to 0, $x$ to 1, and $x^2$ to $2x$, the basis elements, under the differentiation operator, represent the following mappings: $$D:\begin{pmatrix} 1\\0\\0 \end{pmatrix} \mapsto \begin{pmatrix} 0\\0\\0 \end{pmatrix}, \begin{pmatrix} 0\\1\\0 \end{pmatrix} \mapsto \begin{pmatrix} 1\\0\\0 \end{pmatrix}, \begin{pmatrix} 0\\0\\1 \end{pmatrix} \mapsto \begin{pmatrix} 0\\2\\0 \end{pmatrix}.$$

Therefore, the differential map $D$ is represented by the matrix $$D=\begin{pmatrix} 0&1&0\\0&0&2\\0&0&0 \end{pmatrix}$$ which underscores the map $$\begin{pmatrix} 0&1&0\\0&0&2\\0&0&0 \end{pmatrix} \begin{pmatrix} a\\b\\c \end{pmatrix} = \begin{pmatrix} b\\2c\\0 \end{pmatrix}.$$ Evidently, if you have a polynomial $a+bx+cx^2$, its derivative is $b+2cx$. 

The map representing the second derivative, written $D^2$, is represented by the matrix $$D=\begin{pmatrix} 0&0&2\\0&0&0\\0&0&0 \end{pmatrix},$$ underscoring the map $$\begin{pmatrix} 0&0&2\\0&0&0\\0&0&0 \end{pmatrix} \begin{pmatrix} a\\b\\c \end{pmatrix} = \begin{pmatrix} 2c\\0\\0 \end{pmatrix}.$$

(n.b.) Pure maths detour. We call the mapping between $\bbR_2[X]$ and $\bbR^3$ an \emph{isomorphism}, given by $(a, b, c) \mapsto a+bx+cx^2$.
}

One can check that the derivative map for polynomials of degree 3 is given by the matrix $$D=\begin{pmatrix} 0&1&0&0\\0&0&2&0\\0&0&0&3\\0&0&0&0 \end{pmatrix},$$ and the map for polynomials of degree $n$ is the $(n+1)$-by-$(n+1)$ matrix $D$ satisfying $D_{i+1, i}=i$ for $i=1, \dots, n$. 

\defn{Time series}{
	A time series is an infinitely long vector $$\vec{X} = \vec{x_t} = \{x_{-\infty}, \dots, x_{-1}, x_0, x_1, \dots, x_\infty\}$$ that outlines various moments in time $t$. 
	
	When we see difference equations that mention relationships between $x_t$, $x_{t+1}$ et cetera, we are usually working with time series. But $x_t$ can mean a lot of other things in economic contexts.
}

\ex{Matrix acting on the difference equation operator}{Consider the difference equation $$x_{t+1}-x_t = \alpha x_t + \varepsilon.$$

If $\vec{X} = \{x_t\}$ is a set of values satisfying this difference equation relation, then we can form a matrix relation analogous to what we did under the differential map. Note that here, we take an infinite range of values for $t$, meaning that $t$ ranges from $-\infty$ to $\infty$. 

Look at $x_{t+1}-x_t$ on the left-hand side and imagine a vector with infinite components containing all the $x_{t+1}-x_t$ - i.e. $\vec{T}=\begin{pmatrix} \vdots \\ x_t-x_{t-1} \\ x_{t+1}-x_t \\ \vdots \end{pmatrix}$. This vector satisfies $$\Delta \cdot \vec{X} = \vec{T},$$ where $$\Delta=\begin{pmatrix} \hdots & \vdots & \ddots & \ddots & \ddots \\ \ddots & -1 & 1 & \ddots & \ddots \\ \ddots & \ddots & -1 & 1 & \ddots \\ \ddots & \ddots & \ddots & -1 & \vdots \\ \ddots&\ddots&\ddots&\ddots&\hdots \end{pmatrix}$$ where all the $\ddots$ admit zeroes, the $\hdots$ all admit -1's, and the $\vdots$ all admit 1's.

\coro{}{ The \emph{difference operator} is represented by $D$. However, the difference operator admits the \emph{differential map} $\Delta$, which is a matrix. }

Why this? Let's consider a simpler case $$\Delta_{snap}=\begin{pmatrix} -1&1&0 \\ 0 & -1 & 1 \\ 0 & 0 & -1 \end{pmatrix}$$ where we merely took a snapshot out of the middle of the matrix. We can multiply $$\begin{pmatrix} -1&1&0 \\ 0 & -1 & 1 \\ 0 & 0 & -1 \end{pmatrix} \begin{pmatrix} x_{t-1} \\ x_t \\ x_{t+1} \end{pmatrix} = \begin{pmatrix} x_t-x_{t-1} \\ x_{t+1}-x_t \\ \dots-x_{t+1} \end{pmatrix}$$ where the final row of the vector $\dots-x_{t+1}$ is actually $x_{t+2}-x_{t+1}$; we don't have this as we are merely taking a snapshot out of $D$.

In Week 14, we will talk more about the mechanics behind taking a snapshot, what the missing $x_{t+2}$ entails, and how we can fix it using \emph{circular time}.

Therefore, the difference equation satisfies $$D\cdot \vec{x} = \alpha \bbI \cdot \vec{x} + \vec{\varepsilon},$$ where $\vec{\varepsilon}$ now takes the form of a vector representing the error term in an econometric model.

\coro{}{ We will study properties of this matrix in Week 14, including applications to systems of difference and differential equations, and time series. We will also take snapshots out of infinite matrices for illustrative purposes.}
}

If we are working with functions defined on a domain $[a, b]$ that satisfies the condition $f(a)=f(b)$, we have extra, more special conditions that are placed on the linear map. In short, there is an intriguing relationship between the differential map and its transpose when this condition is satisfied. 

Since our polynomial example above does not have this condition, the differential map matrix does not satisfy it. 

Let's detour back to ordinary linear algebra. Recall that for a vector $\vec{x}$, and a matrix $A$, we have that $\vec{x} \cdot A = A^T \cdot \vec{x}$, where $A^T$ is the transpose of $A$. Now let $\vec{y}$ be another vector. We have that $$\vec{x} \cdot (A \cdot \vec{y}) = (A^T \cdot \vec{x}) \cdot \vec{y}.$$ In our function space notation, this is analogous to \begin{equation}f \cdot (D \cdot g) = (D^T \cdot f) \cdot g.\end{equation} The left hand side is the dot product of a function $f$ and the differentiation operator applied on a function $g$.

Let $f: [a,b]\to \bbR$ be continuous and differentiable on $\bbR$. The dot product of $f(x)$ and $g'(x)$ is the integral of $f(x)\cdot \frac{d}{dx} g(x)$. We use integration by parts, where we integrate $\frac{d}{dx} g(x)$ and differentiate $f(x)$. By such, we obtain \begin{align*} \int_a^b f(x) \cdot \left(\frac{d}{dx} g(x)\right) \ dx  &= \int_a^b \frac{d}{dx} \left(f(x)g(x)\right) - \left(\frac{d}{dx} f(x)\right) g(x)\ dx \\ &= \int_a^b \frac{d}{dx} (f(x)\cdot g(x)) \ dx - \int_a^b \left(\frac{d}{dx} f(x)\right) g(x)\ dx \\ &= f(b)g(b)-f(a)g(a) - \int_a^b \left(\frac{d}{dx} f(x)\right) g(x)\ dx.\end{align*}

If our set of functions satisfy $f(a)=f(b)$ for all $f$, then $f(b)-f(a)=0$. This means that $f(b)g(b)-f(a)g(a)=0$, and thus \begin{equation}\int_a^b f(x) \cdot \frac{d}{dx} g(x)\ dx = \int_a^b \left(-\frac{d}{dx} f(x)\right) g(x)\ dx. \end{equation}

Compare equation (13.2) with (13.1). The left-hand side of equation 13.2 is $f \cdot (D\cdot g)$, and the right-hand side of equation 13.2 is $(-D\cdot f)\cdot g$. A direct comparison with equation 13.1 yields the result that $$D^T = -D.$$ Matrices that satisfy the property $D^T=-D$ such as the differentiation operator $D$ on function spaces that satisfy $f(a)=f(b)$ for all functions within the space are called \emph{skew-symmetric matrices}. 

In fact, we don't necessarily require $f(a)=f(b)$. The sufficient condition is that $f(b)g(b) - f(a)g(a)=0$ for any two elements $f, g \in V$, where $V$ is our function space.

The theory regarding the eigenvalues and eigenvectors of skew-symmetric matrices like $D$ is robust. Up to now, however, we have restricted our study of eigenvalues and eigenvectors to symmetric matrices. We will explore skew-symmetric matrices, as well as more results on the eigenvalues and eigenvectors of non-symmetric matrices in Week 16. 

\section{More mathematical tidbits}

This section explores further the theory of Legendre polynomials, the differentiation operator (as explored earlier on in this chapter), and the Dirac delta function. 

\subsection{Legendre polynomials}
Legendre polynomials are a set of polynomials orthogonal to one another. 

\defn{Legendre polynomials}{
	The Legendre polynomials are a set of polynomials $P_n(x)$ that satisfy the condition $$\int_{-1}^1 P_m(x) P_n(x) =0$$ for $n\neq m$. 
	
	These polynomials form the orthogonal basis for polynomials on $[-1, 1]$. In Example 13.4, we saw the restricted case of an orthogonal basis for quadratic functions on $[-1, 1]$.
}

We start constructing this set of polynomials by first considering that $P_0(x)=1$. Since it is given that $\int_{-1}^1 P_m(x) P_n(x) =0$, it remains that $P_1(x)=x$. Then we can further construct $P_2(x)$ to be any scalar multiple of $1-3x^2$.

The Legendre polynomials are determined by \emph{Rodrigues' formula:} $$P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n} (x^2-1)^n$$ which yields the polynomials \begin{align*} P_0(x)&=1 \\ P_1(x) &= x \\ P_2(x) &= \frac{1}{2}(3x^2-1) \\ P_3(x) &= \frac{1}{2}(5x^3-3x)\end{align*} which matches, up to scale, our results in Example 13.4.

\subsection{Eigenfunctions}

Can the linear algebra done in this chapter on function spaces be extended to a study of eigenvalues and eigenvectors? Yes, using the theory of \tbf{eigenfunctions}. 

\defn{Eigenfunction}{An eigenfunction is a function $f$ acting on a linear operator $D$ such as the differentiation operator. It is some function $f$ in that function space, where, when acted upon by the operator $D$, is multiplied by the eigenvalue. 

Explicitly, we have that $$Df = \lambda f.$$

Note the similarities between the eigenvector $\vec{v}$ satisfying $M\vec{v} =\lambda \vec{v}$ and the eigenfunction $f$ that satisfies $Df = \lambda f.$ Also note that $Df$, well, just performs differentiation! More importantly, a differential equation such as $$\frac{df}{dt} = \lambda f$$ is just an eigenvalue equation.
}

\ex{Eigenfunctions of the derivative operator $D$}{
	Consider the derivative operator $D$ that admits the eigenvalue equation $$\frac{d}{dt} f = \lambda f.$$ Simple separation of variables yields $\frac{1}{f}\ df = \lambda\ dt$, or $$f(t) = f_0 e^{\lambda t}$$ for some initial-value constant $f_0$. For example, if $f_0=1$, then $f(t)=e^{\lambda t}.$
}

\ex{Eigenfunctions of the second derivative operator $D^2$}{
	The eigenvalue equation for the second derivative operator $D^2$ corresponds to $$f''=\lambda f.$$ Assume the initial value conditions $f(0) = f(L)=0.$
	
	We first show that the eigenvalues $\lambda$ are all negative. To show this, note that $f''-\lambda f=0$ implies $f(f''-\lambda f)=0$, so \begin{align*} \int_0^L f(f''-\lambda f)&=0 \\ \int_0^L f\cdot f'' \ dx&= \lambda \int_0^L f^2 \ dx \end{align*}
	
	Integration by parts on $\int_0^L f\cdot f'' \ dx$ yields $$\int_0^L f \cdot f'' \ dx = f\cdot f' \big|^L_0 - \int_0^L (f')^2\ dx$$ which equals $f(L)f'(L)-f(0)f'(0)-\int_0^L (f')^2\ dx$. However note that if $f(0)=f(L)=0$, then the former term equals zero, so $$-\int_0^L (f')^2\ dx = \lambda \int_0^L f^2\ dx.$$
	
	Note that both integrals are strictly positive. Therefore, $\lambda$ is negative.
	
	\tbf{Case 1:} First, let $\lambda=0$. This is trivial as the right-hand side of the original equation vanishes, yielding us $f'' = 0$, or $f=c_1+c_2x$ for $c_1, c_2$ constants.
	
	\tbf{Case 2:} Now, let $\lambda <0$. There are methods of solving second-order differential equations that are not covered in Frank's lecture notes. We skip the mathematical derivation and obtain that the general solution for a second-order differential equation of that form is $$f(x) = c_1 \cos(\sqrt{-\lambda} x) + c_2 \sin(\sqrt{-\lambda} x).$$
	
	Now we invoke the boundary conditions. Note that $f(0)=0$ implies that $c_1=0$, and $f(L)=0$ implies that $c_2\sin(\sqrt{-\lambda} L)=0$, or $\sin(\sqrt{-\lambda}L)=0$. This suggests that $\sqrt{-\lambda}=\frac{n\pi}{L}$, or $$\lambda_n = -\frac{n^2\pi^2}{L^2}$$ are the eigenvalues for this eigenvalue equation. Additionally, the eigenfunctions satisfy $$f_n(x) = \sin \frac{n\pi x}{L}.$$
	
	Intuitively, this makes sense. The second derivative of any sine function yields a scalar multiple of a sine function, which is what the second derivative operator does. 
	
	Are there initial value conditions that yield cosine eigenfunctions instead of sine eigenfunctions? Yes, they are the initial value conditions $f'(0)=f'(L)=0$. Observe that $$f'(x) = -c_1\sqrt{-\lambda} \sin (\sqrt{-\lambda}x) + c_2 \sqrt{-\lambda} \cos(\sqrt{-\lambda} x).$$ This time, we require $c_2=0$ and $-c_1 \sqrt{-\lambda}\sin(\sqrt{-\lambda} L) =0$, or $\sin(\sqrt{-\lambda} L) =0$. This yields a very similar set of eigenvalues and eigenfunctions - namely the eigenvalues $$\lambda_n = -\frac{n^2\pi^2}{L^2}$$ and the eigenfunctions $$f_n(x) = \cos \frac{n\pi x}{L}.$$
	
	\coro{}{ The boundary conditions determine the values of the eigenfunctions associated with this eigenvalue equation. }
}

\subsection{Dirac delta function}

The Dirac delta function is a function that only makes sense when evaluated inside an integral. It is not a function in the usual sense, and is commonly referred to as a \emph{distribution} instead of a function. 

\defn{Dirac delta function}{
	The Dirac delta function $\delta(x)$ is defined as $$\delta(x) = \begin{cases} 0, & x\neq 0 \\ \infty, & x=0 \end{cases}$$ such that $$\int_{-\infty}^\infty \delta(x)=1.$$ 
}

Interestingly enough, there is no well-defined function that has this property. We give a few ways to characterise this function. One way to recover this function is to consider a limit. 

\defn{Dirac delta function through the normal distribution}{
	Consider the normal distribution with mean $\mu=0$ and standard deviation $\sigma$. Recall that the probability density function for the normal distribution is given by $$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$
	
	If we attempt to take the limit as $\sigma \to 0$, we get $$\lim\limits_{x\to 0} f(x) = \lim\limits_{x\to 0} \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}= \delta(x).$$
}

Visually, one can view this as taking a Gaussian distribution and squishing the middle of the distribution; this is given by observing the limit as the standard deviation $\sigma$ approaches zero. Note that this is equivalent to defining the \emph{Heaviside step function} $$H(x) = \begin{cases} 0, & x< 0 \\ 1, & x \geq0 \end{cases}$$ and considering the function $$f(x) = nH(x+\frac{1}{2n})\cdot H(\frac{1}{2n}-x).$$ Taking the limit $$\lim\limits_{n\to\infty} f(x)$$ yields the Dirac delta function $\delta(x)$. 

(n.b.) Some technical notes. The "derivative" of the Heaviside step function is the Dirac delta function. Though note that this "derivative" is not the derivative in the usual sense; due to the Dirac delta function being a \emph{distribution}, we have to consider a \emph{weak derivative} instead. This is $$\delta(x) = \frac{d}{dx} H(x).$$

I'd like to finish off by discussing some properties of the Dirac delta function.

\begin{enumerate}
	\item The Dirac delta function emulates the role of evaluating a function at a value $a$. More precisely, we have that $$\int_{-\infty}^\infty \delta(x-a) f(x) = f(a).$$ More importantly, if we take $a=0$, we have that $\int_{-\infty}^\infty \delta(x) f(x) = f(0).$
	\item The Dirac delta function satisfies the identities $$\delta(ax) = \frac{1}{|a|} \delta(x)$$ for all $a\in \bbR$. Additionally, we have that $\delta(-x)=\delta(x)$.
\end{enumerate}

\ex{}{
	Consider the integral $$\int_{-\infty}^\infty (x^2+1) \delta(3x+1)\ dx.$$ Note \begin{align*} \int_{-\infty}^\infty (x^2+1) \delta(3x+1)\ dx &= \frac{1}{3} \int_{-\infty}^\infty (x^2+1) \delta(x+\frac{1}{3})\ dx \\ &= \frac{1}{3} \cdot ((-\frac{1}{3})^2+1) \\ &= \frac{10}{27}. \end{align*}
}

\coro{Division by zero}{
	Consider the equation $kf(x)=0$. A solution to this is $$f(x) = c\delta(k).$$
}

\chapter{Week 14}

In this week, we apply our theory of function spaces in a revised study of the theory of difference and differential equations. In particular, we will appreciate the art of translating a problem in difference equations to a problem in linear algebra. 

\section{Difference equations with time-dependent coefficients}

In Week 7, we worked with difference equations with constant coefficients. These included standalone difference equations of the form $$x_{t+1}-x_t = ax_t+b,$$ or systems of difference equations of the form $$\vec{x}_{t+1}-\vec{x}_t = A\vec{x}_t+\vec{b}.$$ 

In Week 13 we explored the link between time series and difference equations. This drives us to explore difference equations, and systems of difference equations which constants are affected by the time series. 

\subsection{Standalone time-dependent difference equations}

\defn{Time-dependent coefficients}{
	Difference equations with time-dependent coefficients and constant coefficients are similar. In particular, ones with time-dependent coefficients satisfy $$x_{t+1}-x_t = a_t \cdot x_t + b_t.$$
	
	Meanwhile, a system of difference equations with time-dependent coefficients satisfy $$\vec{x}_{t+1}-\vec{x}_t = A_t \cdot \vec{x}_t + \vec{b}_t.$$
}

Let's perform an analysis of time-dependent coefficients similar to what we encountered in Week 7. Letting $x_{t+1} = (a_t+1) \cdot x_t+b_t$, we have that $$x_1 = (a_0+1)x_0+b_0$$ and \begin{align*}x_2 &= (a_1+1)x_1+b_1 \\ &= (a_1+1)((a_0+1)x_0+b_0)+b_1 \\ &= (a_1+1)(a_0+1)x_0+(a_1+1)b_0 + b_1. \end{align*}

In general, $x_t$ satisfies $$x_t = \prod\limits_{j=0}^t (a_j+1)x_0 + \left(\sum\limits_{j=0}^{t-1} \left(\prod\limits_{k=1}^j (a_{t-k}+1)\right) b_{t-1-j}\right).$$

Now we analyse stationary, asymptotic, and periodic states.

Can we have stationary states, meaning $a_tx_t+b_t=0$? This would be difficult as the constants are time-dependent, and would have to be meticulously engineered in order to be independent of time.  

Can we have asymptotic states? Yes, but this typically requires both $a_t$ and $b_t$ to converge. 

Can we have periodic states? Note that a typical example of a periodic state we gave was $\{-1, 1, -1, 1, \dots\}$. If $a_t$ and $b_t$ are defined to be periodic, then we could again engineer a difference equation which gave periodic states.

\subsection{Systems of time-dependent difference equations}

Now let's explore systems of time-dependent difference equations. These satisfy $$\vec{x}_{t+1}-\vec{x}_t = A_t \cdot \vec{x}_t + \vec{b}_t.$$

Let's find a general solution before analysing the various states. We first consider homogeneous equations, where $\vec{b}_t=\vec{0}$ as there is a distinction to be made between systems of equations and standalone equations. Here, observe that we have $\vec{x}_{t+1} = (\bbI+A_t) \cdot \vec{x}_t$. For small values of $t$, we have that $$\vec{x}_1 = (A_0+\bbI) \cdot \vec{x}_0$$ and \begin{align*}\vec{x}_2 &= (A_1+\bbI) \cdot \vec{x}_1 \\ &= (A_1+\bbI)((A_0+\bbI)\cdot \vec{x}_0)  \end{align*} and so on, creating the general solution \begin{align*} \vec{x}_t &= (A_{t-1}+\bbI)\cdot \hdots \cdot (A_1+\bbI)(A_0+\bbI) \cdot \vec{x}_0 \\ &= \prod\limits_{j=0}^{t-1} (A_j+\bbI) \cdot \vec{x}_0. \end{align*} 

We must take caution when evaluating the product of these $t$ matrices. Since matrices do not commute in general, we must further specify that matrix multiplication is done in reverse index order - i.e. from $j=t-1$ down to $j=0$. Frank specifies this by putting a "time-ordering operator" $\bbT$, writing $$\vec{x}_t = \bbT \prod\limits_{j=0}^{t-1} (A_j+\bbI) \cdot \vec{x}_0$$ to specify the multiplication of matrices in descending index order.

\coro{}{ In mathematics, a product $\prod\limits_{j=0}^t x_j$ is generally specified to denote $x_0x_1\dots x_t$. However, the specific order is specified in most non-commutative cases. Taking this convention, it might be more appropriate for us (and hereinafter we use this notation) to write $$\vec{x}_t = \prod\limits_{j=0}^{t-1} (A_{t-1-j}+\bbI)\cdot \vec{x}_0.$$}

Now we move on to inhomogeneous systems of difference equations. Consider the system $$\vec{x}_{t+1} = (A_t+\bbI)\cdot \vec{x}_t + \vec{b}_t.$$ Using the method above, we have, for small values of $t$, that $$\vec{x}_1 = (A_0+\bbI)\cdot \vec{x}_0 + \vec{b}_0$$ and \begin{align*} \vec{x}_2 &= (A_1+\bbI) \cdot \vec{x}_1+ \vec{b}_1 \\ &= (A_1+\bbI) \cdot ((A_0+\bbI)\cdot \vec{x}_0 + \vec{b}_0)+\vec{b}_1 \\ &= (A_1+\bbI)(A_0+\bbI) \cdot \vec{x}_0 + (A_1+\bbI) \cdot \vec{b}_0 + \vec{b}_1.\end{align*}

Let's consider $\vec{x}_3$ for further illustration. Note \begin{align*}\vec{x}_3 &= (A_2+\bbI) \cdot \vec{x}_2 +\vec{b}_2 \\ &= (A_2+\bbI) \cdot ((A_1+\bbI)(A_0+\bbI) \cdot \vec{x}_0 + (A_1+\bbI) \cdot \vec{b}_0 + \vec{b}_1) + \vec{b}_2 \\ &= (A_2+\bbI)(A_1+\bbI)(A_0+\bbI)\vec{x}_0+(A_2+\bbI)(A_1+\bbI) \vec{b}_0+(A_2+\bbI)\vec{b}_1+\vec{b}_2  \end{align*} and in general, $$\vec{x}_t = \prod\limits_{j=0}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{x}_0 + \sum\limits_{k=1}^t \prod\limits_{j=k}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{b}_{k-1}.$$

We can split the solution for $\vec{x}_t$ into the \emph{homogeneous solution} $\vec{x}_t^h = \prod\limits_{j=0}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{x}_0$ and the \emph{particular solution} $\vec{x}_t^p=\sum\limits_{k=1}^t \prod\limits_{j=k}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{b}_{k-1}$. We will further explore the meaning of these two sets of solutions later after exploring how function spaces tie into this theory.

\section{Spaces of solutions to first-order differential equations}

Recall from Week 7 that if we want to find the minima or maxima of a multivariable function $f(\vec{x})$, we can use the method of gradient descent and ascent. Constructing the system of difference equations $$\vec{x}_{t+1} - \vec{x}_t = \alpha_t \nabla f(\vec{x}_t)$$ yields us a set of guesses $\vec{x_1}, \vec{x_2}, \vec{x_3}$ et cetera that allows the sequence $\{\vec{x}_n\}$ to approach the local optimum in the limit as $t\to\infty$.

Simplifying $\vec{V}(\vec{x}_t) = \alpha_t \nabla f(\vec{x}_t)$, we have the system of difference equations \begin{equation} \vec{x}_{t+1}-\vec{x}_t = \vec{V}(\vec{x}_t).\end{equation}

In the theory of function spaces, our $\vec{x}_t$ are not individual values of a sequence but a function $x(t)$ that takes on individual values of $t$ - so we have that $$ \vec{x}(t+1)-\vec{x}(t) = \vec{V}(\vec{x}(t)).$$

Now an important question arises: do the $\vec{x}(t)$ form a function space? We check with our vector space axioms. Recall that if $\vec{x}(t)$ and $\vec{y}(t)$ are functions that satisfy the system, then we have that $\vec{x}(t)+\vec{y}(t)$ satisfies the system, as well as any scalar multiple $k\vec{x}(t)$ and $k\vec{y}(t)$, where $y\in\bbR$. Formally, we have that $$k\vec{x}(t+1) - k\vec{x}(t) = \vec{V}(k\vec{x}(t))$$ which implies that $$\vec{V}(k\vec{x}(t)) = k\vec{V}(\vec{x}(t)).$$ Additionally, we also have that $$\vec{x}(t+1)+\vec{y}(t+1)-\vec{x}(t)-\vec{y}(t) = \vec{V}(\vec{x}(t)+\vec{y}(t)).$$

\thrm{}{
	The functions $\vec{V}(\vec{x}(t))$ that make the set of solutions a vector space are linear forms. More specifically, the functions $\vec{V}$ contain terms that are linear in the respective $\vec{x}(t)$. 
}

\ex{}{
	Consider a simple case. Suppose we have that $$\vec{x}(t+1)-\vec{x}(t) = A(t) \cdot \vec{x}(t)$$ for some constant $A(t)$ that depends on time $t$. (I know this sounds pretty oxymoronic but just bear with the fact that each component of $\vec{x}$ is multiplied by a constant)
	
	Checking our axioms, we have that \begin{align*} \vec{x}(t+1)+\vec{y}(t+1)-\vec{x}(t)-\vec{y}(t) &= \vec{x}(t+1)-\vec{x}(t) + \vec{y}(t+1)-\vec{y}(t) \\ &= A(t) \cdot \vec{x}(t) + A(t) \cdot \vec{y}(t) \\ &= A(t) \cdot (\vec{x}(t)+\vec{y}(t)) \end{align*} and \begin{align*} k\vec{x}(t+1) -k\vec{x}(t) &= k(\vec{x}(t+1)-\vec{x}(t)) \\&= kA(t) \cdot \vec{x}(t) \\&= A(t) \cdot (k\vec{x}(t)). \end{align*} 
	
	\tbf{Intuition:} A notable challenge we had to overcome was the fact that we required $\vec{V}(k\vec{x}(t)) = k\vec{V}(\vec{x}(t)).$ It turns out that linear forms help us overcome this challenge.
}

How do we represent the difference equation $$\vec{x}(t+1) - \vec{x}(t) = A(t) \cdot \vec{x}(t)$$ as a matrix system? Denote $\vec{X}=\{\vec{x}(t)\}$ as the infinite time series $$\vec{X}=\vec{x}(-\infty), \dots, \vec{x}(-1), \vec{x}(0), \vec{x}(1), \dots, \vec{x}(\infty).$$

Recall that in Week 13, we wrote that $$\Delta\cdot \vec{X}=\begin{pmatrix} \hdots & \vdots & \ddots & \ddots & \ddots \\ \ddots & -1 & 1 & \ddots & \ddots \\ \ddots & \ddots & -1 & 1 & \ddots \\ \ddots & \ddots & \ddots & -1 & \vdots \\ \ddots&\ddots&\ddots&\ddots&\hdots \end{pmatrix} \cdot \vec{x} = \begin{pmatrix} \vdots \\ \vdots \\ x(t+1)-x(t) \\ \vdots \\ \vdots \end{pmatrix}.$$

The analogue for systems of differential equations is $$\Delta\cdot \vec{X}=\begin{pmatrix} \hdots & \vdots & \ddots & \ddots & \ddots \\ \ddots & -\bbI & \bbI & \ddots & \ddots \\ \ddots & \ddots & -\bbI & \bbI & \ddots \\ \ddots & \ddots & \ddots & -\bbI & \vdots \\ \ddots&\ddots&\ddots&\ddots&\hdots \end{pmatrix} \cdot \vec{X} = \begin{pmatrix} \vdots \\ \vdots \\ \vec{x}(t+1)-\vec{x}(t) \\ \vdots \\ \vdots \end{pmatrix}$$ where the $\hdots$ represent $-\bbI$, the $\vdots$ represent $\bbI$, and the $\ddots$ are all zeroes. 

\coro{}{Perhaps we were rather lazy with our notation here, for we used $\vec{X}$ to refer to two distinct scenarios. Note that in the $\vec{X}$ referring to one single difference equation, we have that $\vec{X}=\{\dots, x(-1), x(0), x(1), \dots\}$; in the $\vec{X}$ referring to a system of difference equations, we have that $\vec{X}=\{\dots, \vec{x}(-1), \vec{x}(0), \vec{x}(1), \dots\}$.}

The system of difference equations then becomes $$\begin{pmatrix} \hdots & \vdots & \ddots & \ddots & \ddots \\ \ddots & -\bbI & \bbI & \ddots & \ddots \\ \ddots & \ddots & -\bbI & \bbI & \ddots \\ \ddots & \ddots & \ddots & -\bbI & \vdots \\ \ddots&\ddots&\ddots&\ddots&\hdots \end{pmatrix} \cdot \vec{X} = \begin{pmatrix} \ddots & \ddots & \ddots & \ddots & \ddots \\ \ddots & A(t-1) & \ddots & \ddots & \ddots \\ \ddots & \ddots & A(t) & \ddots & \ddots \\ \ddots & \ddots & \ddots & A(t+1) & \ddots \\ \ddots&\ddots&\ddots&\ddots&\ddots \end{pmatrix} \cdot \vec{X},$$ or $$\Delta \cdot \vec{X} = A\cdot \vec{X}$$ in short, where the $\ddots$ are all zeroes.

\coro{}{\tbf{Key idea:} The differential equation $\vec{x}(t+1)-\vec{x}(t) = A(t) \cdot \vec{x}(t)$ is actually a matrix equation $$\Delta \cdot \vec{X} = A \cdot \vec{X}$$ which turns out to be an eigenvalue equation when we write \begin{align*}(\Delta-A) \cdot \vec{X} &=0 \\ &= 0 \cdot \vec{X} .\end{align*} \tbf{This suggests that our solution $\vec{X}$ is an eigenvector of $\Delta-A$ with eigenvalue 0.}}

\section{Inhomogeneous difference equations}

Can we extend the theory of infinite matrices and vectors to second-order difference equations? It turns out that we can, with slight alterations in our theory. 

Consider the inhomogeneous equation $\vec{x}(t+1)-\vec{x}(t) = A(t) \cdot \vec{x}(t)+\vec{f}(t)$. In this case, we use a method analogous to equation 14.2 to obtain $$(\Delta-A) \cdot \vec{X} = \vec{F}$$ where $\vec{F} = \{ f(-\infty), \dots, f(-1), f(0), f(1), \dots, f(\infty) \}$ is a time series.

This produces \begin{equation}\vec{X} = (\Delta-A)^{-1} \cdot \vec{F}. \end{equation}

\defn{Homogeneous and particular solutions}{
	The solution to a difference or a differential equation is the sum of its homogeneous solution and its particular solution. 
	
	The homogeneous solution of an inhomogeneous difference equation is where $\vec{f}(t)$ is set to zero. For example, the difference equation $$t^2\vec{x}_{t+1} - \vec{x}_{t+1} - 2\vec{x}_t = \sqrt{t}$$ has a homogeneous equation that satisfies $t^2\vec{x}_{t+2} - \vec{x}_{t+1}- 2\vec{x}_t=0$. 
	
	The particular solution, meanwhile, is any solution of the differential equation where $\vec{f}(t)$ is not set to zero, or remains being $\vec{f}(t)$.
	
	Let $\vec{x}_h$ be the homogeneous solution and $\vec{x}_p$ be the particular solution. Any solution to the equation is given by $$\vec{x}_{sol} = \vec{c} \cdot \vec{x}_h + \vec{x}_p,$$ where $\vec{c}$ is a vector of constants. Why the vector of constants and why addition? The appendix to this chapter provides some further insights.
}

\ex{Homogeneous and particular solutions for difference equations}{
	To further illustrate homogeneous and particular solutions, let's return to the theory of difference equations explored in the first section of this chapter. Observe that the homogeneous solution to a difference equation consists of the simple case where $b_t=0$, or $\vec{b}_t=0$, and the particular solution specific to what $b_t$ or $\vec{b}_t$ is that outlines how the inhomogeneity arises. 
	
	In what follows, we make the shift towards function spaces by notating $b(t)$ and $\vec{b}(t)$ instead of $b_t$ and $\vec{b}_t$.
	
	Consider the differential equation $$x_{t+1}-x_t = a_t\cdot x_t+b_t.$$ The homogeneous solution $$x_t = \prod\limits_{j=0}^t (a_j+1)x_0$$ arises when $b_t=0$ for all $t$. When $b_t$ is added, the term $\left(\sum\limits_{j=0}^{t-1} \left(\prod\limits_{k=1}^j (a_{t-k}+1)\right) b_{t-1-j}\right)$ is added to form the general solution $$x_t = \prod\limits_{j=0}^t (a_j+1)x_0 + \left(\sum\limits_{j=0}^{t-1} \left(\prod\limits_{k=1}^j (a_{t-k}+1)\right) b_{t-1-j}\right).$$ The additional term is the particular solution. 
	
	Similarly, the general solution for a system of difference equations $$\vec{x}_t = \prod\limits_{j=0}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{x}_0 + \sum\limits_{k=1}^t \prod\limits_{j=k}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{b}_{k-1}$$ can be split into the homogeneous solution $\prod\limits_{j=0}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{x}_0$, corresponding to $\vec{b}_t=0$, and the particular solution $\sum\limits_{k=1}^t \prod\limits_{j=k}^{t-1} (A_{t-1-j}+\bbI) \cdot \vec{b}_{k-1}$.
}

Let's return to the expression $\vec{X} = (\Delta-A)^{-1} \cdot \vec{F}$. Evidently, if $\Delta-A$ has a zero eigenvalue, then $(\Delta-A)^{-1}$ is undefined as $\det(\Delta-A)=0$. Homogeneous solutions $\vec{x}_h$ are mapped onto $\vec{0}$ by the matrix $\Delta-A$, and particular solutions are mapped onto $\vec{F}$ by the matrix $A$ that is specific to the value of $\vec{f}(t)$.

The number of homogeneous solutions to the matrix system depends on the multiplicity of the zero eigenvalue. If there are $n$ zero eigenvalues, then there are $n$ distinct homogeneous solutions.

Just now, we observed that if $\Delta-A$ had eigenvalues with value zero (meaning that it allows homogeneous solutions), then the inverse matrix $(\Delta-A)^{-1}$ does not exist. We can solve this by removing the eigenvalue-eigenvector pair(s) that correspond to the zero eigenvalue. Simply put, note that the determinant of a matrix is the product of its eigenvalues, or $$\det(M) = \prod\limits_{j=1}^n \mu_j,$$ where $\mu_j$ are the eigenvalues of $M$. It remains that to get a non-zero determinant, we get rid of the zero eigenvalues. Though how do we "remove the eigenvalues of $M$", and how does this affect our matrix equation?

\defn{"Inverse" of a non-invertible matrix}{
	Recall that the spectral decomposition of a diagonalisable matrix $M$ is $$M= \sum\limits_{j=1}^n \mu_j (\hat{m}_j \otimes \hat{m_j}),$$ where $\mu_j$ and $\hat{m}_j$ are the eigenvalues and eigenvectors of $M$ respectively. For an invertible matrix, the inverse of said matrix is $$M^{-1}= \sum\limits_{j=1}^n \mu_j^{-1} (\hat{m}_j \otimes \hat{m_j}).$$ Then you realise that for non-invertible matrices, at least one of the eigenvalues is $\mu_j=0$. Thus, you cannot compute $\mu_j^{-1}$. To fix this, define $$\mu_j = \begin{cases} \mu_j & \mu_j \neq 0 \\ 0 & \mu_j =0 \end{cases}$$ and so $$M^{-1}= \sum\limits_{\mu_j \neq 0} \mu_j^{-1} (\hat{m}_j \otimes \hat{m_j})$$
}

\ex{Inverting the uninvertible}{
	Consider the symmetric matrix $M= \begin{pmatrix} 1&1&2 \\ 1&2&3\\ 2&3&5 \end{pmatrix}$. 
	
	Let's use this opportunity to recall a few facts we known about matrices and their spectral decomposition. These facts are: \begin{align*} \bbI &= \sum\limits_{j=1}^n  \hat{m}_j \otimes \hat{m}_j  \\ M &= \sum\limits_{j=1}^n  \mu_j (\hat{m}_j \otimes \hat{m}_j) \\ M^{-1} &= \sum\limits_{j=1}^n  \mu_j^{-1} (\hat{m}_j \otimes \hat{m}_j)\ \text{for}\ \mu_j \neq0 \end{align*}
	
	The eigenvalues of this matrix and their corresponding eigenvectors are $$\{ 0, \begin{pmatrix} -1 \\ -1 \\ 1 \end{pmatrix}\}, \{4+\sqrt{13}, \begin{pmatrix} 4-\sqrt{13} \\ -3+\sqrt{13} \\ 1 \end{pmatrix} \},\ \text{and}\ \{ 4-\sqrt{13}, \begin{pmatrix} 4+\sqrt{13}\\ -3-\sqrt{13}\\ 1 \end{pmatrix}\}.$$
	
	Normalising these eigenvectors yield $$\begin{pmatrix} -\frac{\sqrt{3}}{3} \\-\frac{\sqrt{3}}{3} \\ \frac{\sqrt{3}}{3} \end{pmatrix}, \frac{1}{6} \begin{pmatrix} \sqrt{\frac{78}{26-7\sqrt{13}}} - 2 \sqrt{\frac{6}{26-7\sqrt{13}}} \\ \sqrt{\frac{78}{26-7\sqrt{13}}} - 5 \sqrt{\frac{6}{26-7\sqrt{13}}} \\ 2\sqrt{\frac{78}{26-7\sqrt{13}}} - 7 \sqrt{\frac{6}{26-7\sqrt{13}}}\end{pmatrix},\ \text{and}\ \frac{1}{6}\begin{pmatrix} \sqrt{\frac{78}{26-7\sqrt{13}}} + 2 \sqrt{\frac{6}{26-7\sqrt{13}}} \\ \sqrt{\frac{78}{26-7\sqrt{13}}} + 5 \sqrt{\frac{6}{26-7\sqrt{13}}} \\ 2\sqrt{\frac{78}{26-7\sqrt{13}}} +7 \sqrt{\frac{6}{26-7\sqrt{13}}}\end{pmatrix}.$$
	
	Name the above eigenvectors $\hat{m}_1, \hat{m}_2$, and $\hat{m}_3$ respectively. We notice that $\hat{m}_1 \otimes \hat{m}_1 = \begin{pmatrix} \frac{1}{3} & \frac{1}{3} & -\frac{1}{3} \\ \frac{1}{3} & \frac{1}{3} & -\frac{1}{3} \\  -\frac{1}{3} & -\frac{1}{3} & \frac{1}{3} \end{pmatrix}$.
	
	Let $a =  \sqrt{\frac{78}{26-7\sqrt{13}}}$ and $b= \sqrt{\frac{6}{26-7\sqrt{13}}}$. We note $$\hat{m}_2 \otimes \hat{m}_2 = \frac{1}{36}\begin{pmatrix} (a-2b)^2 & (a-2b)(a-5b) & (a-2b)(2a-7b) \\ (a-5b)(a-2b) & (a-5b)^2 & (a-5b)(2a-7b) \\ (2a-7b)(a-2b) & (2a-7b)(a-5b) & (2a-7b)^2 \end{pmatrix}$$ and $$\hat{m}_3 \otimes \hat{m}_3 = \frac{1}{36}\begin{pmatrix} (a+2b)^2 & (a+2b)(a+5b) & (a+2b)(2a+7b) \\ (a+5b)(a+2b) & (a+5b)^2 & (a+5b)(2a+7b) \\ (2a+7b)(a+2b) & (2a+7b)(a+5b) & (2a+7b)^2 \end{pmatrix}.$$
	
	Here, the matrix $$M^{-1} = \frac{1}{4+\sqrt{13}} (\hat{m}_2 \otimes \hat{m}_2) + \frac{1}{4-\sqrt{13}}(\hat{m}_3 \otimes \hat{m}_3).$$
}
%\ex{Inverting the uninvertible}{
%	Consider $M= \begin{pmatrix} -5&12&  -7 \\ -13 & 30&-17\\-18&42&-24 \end{pmatrix}$. The eigenvalue-eigenvector pairs are $\{0, \begin{pmatrix} 1\\1\\1 \end{pmatrix} \}$, $\{-2, \begin{pmatrix} 1\\2\\3 \end{pmatrix} \}$, and $\{3, \begin{pmatrix} 1\\3\\4 \end{pmatrix} \}$.
%	
%	Let's use this opportunity to check a few facts we known about matrices and their spectral decomposition. These facts are: \begin{align*} \bbI &= \sum\limits_{j=1}^n  \hat{m}_j \otimes \hat{m}_j  \\ M &= \sum\limits_{j=1}^n  \mu_j (\hat{m}_j \otimes \hat{m}_j) \\ M^{-1} &= \sum\limits_{j=1}^n  \mu_j^{-1} (\hat{m}_j \otimes \hat{m}_j)\ \text{for}\ \mu_j \neq0 \end{align*}
%	
%	Firstly note that the normalised vectors are $\hat{m}_1=\begin{pmatrix} \frac{1}{\sqrt{3}}\\ \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \end{pmatrix}, \hat{m}_2=\begin{pmatrix} \frac{1}{\sqrt{14}}\\ \frac{2}{\sqrt{14}} \\ \frac{3}{\sqrt{14}} \end{pmatrix},$ and $\hat{m}_3=\begin{pmatrix} \frac{1}{\sqrt{26}}\\ \frac{3}{\sqrt{26}} \\ \frac{4}{\sqrt{26}} \end{pmatrix}$.
%	
%	We find the tensor products of each of the normalised vectors with itself to obtain $\hat{m}_1 \otimes \hat{m}_1 = \begin{pmatrix} \frac{1}{3}& \frac{1}{3}& \frac{1}{3}\\ \frac{1}{3}& \frac{1}{3}& \frac{1}{3}\\ \frac{1}{3}& \frac{1}{3}& \frac{1}{3}\end{pmatrix}, \hat{m}_2 \otimes \hat{m}_2 = \begin{pmatrix} \frac{1}{14}&\frac{2}{14}&\frac{3}{14}\\ \frac{2}{14}&\frac{4}{14}&\frac{6}{14}&\\ \frac{3}{14}&\frac{6}{14}&\frac{9}{14} \end{pmatrix}$, and $\hat{m}_3 \otimes \hat{m}_3 = \begin{pmatrix} \frac{1}{26} & \frac{3}{26} &\frac{4}{26} \\ \frac{3}{26}&\frac{9}{26}&\frac{12}{26}\\ \frac{4}{26}&\frac{12}{26}\frac{16}{26} \end{pmatrix}$.
%	
%	One can
%}

\coro{A bit of technical context}{Suppose that we removed the zero-eigenvalue by removing the part of $\vec{F}$ proportional to the eigenvector corresponding to the zero-eigenvalue. That way, we can recover the map between $\Delta-A$ and $\vec{F}$. Instead, we write, letting $M=\Delta-A$, $$\bar{M} = \sum\limits_{\hat{m}_j \neq \hat{m}_0} \mu_j (\hat{m_j} \otimes \hat{m_j})$$ simply by removing the faulty eigenvalue-eigenvector pair that forms our spectral decomposition from the summation. (Note that here we assume first that there is one faulty eigenvalue-eigenvector pair. If there happens to be multiple zero eigenvalues, then there would be more removals, which would be trickier to execute but the process is in essence the same. There would be, however, particular solutions for each zero eigenvalue.)

Correspondingly, we can write $\vec{F}=F_0\hat{m_0} + \vec{F}_\perp$ and $\vec{X}=X_0\hat{m_0} + \vec{X}_\perp$, where $\vec{F}_\perp$ and $\vec{X}_\perp$ contains a linear combination of eigenvectors all perpendicular to $\hat{m_0}$.  

By this, our matrix equation $(\Delta-A)\cdot \vec{X} = \vec{F}$ becomes $$(\Delta-A) \cdot (X_0\hat{m_0} + \vec{X}_\perp) =F_0\hat{m_0} + \vec{F}_\perp$$

Since $X_0\hat{m_0}$ maps to zero, this equation yields a particular solution satisfying $$\bar{M}\cdot \vec{X}_\perp = \vec{F}_\perp$$ and the homogeneous solution that satisfies $$F_0\hat{m_0}=0$$ under the requirement that $F_0 =0$. 

Now we recover the homogeneous and particular solutions. Given our rectified matrix $\bar{M} = \sum\limits_{\hat{m}_j \neq \hat{m}_0} \mu_j (\hat{m_j} \otimes \hat{m_j})$, its inverse is $\bar{M}^{-1} = \sum\limits_{\hat{m}_j \neq \hat{m}_0} \mu_j^{-1} (\hat{m_j} \otimes \hat{m_j})$. Therefore, the particular solution is $$\vec{X}_p=\bar{M}^{-1} \cdot \vec{F}_\perp$$ and the homogeneous solution is $$\vec{X}_h = h_0 \hat{m_0}$$ for some constant $h_0$. Therefore, the general solution is $$\vec{X}= \vec{X}_h+\vec{X}_p = h_0\hat{m_0} + \bar{M}^{-1} \cdot \vec{F}_\perp.$$}

The homogeneous solution is determined up to scale and thus the constant $h_0$ can take on any value. Theory-wise, what you see in the appendix regarding the construction of homogeneous solutions is illustrated here by the "faulty eigenvector" $\hat{m_0}$. 

%\ex{Removing faulty eigenvectors}{
%	Consider the matrix $$M= \frac{1}{4} \begin{pmatrix} 3&1&-7&3 \\ 1&3&3&-7 \\ -7&3&3&1 \\ 3&-7&1&3\end{pmatrix}.$$ The matrix has characteristic polynomial $$\lambda^4-3\lambda^3-4\lambda^2+12\lambda,$$ which yields the eigenvalues $\lambda= 3, -2, 2, 0.$ 
%}

\subsection{Exploring second-order difference equations}
 
 \defn{Second-order difference equations}{
 	A second-order difference equation is a difference equation of the form $$x_{t+2}+a\cdot x_{t+1}+b\cdot x_t=V(x_t)$$ where $V$ is a function. 
	
	In our notation of writing second-order difference equations in terms of the differences between successive terms, we can write $$(x_{t+2}-x_{t+1})-(x_{t+1}-x_t) = V(x_t)$$ or $$\Delta x_{t+1} - \Delta x_t = V(x_t).$$ 
 }
 
How do we create a difference map for second-order difference equations? By considering $\Delta^2.$ Let's first take a snapshot out of $\Delta$. Consider $$\Delta_{snap} = \begin{pmatrix}-1&1&0&0&0 \\0&-1&1&0&0 \\ 0&0&-1&1&0 \\ 0&0&0&-1&1 \\ 0&0&0&0&-1 \end{pmatrix}. $$ Notice that $$\Delta_{snap}^2 =  \begin{pmatrix}1&-2&1&0&0 \\0&1&-2&1&0 \\ 0&0&1&-2&1 \\ 0&0&0&1&-2 \\ 0&0&0&0&1 \end{pmatrix}.$$

This means that $\Delta^2$ is a matrix with all zeroes except for ones on its main diagonal, $-2$'s on the diagonal directly right of the main diagonal, and ones on the diagonal directly right of the diagonal containing the $-2$'s. 
 
\subsection{Taking snapshots out of the time-series}

Working with infinitely large matrices can seem rather dissatisfying. Instead of working with matrices that are infinitely large, our difference map $\Delta$ simplifies tremendously if we are describing economic activity that shows periodic behaviour. 

Suppose we only take a snapshot of four time periods - i.e. a 4-by-4 matrix $$\Delta_{snap} = \begin{pmatrix} -1&1&0&0 \\ 0&-1&1&0 \\ 0&0&-1&1 \\ 0&0&0&-1 \end{pmatrix}.$$ 

We have two approaches. We can either consider what happens when the time series ends at $x(4)$, or continue the time series by having $x(1)$ succeed $x(4)$, then having $x(2)$ succeed $x(1)$ and so on. The former case is what we call \emph{linear time}, and the latter case is called \emph{periodic time}, or \emph{circular time}.

\tbf{Case 1 (Time series ending at $x(4)$ - linear time):} Taking $\vec{X} = \{x(1), x(2), x(3), x(4)\}$, we get that $$\Delta_{snap} \cdot \vec{X} = \begin{pmatrix} x(2)-x(1) \\ x(3)-x(2) \\ x(4)-x(3) \\ -x(4) \end{pmatrix}$$ which is rather unsatisfying as it assumes that $x(5)$ and so on are all zero. 

Let's try finding homogeneous solutions. The equation $\Delta_{snap} \cdot \vec{X} =\vec{0}$ implies that $x(4)=0$. Additionally, $x(4)-x(3)=0$ also implies that $x(3)=0$, and soon we have that $\vec{X}=\vec{0}$. 

However, let's go one step back. $\Delta_{snap} \cdot \vec{X} =0$ is equivalent to saying that the derivative of the entries in $\vec{X}$ is zero, so $\vec{X}=\{k, k, k, k\}$ for some constant $k$. This works somewhat - $\Delta \cdot \vec{X} = \begin{pmatrix} 0\\0\\0\\-k \end{pmatrix}$ which isn't the zero matrix. Evidently we see that the absence of the $x(t)$ terms for $t\geq 5$ gives us some trouble - this makes sense as the time series is expected to go on forever, but does not. If we have infinitely many components, however, this problem conveniently vanishes. 

Is $\Delta_{snap}$ invertible? It is! We can write $$\Delta^{-1} = \begin{pmatrix} -1&-1&-1&-1\\0&-1&-1&-1\\0&0&-1&-1\\0&0&0&-1 \end{pmatrix}$$ and find that $$\vec{X} = \begin{pmatrix} -4f\\-3f\\-2f\\-f \end{pmatrix}.$$ Theoreticaly, this is how you "integrate a vector".

The characteristic polynomial of $\Delta_{snap}$ is $(\lambda+1)^4$ which yields an eigenvalue $\lambda=-1$ of multiplicity 4. The corresponding eigenvectors are all $\vec{v} = \begin{pmatrix} 1\\0\\0\\0\end{pmatrix}$.

\tbf{Case 2 ($x(4)$ leading back to $x(1)$ - circular time):} Instead of having $\Delta_{snap} \cdot \vec{X} = \begin{pmatrix} x(2)-x(1) \\ x(3)-x(2) \\ x(4)-x(3) \\ -x(4) \end{pmatrix}$, what if we had that $$\Delta_{snap} \cdot \vec{X} = \begin{pmatrix} x(2)-x(1) \\ x(3)-x(2) \\ x(4)-x(3) \\ x(1)-x(4) \end{pmatrix}$$ instead? This is equivalent to what number theorists call "taking the indices $\mod 4$" - i.e. replacing $x(t)$ with $x(t')$, where $t'$ is the remainder of $t$ when divided by 4 (we replace $x(0)$ with $x(4)$). The key point is that $x(4)$ circles back to $x(1)$ instead of just stopping at $x(4)$ like in Case 1. 

However, our new "snapshot" matrix, with the added $x(1)$ in the last row, is now $$\Delta_{snap} = \begin{pmatrix} -1&1&0&0 \\ 0&-1&1&0 \\ 0&0&-1&1 \\ 1&0&0&-1 \end{pmatrix}.$$ Let $\vec{x}=\{k, k, k, k\}$, as in Case 1. Evidently, $\Delta_{snap} \cdot \vec{x}=0$. 

What are the eigenvalues in this case? The characteristic equation of $\Delta_{snap}$ is $\lambda(\lambda^3+4\lambda^2+6\lambda+4)=0$ as the matrix has zero determinant, or $\lambda=-2, -1+i, -i, 0$. 

The corresponding eigenvectors are $\vec{v}_1=\{-1, 1, -1, 1\}, \vec{v}_2 = \{i, -1, -i, 1\}, \vec{v}_3 = \{-i, -1, i, 1\}$, and $\vec{v}_4 = \{1,1,1,1\}$.

\ex{5 time periods}{Suppose that we have a time series and we take a snapshot of 5 time periods. We differentiate between the \tbf{linear time} and the \tbf{circular time} scenarios. Suppose that in \tbf{linear time}, these periods adhere to the following relationship:
\begin{align*}
	x_2 -x_1&= 2x_1+0.4\\
	x_3 -x_2&= x_2+x_1 + 0.2\\
	x_4 -x_3&= 3x_3+x_2 + 0.3 \\
	x_5 -x_4&= x_4+2x_3 + 0.2 \\
	x_6 -x_5&= 2x_5 + 3x_4 + 0.3
\end{align*}

Recall that the system of differential equations $\vec{x}_{t+1} - \vec{x}_t = A(t) \cdot \vec{x}_t $ takes on the form $$\Delta \cdot \vec{X} = M \cdot \vec{X} + \vec{V}.$$ It turns out that our solution $\vec{X} = \{x_1, \dots, x_5\}$ is an eigenvector of the matrix $\Delta-A$, with eigenvalue 0.

Then note $\Delta = \begin{pmatrix} -1 & 1 & 0 & 0 & 0 \\ 0 & -1 & 1 & 0 & 0 \\ 0 & 0 & -1 & 1 & 0 \\ 0 & 0 & 0 & -1 & 1 \\ 0&0&0&0&1 \end{pmatrix} $. 

Thus, $\Delta \cdot \vec{X} = M \cdot \vec{X} + \vec{V}$ becomes $$ \begin{pmatrix} -1 & 1 & 0 & 0 & 0 \\ 0 & -1 & 1 & 0 & 0 \\ 0 & 0 & -1 & 1 & 0 \\ 0 & 0 & 0 & -1 & 1 \\ 0&0&0&0&1 \end{pmatrix} \cdot \begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}= \begin{pmatrix} 2 & 0 & 0 & 0 & 0 \\ 1 & 1 &  & 0 & 0 \\ 0 & 1 & 3 & 0 & 0 \\ 0 & 0 & 2 & 1 & 0 \\ 0&0&0&3&2 \end{pmatrix}\cdot  \begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}+ \begin{pmatrix} 0.4\\0.2\\0.3\\0.2\\0.3 \end{pmatrix}.$$

A solution would evidently exist if the matrix $\Delta-M$ has zero determinant. In this circumstance, it is the matrix $$\Delta -M  = \begin{pmatrix} -3&1&0&0&0 \\ -1&-2&1&0&0\\0&-1&-4&1&0 \\ 0&0&-2&-2&1 \\0&0&0&-3&-1 \end{pmatrix}$$

which has determinant $-169\neq0$. What if we were working in circular time? Not much would change, except for the fact that the time periods adhere to the relationship \begin{align*}
	x_2 -x_1&= 2x_1+0.4\\
	x_3 -x_2&= x_2+x_1 + 0.2\\
	x_4 -x_3&= 3x_3+x_2 + 0.3 \\
	x_5 -x_4&= x_4+2x_3 + 0.2 \\
	x_1 -x_5&= 2x_5 + 3x_4 + 0.3
\end{align*}

as $x_5$ cycles back to $x_1$. In this case, the matrix equation would be $$ \begin{pmatrix} -1 & 1 & 0 & 0 & 0 \\ 0 & -1 & 1 & 0 & 0 \\ 0 & 0 & -1 & 1 & 0 \\ 0 & 0 & 0 & -1 & 1 \\ 1&0&0&0&1 \end{pmatrix} \cdot \begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}= \begin{pmatrix} 2 & 0 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 & 0 \\ 0 & 1 & 3 & 0 & 0 \\ 0 & 0 & 2 & 1 & 0 \\ 0&0&0&3&2 \end{pmatrix}\cdot  \begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}+ \begin{pmatrix} 0.4\\0.2\\0.3\\0.2\\0.3 \end{pmatrix}.$$

A possible solution would be $$\vec{X} = (\Delta-M)^{-1} \cdot \vec{V}.$$ In this example, $$\vec{X} = \frac{1}{169} \begin{pmatrix}  -49 & -22 & -5 & -1 & -1 \\ 22 & -66 & -15 & -3 & -3 \\ -5&15&-35&-7&-7 \\ 2&-6&14&-31&-31 \\ -6&18&-42&93&-76\end{pmatrix} \cdot  \begin{pmatrix} 0.4\\0.2\\0.3\\0.2\\0.3 \end{pmatrix}.$$
}

% bit of spoilers?

\section{Appendix: Solving second-order differential equations}

\tbf{This is not exam content!} 

Frank does not explore specific ways or tools of solving differential equations. We will further illustrate the theory of homogeneous and particular solutions using second-order differential equations. Those who took A Level Further Maths in secondary school may find this rather familiar. 

\subsection{Homogeneous second-order differential equations}

\defn{Second-order differential equation}{A second-order differential equation is a differential equation of the form $$f(x)\frac{d^2y}{dx^2}+g(x)\frac{dy}{dx}+h(x)y=P(x).$$

A nice bit of intuition is that the solution to this differential equation is the sum of the homogeneous solution $x_h$ (that depends on $f(x)\frac{d^2y}{dx^2}+g(x)\frac{dy}{dx}+h(x)y=0$) and the particular solution $x_p$. 
}

Note that we call any differential equation where $P(x)=0$ a homogeneous differential equation. We first consider the theory behind homogeneous differential equations before viewing second-order theory in general. In this appendix, we will only deal with the case where $f(x), g(x)$, and $h(x)$ are constants. Note that just like first-order differential equations, second-order equations with non-constant coefficients do not always have closed-form solutions. 

\thrm{Linear combinations of solutions}{
	Let $f_1(x)$ and $f_2(x)$ be solutions to a homogeneous differential equation. Then, $$f(x)=c_1f_1(x)+c_2f_2(x)$$ is also a solution.
	
	The specific solution depends on the initial value conditions given.
}

\coro{}{ You will see in the following examples that there are exactly two constituent functions that make up the homogeneous solution. This will become clearer once we introduce the three cases encountered when finding homogeneous solutions. }

\defn{Characteristic equation of a homogeneous linear differential equation}{
	Let $$a y''+by'+cy=0$$ be a second-order homogeneous linear differential equation. Its characteristic equation is $ar^2+br+c=0$. 
}

The homogeneous solution is related to the roots of the characteristic equation. 

\tbf{Case 1 (Two distinct real roots):} If the roots $r_1$ and $r_2$ of the characteristic equation are distinct real roots, then the roots yield the solutions $y_1(t) =e^{r_1t}$ and $y_2(t) =e^{r_2t}$. By the theorem on linear combinations as discussed, the homogeneous solution is any linear combination $$y(t) = c_1e^{r_1t}+c_2e^{r_2t}$$ of $y_1(t)$ and $y_2(t)$. 

\ex{}{
	Consider $$y''+6y'+8y=0$$ with the initial value conditions $y(0)=0, y'(0)=1$. The associated characteristic equation is $r^2+6r+8=0$, with roots $r_1, r_2= -2, -4$.
	
	Therefore, the general solution is $y(t)=c_1 e^{-2t}+c_2e^{-4t}$ which admits the derivative $y'(t)=-2c_1e^{-2t}-4c_2e^{-4t}$. Substituting the initial conditions yield $c_1+c_2=0$ and $-2c_1-4c_2=1$, yielding the solutions $c_1=\frac{1}{2}, c_2=-\frac{1}{2}$. 
	
	The solution is $$y(t)=\frac{1}{2}e^{-2t}-\frac{1}{2}e^{-4t}.$$
}

\tbf{Case 2 (Repeated roots):} If the roots of the characteristic equation are $r_1=r_2$, then the homogeneous solution is any linear combination $$y(t) = c_1e^{rt}+c_2te^{rt},$$ where $r=r_1=r_2$ is the repeated root. Note the additional $t$ factor in the second term.

The roots yield the solutions $y_1(t)=e^{r_1t}=e^{rt}$ and $y_2(t)=e^{r_2t}=e^{rt}$. 

\ex{}{
	Consider the differential equation $$y''-4y'+4y=0$$ with the initial value conditions $y(0)=4, y'(0)=-2$. The associated characteristic equation is $r^2-4r+4=0$, yielding the repeated root $r=2$. 
	
	Thus, the general solution is $y(t) = c_1e^{2t}+c_2te^{2t}$ with derivative $y'(t)=2c_1e^{2t}+c_2e^{2t}+2c_2te^{2t}$. Substituting the initial value conditions yield us $c_1=4$ and $2c_1+c_2=-2$, or $c_2=-10$. 
	
	Thus, the solution is $$y(t) = 4e^{2t}-10te^{2t}.$$
}

\tbf{Case 3 (Complex conjugate roots):} If the roots of the characteristic equation are complex roots, they must be complex conjugates. Let the two roots be $r_1=a+bi, r_2=a-bi.$ The homogeneous solution is any linear combination $$y(t) = c_1e^{at}\cos(bt)+c_2e^{at}\sin(bt).$$ The roots yield the solutions $y_1(t)=e^{(a+bi)t}$ and $y_2(t) =e^{(a-bi)t}$, and the homogeneous solution is attained using Euler's formula $e^{it}=\cos(t)+i \sin (t)$

\ex{}{
	Consider the differential equation $$y''-2y'+4y=0$$ satisfying $y(0)=3, y'(0)=4.$ The characteristic equation $r^2-2r+4=0$ has roots $r_1, r_2 = 1\pm \sqrt{3}i$. The general solution is $y(t)=c_1e^t \cos(\sqrt{3}t)+c_2e^t \sin(\sqrt{3}t)$.
	
	Differentiation yields $$y'(t)=c_1e^{t}\cos(\sqrt{3}t)-\sqrt{3}c_1e^{t}\sin(\sqrt{3}t)+c_2e^{t}\sin(\sqrt{3}t)+\sqrt{3}c_2e^{t}\cos(\sqrt{3}t)$$ which is certainly a mouthful. Note that $y(0)=3$ yields $c_1=3$, so now we have $$y'(t)=3e^{t}\cos(\sqrt{3}t)-3\sqrt{3}e^{t}\sin(\sqrt{3}t)+c_2e^{t}\sin(\sqrt{3}t)+\sqrt{3}c_2e^{t}\cos(\sqrt{3}t)$$ Now, $y'(0)=4$, so $y'(0)=3-\sqrt{3}c_2=4$ yields $c_2=-\frac{1}{\sqrt{3}}$. The solution is thus $$y(t)=3e^t\cos(\sqrt{3}t)-\frac{1}{\sqrt{3}} e^t \sin(\sqrt{3}t).$$
}

\subsection{Inhomogeneous second-order differential equations}

Having dealt with the homogeneous solutions, let's move on to the particular solutions. First, some intuition on how the particular solutions relate to the homogeneous solutions.

\thrm{}{
	Let \begin{equation} ay''+by'+cy=P(x)\end{equation} be an inhomogeneous differential equation. Furthermore, let \begin{equation} ay''+by'+cy=0\end{equation} be its associated homogeneous differential equation. Define $Y_1(x)$ and $Y_2(x)$ as the two solutions to the differential equation. Then, $Y_1(x)-Y_2(x)$ is a solution to the associated homogeneous differential equation (14.5). 
	
	By the theorem on linear combinations discussed earlier on, this means that we can write $$Y_1(x)-Y_2(x)=c_1y_1(x)+c_2y_2(x),$$ where $y_1(x)$ and $y_2(x)$ are solutions to (14.5). 
}

We can directly check that $Y_1(x)-Y_2(x)$ is a solution to the homogeneous differential equation. By definition, we have that \begin{align*} &\ \ \ \ \ a(Y_1(x)-Y_2(x))''+b(Y_1(x)-Y_2(x))'+c(Y_1(x)-Y_2(x)) \\ &= (aY_1(x)+bY_1(x)+cY_1(x))-(aY_2(x)+bY_2(x)+cY_2(x)) \\ &= P(x)-P(x) \\ &=0.\end{align*}

Why this theorem? Since $Y_1(x)$ and $Y_2(x)$ are any solutions, let's demonstrate the relationship between the homogeneous and particular solutions by letting $Y_1(x)=y(x)$ be the general solution. Furthermore, let $y_p(x)$ be any arbitrary particular solution. The theorem states that we can write \begin{align*} y(x)-y_p(x) &= c_1y_1(x)+c_2y_2(x) \\ y(x) &= y_p(x) +c_1y_1(x)+c_2y_2(x) \\ y(x) &= y_p(x)+y_h(x).\end{align*}

How do we find these particular solutions? Two methods are commonly used - the Method of Undetermined Coefficients, and the Variation of Parameters. The Method of Undetermined Coefficients may seem unsatisfying, for it lies in a guess-and-check method of finding the particular solutions and only works for a small class of $P(x)$. 

\ex{Method of Undetermined Coefficients}{
	The differential equation $$y'' - 10y'+24y = 4e^{5x}$$ has characteristic equation $r^2-10r+24$, meaning that the two roots are $r=6, r=4.$ However the focus here lies not in the homogeneous solutions, but in the meaning of $4e^{5x}$. 
	
	Our aim with this method is to guess a sensible function that, when put through the L.H.S., will yield an expression similar to the R.H.S.
	
	Since we know that exponential functions are invariant under differentiation except for the changing of coefficients, a good first guess would be to set a particular solution to be of the form $y_p(x)=Ae^{5x}$, and then differentiate. Note that $y''-10y'+24y = 25Ae^{5x}-50Ae^{5x}+24Ae^{5x}=4e^{5x}$, meaning that $A=-4.$ A particular solution to the differential equation is thus $y_p(x)=-4e^{5x}$. 
	
	But wait! Aren't there infinitely many particular solutions? We invoke help from the homogeneous solutions. From the roots, we have that $y_h(x)=c_1e^{6x}+c_2e^{4x}$. Thus, the general solution is \begin{align*} y(x) &= y_h(x) + y_p(x) \\ &= c_1e^{6x}+c_2e^{4x}-4e^{5x}. \end{align*}
	
	Finding specific values of $c_1$ and $c_2$ relies on the initial conditions. 
}

We were lucky that the exponential function had such nice, convenient qualities. Polynomials are a class of functions that fortunately also share such pleasant qualities. 

\ex{}{
	Consider the differential equation $$y''-10y'+24y = 4x^2-6x+10.$$
	
	It makes sense for us to try a quadratic polynomial as the derivative of a quadratic polynomial is a linear polynomial and the second derivative of a quadratic polynomial is a constant. Therefore, let $y_p(x) = Ax^2+Bx+C$. Since $y_p'(x)=2Ax+B$ and $y_p''(x)=2A$, we have that \begin{align*} 2A-10(2Ax+B)+24(Ax^2+Bx+C) &= 2A-20Ax-10B+24Ax^2+24Bx+24C \\ &= 4x^2-6x+10. \end{align*}
	
	Comparing coefficients, we get that $A=\frac{1}{6}, B=-\frac{1}{9}, C=\frac{77}{216}$ and thus our particular solution is $y_p(x) = \frac{1}{6}x^2-\frac{1}{9}x+\frac{77}{216}.$
}

Here is a rule of thumb for this method.

\begin{enumerate}
	\item If $P(x)=ae^{bt}$, guess $Ae^{Bt}$.
	\item For any sinusoidal function $a_1\cos(b_1t), a_2\sin(b_2t)$, or sums of sinusoidal functions $a_1\cos(bt)+a_2\sin(bt)$, guess $A \cos(bt)+B\sin(bt)$.
	\item For a degree $n$ polynomial, guess a degree $n$ polynomial.
\end{enumerate}

Sums and products of the functions described above tend to pose us a greater challenge, but they work in a similar fashion.

\ex{}{
	Notice that the right-hand side of the differential equation $$y''+6y'+8y=-2xe^{2x}$$ is a product of two functions - a polynomial $-2x$ and an exponential function $e^{2x}$. An educated guess would be to put the function $(Ax+B)(Ce^{2x})$. 
	
	Differentiation yields $ACe^{2x}+2(Ax+B)(Ce^{2x}) = (AC+2BC)e^{2x}+2Axe^{2x}.$ We would then set $2A=-2$ and $AC+2BC=0$ and proceed. This is a system of two equations in three unknowns. We cannot solve for all the constants. A mathematically astute observation would be to multiply the $C$ into $Ax+B$. Note that $Ce^{2x}(At+B) = e^{2x}(ACt+BC)$ means that we can simply remove the constant $C$ - $AC$ and $BC$ are constants, after all.
	
	We get $$4A e^{2x} + 2(2Ax + 2B)e^{2x} + 6(Ae^{2x}+2(Ax+B)e^{2x})+(Ax+B)(e^{2x})=-2xe^{2x}.$$ Isolating the $xe^{2x}$ and $e^{2x}$ terms and simplifying yields $A=-\frac{1}{8}$ and $B=\frac{11}{136}$. 
}

In general, particular solutions involving products of functions on the right-hand side involve conjuring up guesses for each of the individual functions that make up the product, then making some adjustments such that each of the variables can be solved for individually.

Sums of functions are easier. If $y_{p1}(t)$ and $y_{p2}(t)$ are particular solutions for $ay''+by'+cy=P_1(x)$ and $ay''+by'+cy=P_2(x)$ respectively, then $y_{p1}(t)+y_{p2}(t)$ is a particular solution for $$ay''+by'+cy = P_1(x)+P_2(x).$$

\ex{Guesses for particular solutions}{
	If we had a differential equation $$y''-6y'+2y = e^{6x}+4te^{2x}$$ then the guess for the particular solution would be of the form $$y_p(x) = Ae^{6x}+(Bt+C)e^{2x}.$$ 
	
	For the differential equation $$y''+2y'-4y=3x^2+2\cos{2x},$$ we would guess $$y_p(x) = Ax^2+Bx+C+D\cos(2x)+E\sin(2x).$$
}

Take care in ensuring that after performing differentiation and comparing coefficients, you obtain a system of equations with a unique solution.

\defn{Variation of Parameters}{
	The method of parameter variation is a sophisticated one. Consider a differential equation of the form $$y''+ay'+by = P(x),$$ and let $y_1(x)$ and $y_2(x)$ be the two constituent functions of the homogeneous solution. 
	
	Then, the particular solutions to the differential equation are of the form $$y_p(x) = -y_1 \int \frac{y_2 P(x)}{W(y_1(x), y_2(x))}\ dx+y_2 \int \frac{y_1 P(x)}{W(y_1(x), y_2(x))}\ dx,$$
	
	where the function $W(y_1(x), y_2(x))$ is the determinant of the \emph{Wronskian matrix} $$W = \begin{pmatrix} y_1 & y_2 \\ y_1' & y_2' \end{pmatrix}$$ which is $y_1y_2'-y_2y_1'$. 
	
	In other words, the particular solutions to the differential equation are $$y_p(x) = -y_1 \int \frac{y_2 P(x)}{y_1y_2'-y_2y_1'}\ dx+y_2 \int \frac{y_1 P(x)}{y_1y_2'-y_2y_1'}\ dx.$$
}

A pain point of this method lies in the non-trivial nature of solving the integrals. This method also requires the homogeneous solutions $y_1(x)$ and $y_2(x)$ before the particular solutions are found, unlike the method of undetermined coefficients.  

\coro{}{ It doesn't matter which of the constituent functions are $y_1(x)$ or $y_2(x)$, for they yield the same result. A neat exercise would be to prove this fact. }

Before we jump into some examples, I would like to comment on the constants of integration after performing the two integrals. Let the constant of integration for the first and second terms of $y_p(x)$ be $c_1$ and $c_2$ respectively. Then, \begin{align*} y_p(x) &= -y_1 \left( \int \frac{y_2 P(x)}{y_1y_2'-y_2y_1'}\ dx+c_1 \right) + y_2 \left( \int \frac{y_1 P(x)}{y_1y_2'-y_2y_1'}\ dx +c_2 \right) \\ &= -y_1 \int \frac{y_2 P(x)}{y_1y_2'-y_2y_1'}\ dx+y_2 \int \frac{y_1 P(x)}{y_1y_2'-y_2y_1'}\ dx - y_1c_1 + y_2c_2. \end{align*} 

Evidently we see the addition of the term $- y_1c_1 + y_2c_2$.

But wait! Isn't $- y_1c_1 + y_2c_2$ a solution to the homogeneous equation? It turns out that it is a solution, and equals zero. Therefore, the constants of integration cancel out. For simplicity's sake, we take $c_1$ and $c_2$ to be zero.

\ex{}{
	Consider the differential equation $$y''-2y'+y = x^2+4e^x.$$ The characteristic equation $r^2-2r+1=0$ has repeated roots, so the homogeneous solution is $$y_h(x) = c_1e^x+c_2 xe^x.$$ Here the two constituent functions are $e^{x}$ and $xe^{x}$, so the Wronskian is the determinant of the matrix $\begin{pmatrix} e^x & xe^x\\ e^x & e^x+xe^x \end{pmatrix}$ which is $e^x(e^x+xe^x) - xe^x(e^x) = e^{2x} + xe^{2x}-xe^{2x} = e^{2x}$. 
	
	Without loss of generality, let $y_1=e^x$ and $y_2=xe^x$. Thus, \begin{align*} y_p(x) &= -e^x \int \frac{xe^x(x^2+4e^x)}{e^{2x}}\ dx + xe^x \int \frac{e^x(x^2+4e^x)}{e^{2x}}\ dx \\ &= -e^x \int xe^{-x}(x^2+4e^x)\ dx + xe^x \int e^{-x}(x^2+4e^x)\ dx \\ &= -e^x \int x^3 e^{-x} + 4x\ dx + xe^x \int x^2e^{-x}+4\ dx \end{align*}
	
	The integrals $\int x^3 e^{-x}\ dx$ and $\int x^2 e^{-x}\ dx$ can be solved using repeated integration by parts. Omitting steps, the particular solution is $$y_p(x) = -x^3+ x^2+4x+6 +(-2x^2+4x)e^x.$$ Therefore, the general solution is $$y(x) = c_1e^x+c_2 xe^x-x^3+ x^2+4x+6 +(-2x^2+4x)e^x.$$
}

\ex{}{
	Now consider $$4y''-y = 6 \sin(3x).$$ The characteristic equation is $4r^2-1$, which yield the real roots $r=\pm \frac{1}{2}.$ The homogeneous solution is thus $c_1e^{\frac{x}{2}}+c_2e^{-\frac{x}{2}}$. 
	
	Let $y_1(x)=e^{\frac{x}{2}}$ and $y_2(x)=e^{-\frac{x}{2}}$. Thus, $y_1'(x) = \frac{1}{2}e^{\frac{x}{2}}$ and $y_2'(x) = -\frac{1}{2}e^{-\frac{x}{2}}.$ This means that the determinant of the Wronskian is $e^{\frac{x}{2}} \cdot (-\frac{1}{2}e^{-\frac{x}{2}}) - e^{-\frac{x}{2}} \cdot \frac{1}{2}e^{\frac{x}{2}}=-\frac{1}{2}-\frac{1}{2}=-1.$
	
	The particular solution is \begin{equation*} y_p(x) = -e^{\frac{x}{2}} \int -6e^{-\frac{x}{2}}\sin(3x)\ dx +e^{-\frac{x}{2}} \int -6e^{\frac{x}{2}} \sin(3x)\ dx  \end{equation*}
	
	wherein the two constituent integrals are solvable (albeit tedious) by performing integration by parts twice. We obtain the solution $y_p(x)=-\frac{24}{37} \sin(3x)$. The general solution is $$y(x)=c_1e^{\frac{x}{2}}+c_2e^{-\frac{x}{2}}-\frac{24}{37} \sin(3x).$$
}

We see that particular solutions involving sinusoidal functions or exponential functions are tedious to solve, but at least solvable. A simple example of an second-order differential equation that cannot be solved using our usual methods is $$y'' = e^{-x^2}$$ as the integral of $e^{-x^2}$ does not contain an elementary solution. Though consider $$y''-y = e^{-x^2}.$$ 

The homogeneous solutions satisfy $y_h(x)=c_1 e^x+c_2 e^{-x}$. Note $y_1(x)=e^x, y_2(x)=e^{-x}, y_1'(x)=e^x, y_2(x)=-e^{-x}$, and thus the determinant of the Wronskian is $-e^x(e^{-x})-e^{-x} e^x=-2$. However note that the integral $\int y_2(x) P(x)\ dx = \int e^{-x} e^{-x^2}\ dx=\int e^{-x^2-x}\ dx$, which does not have a closed-form solution. This demonstrates that a problem with the variation of parameters method is the inability to solve the integrals within the particular solution.

\chapter{Week 15}

The chief aim of this chapter is simple. It is to apply the material on function spaces in difference equations to differential equations. It is similar to the lecture content last week.

\section{First-order ODEs and their states}

Consider a linear first-order ordinary differential equation of the form $$\frac{dy}{dx} = g(x) \cdot y(x)+f(x).$$ Here we split up into two cases - where $f(x)=0$ and where $f(x)\neq0$. Recall from last week that they correspond to homogeneous and inhomogeneous solutions of the ODE. 

Similar to last week, we first consider the simpler case where $g(x)=g$ and $f(x)=f$ are constant functions before considering general $g(x)$ and $f(x)$.

\tbf{Case 1 (Constant coefficients):} Consider the equation $$\frac{dy}{dx}=gy+f.$$ From the appendix in Week 7, one can use the integrating factor method to solve this equation. The integrating factor is $e^{-gx}$, yielding us \begin{align*} e^{-gx} \frac{dy}{dx} &= e^{-gx} (gy+f) \\ (ye^{-gx})' &= e^{-gx}f \\ ye^{-gx} &= -\frac{f}{g} e^{-gx}+C \\ y&= -\frac{f}{g}+Ce^{-gx}.\end{align*}

Of course, the stationary solution is when $gy+f=0$, or $$y=-\frac{f}{g}.$$ But wait! Wasn't the stationary solution featured in the solution by integrating factor?

Last week's chapter discussed homogeneous and particular solutions, where any general solution to this differential equation would consist of the sum of the particular and a constant multiple of the homogeneous solution. The homogeneous solution arises when $f=0$. Note that the particular solution is precisely the stationary solution in this case.

Evidently, when $f=0$, we have that $\frac{dy}{dx} = gy$ yields us $y(x)= ce^{gx}$ for constants $c$ and $g$. Thus, any general solution is of the form $$kce^{gx}-\frac{f}{g}$$ which is a sum of homogeneous and particular solutions. Note that we truncate $kc$ to just $c$ as both $k$ and $c$ are constants.

Are there asymptotic states? Keep in mind that since $y(x) = ce^{gx}$, the asymptotic states rely on the convergence of $y(x)$. There are asymptotic states if $g<0$, and there are no asymptotic states if $g>0$.

Are there periodic states? No, as $y'(x) >0$.

\ex{}{
	Let $f=4$ and $g=2$. The differential equation $$\frac{dy}{dx} = 2y+4$$ yields the stationary state $y=-2.$ The solutions to the differential equation is $$ce^{2x}-2,$$ for $c$ a constant. Evidently, this comprises the homogeneous solution $ce^{2x}$ and the particular solution $-2$. 
}

\tbf{Case 2 (Non-constant coefficients):} When we have $\frac{dy}{dx}=g(x)y+f(x)$, we can similarly use the method of integrating factors to obtain a solution. Indeed, for a homogeneous equation $$\frac{dy}{dx} = g(x) y,$$ the solution $y=ce^{\int g(x)\ dx}$ suffices.

Note that the integrating factor is $e^{\int -g(x)\ dx}$. If we include the $f(x)$ term, the solution looks like this:
\begin{align*}
\frac{dy}{dx} - g(x)y &= f(x) \\
e^{-\int g(x)\,dx}\left(\frac{dy}{dx} - g(x)y\right) &= e^{-\int g(x)\,dx}f(x) \\
\frac{d}{dx}\Bigl(e^{-\int g(x)\,dx}y\Bigr) &= e^{-\int g(x)\,dx}f(x) \\
e^{-\int g(x)\,dx}y &= \int e^{-\int g(x)\,dx}f(x)\,dx + C \\
y &= e^{\int g(x)\,dx}\left(\int e^{-\int g(x)\,dx}f(x)\,dx + C\right) \\
y &= \underbrace{e^{\int g(x)\,dx}\int e^{-\int g(x)\,dx}f(x)\,dx}_{\text{particular solution}}
    + \underbrace{Ce^{\int g(x)\,dx}}_{\text{homogeneous solution}}.
\end{align*}

which as you can probably expect, consists of the homogeneous solution $Ce^{\int g(x)\ dx}$ and the particular solution $e^{\int g(x)\ dx} \int e^{\int -g(x)\ dx} f(x) \ dx$. 

The stationary states are $y(x) = -\frac{f(x)}{g(x)}$. The existence of asymptotic states depends on the convergence of the function $g(x)$ as $x \to \infty$; generally, there would not be periodic states.

\section{System of linear ODEs}

Now consider a system of linear ordinary differential equations. This is a system of the form $$\frac{d\vec{y}}{dx} = M \cdot \vec{y} + \vec{f}.$$ The homogeneous solution is attained when $\vec{f}=\vec{0}$. 

First we consider the homogeneous solutions. They occur when $M \cdot \vec{y} + \vec{f}=0$, or $$\vec{y} = -M^{-1} \cdot \vec{f}.$$ Of course this depends on whether $M$ is invertible, or has non-zero determinant. 

What if $M$ is non-invertible? Recall that we encountered this situation in Week 14; we deal with an analogous scenario in Week 15. In this case, we remove all the eigenvalue-eigenvector pairs with eigenvalues of zero so as to make the determinant non-zero. Here, we write $$M=\sum\limits_{j=1}^n \mu_j (\hat{m}_j \otimes \hat{m}_j)$$ with $\mu_j$ and $\hat{m}_j$ being the eigenvalues and eigenvectors of $M$. We then remove the zero eigenvalues to obtain a rectified matrix $$\bar{M} = \sum\limits_{j} \mu_j (\hat{m}_j \otimes \hat{m}_j)\ \text{except when}\  \hat{m}_j =0.$$  

Recall that the homogeneous solution corresponds to the vectors satisfying $\vec{f}=0$, which just so happens to be the eigenvectors corresponding to the zero-eigenvalue that we just omitted. Thus, the homogeneous solution is $c \hat{m}_0$ where $\hat{m}_0$ is the eigenvector corresponding to the zero-eigenvalue. We conclude that $$\vec{y} = c\hat{m}_1 - \bar{M}^{-1}.$$

\coro{}{ If there is more than one zero-eigenvalue, the homogeneous solution is a linear combination of the eigenvectors corresponding to the zero-eigenvalue. This is $c_0 \hat{m}_0^{0}+\dots+c_n \hat{m}_0^{n}$.}

What do the general solutions look like? Recall that the normalised eigenvectors, given that we work with symmetric matrices $M$, form an orthonormal eigenbasis for $M$. Thus, each solution $\vec{y}$ is a linear combination $$\vec{y}(x) = \sum\limits_{j=1}^n y_j(x) \hat{m}_j$$ and each particular solution $\vec{f}$ is a linear combination $$\vec{f} = \sum\limits_{j=1}^n f_j \hat{m}_j.$$ 

Observe that \begin{align*} \frac{d\vec{y}}{dx} &= M \cdot \vec{y} + \vec{f} \\ \frac{d}{dx} \left(\sum\limits_{j=1}^n y_j(x) \hat{m}_j \right)&= M \cdot \left( \sum\limits_{j=1}^n y_j(x) \hat{m}_j \right) + \left( \sum\limits_{j=1}^n f_j \hat{m}_j\right) \\ \sum\limits_{j=1}^n \left(\frac{d}{dx} y_j(x) \hat{m}_j  \right)&= \sum\limits_{j=1}^n \left( y_j(x) \cdot (M \cdot \hat{m}_j) + f_j\hat{m}_j \right) \\ \sum\limits_{j=1}^n \left(\frac{d}{dx} y_j(x) \hat{m}_j  \right)&= \sum\limits_{j=1}^n \left( y_j(x) \cdot (\mu_j \cdot \hat{m}_j) + f_j\hat{m}_j \right) \end{align*} where the last line used the fact that the eigenvalue equation satisfies $M \cdot \hat{m}_j = \mu_j \cdot \hat{m}_j$.

Note that component-wise, we have that $$\frac{d}{dx} y_j(x) = \mu_j y_j(x)+f_j.$$ The method of integrating factors yield the solution $$y_j(x) = c_j e^{\mu_j x}-\frac{f_j}{\mu_j}.$$ In essence, we separated a system of $n$ differential equations into its constituent parts. This is important as one differential equation may contain sums of multiple constituent functions. An example is $\frac{dy_1(x)}{dx} = 2y_1(x)+4y_2(x)+3, \frac{dy_2(x)}{dx}=\dots.$

This is complicated to solve. Our eigenvalue method represents $\frac{dy_1(x)}{dx}$ (and for that matter, $\frac{dy_j(x)}{dx}$) solely in terms of terms linear in $y_1(x)$. 

To recover our general solution from the constituent solutions in $y_j(x)$, we replace our $\mu_j$'s with $M$'s and replace coefficients with vectors of coefficients. We note that $-\frac{f_j}{m_j} = -M^{-1} \cdot \vec{f}$ and $c_j e^{m_j x} = \vec{c} \cdot e^{Mx}$. Thus, instead of $y_j(x) = c_j e^{\mu_j x}-\frac{f_j}{m_j}$, we have the general solution $$\vec{y}(x) = \vec{c}  \cdot e^{Mx} - M^{-1} \cdot \vec{f}.$$

In difference and differential equations we are usually given an initial condition. Suppose that are initial condition $\vec{y}(0)=\vec{y}_0$. Then, the general solution yields $\vec{y}(0) = \vec{c} - M^{-1} \cdot \vec{f}$, or $$\vec{c} = \vec{y}(0)+M^{-1} \cdot \vec{f}.$$

Are there asymptotic states? Well, note that our general solution has the term $e^{Mx}$. Chapter 6 tells us that we can use eigendecomposition to attain $e^{Mx}=\sum\limits_{j=1}^n e^{\mu_j x} (\hat{m}_j \otimes \hat{m}_j)$. Therefore, asymptotic states exist if the eigenvalues $\mu_j$ are all negative.

We entertained a discussion of periodic states in Week 7's discussion, and also briefly in Week 14's discussion of difference equation. Note that if we let $\mu_j = \alpha_j + \beta_j i$, then $$e^{\mu_j x} = e^{(\alpha_j + \beta_ji)x} = e^{\alpha j} (\cos (\beta x)+i \sin (\beta x)),$$ where the existence of $\cos (\beta x)+i \sin (\beta x)$ demonstrates periodic behaviour.

\ex{}{
	Consider the system of differential equations given by $$\frac{d\vec{y}}{dx} = \begin{pmatrix} 1&2  \\ 3&2\end{pmatrix} \vec{y}$$ where $\vec{y}(0) = \begin{pmatrix} 0\\4 \end{pmatrix}$. This example has $\vec{f}=0$ so our solution is in the form $$\vec{y}(x) = \vec{y}(0) \cdot e^{Mx}.$$
	
	Note that the eigenvalues of $M=\begin{pmatrix} 1&2 \\ 3&2\end{pmatrix}$ are $\lambda_1, \lambda_2 = -1, 4$ with corresponding eigenvectors $\vec{m}_1=\begin{pmatrix} -1 \\ 1 \end{pmatrix}$ and $\vec{m}_2=\begin{pmatrix} 2 \\ 3 \end{pmatrix}$. Normalisation yields $\hat{m}_1=\begin{pmatrix} -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}$ and $\hat{m}_2=\begin{pmatrix} \frac{2}{\sqrt{13}} \\ \frac{3}{\sqrt{13}} \end{pmatrix}$. 
	
	Thus the spectral decomposition of $e^{Mx}$ satisfies \begin{align*} e^{Mx} &= \sum\limits_{i=1}^2 e^{m_ix} (\hat{m}_i \otimes \hat{m}_i) \\ &= e^{-x} \cdot \begin{pmatrix} \frac{1}{2} & -\frac{1}{2} \\ -\frac{1}{2} & \frac{1}{2} \end{pmatrix} + e^{4x} \cdot \begin{pmatrix} \frac{4}{13} & \frac{6}{13} \\ \frac{6}{13} & \frac{9}{13} \end{pmatrix} \\ &= \begin{pmatrix} \frac{1}{2}e^{-x}+\frac{4}{13}e^{4x} & -\frac{1}{2}e^{-x}+\frac{6}{13}e^{4x} \\ -\frac{1}{2}e^{-x}+\frac{6}{13}e^{4x} & \frac{1}{2}e^{-x}+\frac{9}{13}e^{4x}\end{pmatrix} \end{align*}
	
	The general solution is $$\vec{y}(x)=\vec{y}(0) \cdot e^{Mx} = \begin{pmatrix} -2e^{-x}+\frac{24}{13} e^{4x} \\ 2e^{-x}+\frac{36}{13} e^{4x} \end{pmatrix}. $$
}

\section{Linear approximations to non-linear ODEs}

\defn{Non-linear systems of ODEs}{
	A non-linear system of ODEs is a system of the form $$\frac{d\vec{y}}{dx}=\vec{V}(\vec{y}(x)).$$ This is a system of $n$ variables, where $n$ is the dimension of $\vec{V}$.
}

\ex{}{
	If we take $\vec{V}$ to have 3 components (and thus $\vec{y}$ has 3 components too), an example of a first-order non-linear system of differential equations looks like \begin{align*}  \frac{dy_1}{dx} &= x^2y_1(x)+2y_2(x) \\ \frac{dy_2}{dx} &= y_3(x)+4 \\ \frac{dy_3}{dx} &= y_2(x)+xy_3(x)-3. \end{align*}
}

The way we use Jacobians as a means of linear approximation is very similar to what we've encountered in the past, especially in the part on non-linear approximation in Week 7. In particular, we linearise a function around the stationary state $\vec{y}_s$. Note that if you write $\vec{y}(x) = \vec{y}_s + \vec{\delta}(x)$, where $\vec{\delta}(x)$ denote small deviations around the stationary state, then $$\vec{V}(\vec{y}(x)) = \vec{V}(\vec{y}_s) + \vec{\delta}(x) \cdot J,$$ where $J$ is the Jacobian matrix $$J = \begin{pmatrix} \frac{\partial}{\partial y_1} V_1(\vec{y}_s) & \hdots & \frac{\partial}{\partial y_1} V_n(\vec{y}_s) \\ \vdots & \ddots & \vdots \\ \frac{\partial}{\partial y_n} V_1(\vec{y}_s) & \hdots & \frac{\partial}{\partial y_n} V_n(\vec{y}_s)\end{pmatrix}.$$

\coro{}{ Week 7 describes this theory in terms of \emph{difference equations}, where the Jacobian is used to describe the difference $\delta_{t+1} - \delta_t$. Nevertheless it should not be difficult to find an analogue between difference and differential equations. The equation for an approximation near the stationary state is \begin{align*}\frac{d}{dx} \vec{y}(x) &= \vec{V}(\vec{y}(x)) \\ &= \vec{V}(\vec{y}_s) + \vec{\delta}(x) \cdot J\end{align*} thus forming a linear approximation of a non-linear system of differential equations. }

Performing eigendecomposition on the Jacobian matrix may seem difficult if the matrix is not symmetric. Nevertheless, a discussion of this comes in Week 16. For now, note that if the vector $\vec{V}(\vec{y})$ is the gradient $\nabla \vec{f}(\vec{y})$ of a function $\vec{f}(\vec{y})$, then the Jacobian is symmetric as the Jacobian matrix of a gradient function is a Hessian matrix. If you're unconvinced that the Jacobian of a gradient function is a Hessian, directly evaluate the second partial derivatives.

After performing eigendecomposition, one can refer to the section above to determine stationary, asymptotic, and periodic states. 

\chapter{Week 16}

In previous weeks, we have assumed that all eigenvalue and eigenvector problems pertain to symmetric matrices. This week's content extends this theory to non-symmetric matrices. 

\section{Left and right real-valued eigenvectors}

Recall that in Week 6, we encountered the \emph{spectral theorem for symmetric matrices}, which states that:

\defn{Spectral theorem}{
	Let $M$ be a symmetric matrix with $n$ rows and columns and real entries. Then, \begin{enumerate} \item $M$ has $n$ real eigenvalues.\item $M$ has $n$ eigenvectors that form an orthonormal basis. \end{enumerate}
}

Two questions arise. 

\tbf{What is the use of this theorem?} The spectral theorem means that we can reconstruct our matrix $$M=\sum\limits_{j=1}^n \mu_j (\hat{m}_j \otimes \hat{m}_j)$$ since the eigenvectors form an orthonormal basis.

\tbf{Why restrict ourselves to symmetric matrices?} Symmetric matrices satisfy $M^T=M$. Note that in general, matrices do not commute with vectors. So the eigenvalue problem $$M \cdot \vec{m}_j = \mu_j \cdot \vec{m}_j$$ is different from $$\hat{m}_j \cdot M= \hat{m}_j \cdot \mu_j.$$ The property $M^T=M$ saves this as $$\hat{m}_j \cdot M = M^T \cdot \hat{m}_j = M \cdot \hat{m}_j$$ since $M^T=M$. 

In general, when we aren't working with symmetric matrices, we have to make this distinction. How? Using \emph{left and right eigenvectors}.

\defn{Left and right eigenvectors}{
	Recall that the eigenvalue problem is $$M \cdot \vec{m}_j = \mu_j \cdot \vec{m}_j.$$ Though note that $\vec{m}_j \cdot M$ and $M \cdot \vec{m}_j$ yield separate results for non-symmetric matrices.
	
	The \tbf{right eigenvector} is defined as the eigenvector $\vec{r}_j$ such that $$M \cdot \vec{r}_j = \mu_j \cdot \vec{r}_j$$ such that $\vec{r}_j$ is the right term of the left-hand side. Meanwhile, the \tbf{left eigenvector} is defined as the eigenvector $\vec{l}_j$ satisfying $$\vec{l}_j \cdot M  = \vec{r}_j \cdot  \mu_j. $$
	
	Note there are no left and right eigenvalues as real-valued scalars commute under vector and matrix multiplication.
}

The set of eigenvectors of a symmetric matrix are orthogonal to one another. Let $M \vec{v}_1 = \lambda_1 \vec{v}_1$ and $M \vec{v}_2 = \lambda_2 \vec{v}_2$. Note that \begin{align*} \lambda_1 \cdot (\vec{v}_1 \cdot \vec{v}_2) &= (M \vec{v}_1) \cdot \vec{v}_2 \\ &= \vec{v}_1 \cdot (M \vec{v}_2) \\ &= \vec{v}_1 \cdot (\lambda_2 \cdot \vec{v}_2)\end{align*}

which leaves the conclusion $\vec{v}_1 \cdot \vec{v}_2 =0$. 

Now we translate this to left and right eigenvectors. In the following, observe that $\mu_2$ is the eigenvalue corresponding to $\vec{r}_2$, and $\mu_1$ is the eigenvalue corresponding to $\vec{r}_1$. We have that $$\vec{l}_1 \cdot (M \cdot \vec{r}_2) =  \vec{l}_1 \cdot \mu_2 \vec{r}_2.$$ Under another way of putting our brackets, we have $$(\vec{l}_1 \cdot M) \cdot \vec{r}_2 = \mu_1 \vec{l}_1 \cdot \vec{r}_2.$$ 

Assume that $\mu_1 \neq \mu_2$. This then means that $\vec{l}_1 \cdot \vec{r}_2=0$, which is the same as our symmetric matrix case. This suggests that we can invariably find a set of left and right eigenvectors for any non-symmetric eigenvalue problem that satisfies \[
\vec{l}_j \cdot \vec{r}_k =
\begin{cases}
0 & \text{if } j \neq k, \\
\text{non-zero} & \text{if } j = k.
\end{cases}
\]

If we had a normalised set of left and right-eigenvectors $\hat{l}_j$ and $\hat{l}_k$ instead, then $$\hat{l}_j \cdot \hat{r}_k= \begin{cases} 0 & \text{if}\ j =k \\ 1& \text{if}\ j \neq k. \end{cases}$$

\coro{}{
	The relationship between the left and right-eigenvectors can be simplified to the statement $$\vec{l}_j \cdot \vec{r}_k = \delta_{jk} (\vec{l}_j \cdot \vec{r}_j),$$ where $\delta_{jk}$ is the Kronecker delta function, defined as $$\delta_{jk} = \begin{cases} 0\ \text{if}\ j = k \\ 1\ \text{if}\ j \neq k. \end{cases}$$
}

\ex{}{Consider the non-symmetric matrix $$M=\begin{pmatrix} 1 & 4 \\ \frac{1}{2} & 1 \end{pmatrix}.$$
	Its characteristic polynomial is $\lambda^2-2\lambda-3$, which yields the eigenvalues $\lambda=3, \lambda=-1$. 
	
	\tbf{Case 1: Right eigenvectors.} Here we have $M \cdot \vec{r}_1 = 3 \vec{r}_1$ or $M \cdot \vec{r}_2= - \vec{r}_2$ For the case $\mu=3$, we have $$\begin{pmatrix} 1 & 4 \\ \frac{1}{2} & 1 \end{pmatrix} \cdot \begin{pmatrix} r_{j1} \\ r_{j2} \end{pmatrix} = \begin{pmatrix} 3r_{j1} \\ 3r_{j2} \end{pmatrix},$$ or $\vec{r}_1 = \begin{pmatrix} 2\\1 \end{pmatrix}$. Similarly, we have that $\vec{r}_2 = \begin{pmatrix} -2 \\1 \end{pmatrix}$. 
	
	\tbf{Case 2: Left eigenvectors.} I'll skip the details, but we have $\vec{l}_1 = \begin{pmatrix} 1\\2 \end{pmatrix}$. Similarly, we have that $\vec{l}_2 = \begin{pmatrix} 1 \\-2 \end{pmatrix}.$
}

Recall that in the spectral decomposition of matrices, the following properties are satisfied (and are repeated numerous times throughout this text): \begin{align*} \bbI &= \sum\limits_{j=1}^n  \hat{m}_j \otimes \hat{m}_j  \\ M &= \sum\limits_{j=1}^n  \mu_j (\hat{m}_j \otimes \hat{m}_j) \end{align*}

We saw that the left and right eigenvectors were similar to the eigenvectors of a symmetric matrix in that the eigenvectors were orthogonal to one another. Yes, but we have to take some care.

\defn{Spectral relations for non-symmetric matrices}{
	For a set of eigenvalues $\mu_j$ of a matrix $M$ which has left and right eigenvectors $\vec{l}_j$ and $\vec{r}_j$, we have that $$\bbI = \sum\limits_{j=1}^n  \hat{r}_j \otimes \hat{l}_j$$ where $\hat{r}_j$ and $\hat{l}_j$ are the normalised eigenvectors. Furthermore, $$M = \sum\limits_{j=1}^n  \mu_j (\hat{r}_j \otimes \hat{l}_j).$$
}

\tbf{Proof:} Suppose we have a vector $\vec{b}$ represented in terms of a sum of the basis right-eigenvectors as $$\vec{b} = \sum\limits_{k=1}^n b_k \hat{r}_m.$$ Then, \begin{align*} \bbI \cdot \vec{b} &= (\sum\limits_{j=1}^n (\hat{r}_j \otimes \hat{l}_j))\cdot (\sum\limits_{m=1}^n b_m \hat{r}_m) \\ &= \sum\limits_{j=1}^n \sum\limits_{m=1}^n b_m (\hat{r}_j \otimes \hat{l}_j) \cdot \vec{r}_m \\ &= \sum\limits_{j=1}^n \sum\limits_{m=1}^n b_m \hat{r}_j (\hat{l}_j \cdot \hat{r}_m) \\ &= \sum\limits_{j=1}^n \sum\limits_{m=1}^n b_m \vec{l}_j (\hat{r}_j \cdot \hat{r}_m) \\ &=  \sum\limits_{j=1}^n  b_m \vec{l}_j \\ &= \vec{b},\end{align*}

in which the second-last line employs the fact that the dot product of two distinct normalised left or right-eigenvectors is 1.

A similar proof ensues for why $M = \sum\limits_{j=1}^n  \mu_j (\hat{r}_j \otimes \hat{l}_j).$

\coro{}{
	It is crucial to note that $\hat{r}_j \otimes \hat{l}_j \neq \hat{l}_j \otimes \hat{r}_j$. One cannot simply swap the $\hat{r}_j$ and $\hat{l}_j$ around as the tensor product is non-commutative. 
	
	Let's view this non-commutativity explicitly. Let $\vec{a} = \begin{pmatrix} a_1 \\a_2\\a_3 \end{pmatrix}$ and $\vec{b} = \begin{pmatrix} b_1 \\b_2\\b_3 \end{pmatrix}$. Then, $$\vec{a} \otimes \vec{b} = \begin{pmatrix} a_1b_1 & a_1b_2 & a_1b_3 \\ a_2b_1 & a_2b_2 & a_2b_3 \\ a_3b_1 & a_3b_2 & a_3b_3 \end{pmatrix},$$ but \begin{align*} \vec{b} \otimes \vec{a} &= \begin{pmatrix} b_1a_1 & b_1a_2 & b_1a_3 \\ b_2a_1 & b_2a_2 & b_2a_3 \\ b_3a_1 & b_3a_2 & b_3a_3 \end{pmatrix} \\ &= \begin{pmatrix} a_1b_1 & a_1b_2 & a_1b_3 \\ a_2b_1 & a_2b_2 & a_2b_3 \\ a_3b_1 & a_3b_2 & a_3b_3 \end{pmatrix}^T.  \end{align*} Therefore, we have that $$\vec{a} \otimes \vec{b} = (\vec{b}\otimes \vec{a})^T.$$
	
	This result also implies that one could write $$M = \sum\limits_{j=1}^n \mu_j (\hat{l}_j \otimes \hat{r}_j)^T.$$
}

We noticed, for symmetric matrices, that the powers of a matrix satisfied $$M^k = \sum\limits_{j=1}^n  \mu_j^k (\hat{m}_j \otimes \hat{m}_j).$$ More importantly, we could define functions on matrices as $$f(Mx) = \sum\limits_{j=1}^n  f(\mu_jx) (\hat{m}_j \otimes \hat{m}_j).$$ The matrix exponential is the most memorable. It is $$e^{Mx} = \sum\limits_{j=1}^n  e^{\mu_j x} (\hat{m}_j \otimes \hat{m}_j).$$ In Week 14 we discovered a way to repair the inverse should one of the eigenvalues be zero, meaning that $\mu_j^{-1}$ was undefined. This way was to "remove the faulty eigenvalue-eigenvector pair", or to set $\mu_j^{-1}=0$ should $\mu_j=0$. 

An alternative method to define functions on matrices would be to uncover the power series of said function. Consider the power series $$f(M)= \sum\limits_{j=0}^\infty f_j M^j$$ in which we employ the Taylor series expansion of $f$. This would then be $$f(M) =\sum\limits_{j=0}^\infty f_j \left(\sum\limits_{k=0}^n \mu_k^j (\hat{r}_k \otimes \hat{l}_k) \right).$$

\section{Left and right complex-valued eigenvectors}

If the characteristic equation of a matrix admits complex roots, then there are complex eigenvalues and potentially complex eigenvectors. A real-valued matrix admits a characteristic equation with real coefficients, which suggests that complex roots must come in conjugate pairs. Therefore, if $\lambda$ is an eigenvalue, then $\lambda^*$, its conjugate, must also be an eigenvalue. 

The treatment of right and left-eigenvalues for real-valued matrix with complex eigenvalues and eigenvectors is similar to what we've seen above.

\ex{}{
	Consider the matrix $M=\begin{pmatrix} -1 & 4 \\ -2 & 1\end{pmatrix}$. It has characteristic equation $\lambda^2+2\lambda+7$, which yields the eigenvalues $\lambda_{1, 2} = -1+\sqrt{6}i, -1-\sqrt{6}i$. 
	
	The right eigenvectors satisfy $M \cdot \vec{r} = (-1\pm \sqrt{6}i) \vec{r}$ for eigenvectors $\vec{r}$ corresponding to both eigenvalues. Explicitly, we have \begin{align*} \begin{pmatrix} -1 & 4 \\ -2 & 1\end{pmatrix} \cdot \begin{pmatrix} r_1 \\ r_2 \end{pmatrix} &= \begin{pmatrix}  (-1\pm \sqrt{6}i) r_1 \\ (-1\pm \sqrt{6}i) r_2 \end{pmatrix}  \\ \begin{pmatrix} -r_1+4r_2 \\ -2r_1+r_2 \end{pmatrix} &= \begin{pmatrix}  (-1\pm \sqrt{6}i) r_1 \\ (-1\pm \sqrt{6}i) r_2 \end{pmatrix} \\ \begin{pmatrix} \pm \sqrt{6}i r_1-4r_2 \\ 2r_1+(\pm \sqrt{6}i-2)r_2  \end{pmatrix} &= \begin{pmatrix} 0\\0 \end{pmatrix}\end{align*}
	
	This might be confusing to look at but note that the equation above splits into two cases: $\begin{pmatrix}  \sqrt{6}i r_1-4r_2 \\ 2r_1+(\sqrt{6}i-2)r_2  \end{pmatrix} = \begin{pmatrix} 0\\0 \end{pmatrix}$ for the eigenvalue $\lambda_1=-1+\sqrt{6}i$, 
	
	and $\begin{pmatrix} -\sqrt{6}i r_1-4r_2 \\ 2r_1+(-\sqrt{6}i-2)r_2  \end{pmatrix} = \begin{pmatrix} 0\\0 \end{pmatrix}$ for the eigenvalue $\lambda_2=-1-\sqrt{6}i$.
	
	Simplification yields that the right eigenvalue corresponding to $\lambda_1$ is $\vec{r}_1=\begin{pmatrix} 2i\sqrt{6} \\ 1 \end{pmatrix}$, and the one corresponding to $\lambda_2$ is $\vec{r}_2=\begin{pmatrix} -2i\sqrt{6} \\ 1 \end{pmatrix}$. 
	
	Similarly, the left eigenvalue corresponding to $\lambda_1$ is $\vec{l}_1=\begin{pmatrix} -2i\sqrt{6} \\ 1 \end{pmatrix}$, and the one corresponding to $\lambda_2$ is $\vec{l}_2=\begin{pmatrix} 2i\sqrt{6} \\ 1 \end{pmatrix}$.
} 

One can then use this to verify results regarding the spectral decomposition of the identity matrix and any arbitrary matrix. Note that the normalised eigenvectors are: 
	
	\begin{align*} \hat{r}_1&=\begin{pmatrix} \frac{2i\sqrt{6}}{5} \\ \frac{1}{5} \end{pmatrix} \\ \hat{r}_2&=\begin{pmatrix} \frac{-2i\sqrt{6}}{5} \\ \frac{1}{5} \end{pmatrix} \\ \hat{l}_1&=\begin{pmatrix} \frac{-2i\sqrt{6}}{5} \\ \frac{1}{5} \end{pmatrix} \\ \hat{l}_2&=\begin{pmatrix} \frac{2i\sqrt{6}}{5} \\ \frac{1}{5} \end{pmatrix} \end{align*} and one can recover the identities for $\bbI$ and $M$ by direct verification.
	
\coro{}{You might be tempted to make the observation that $\vec{r}_2$ is the complex conjugate of $\vec{r}_1$, and that $\vec{l}_1$ is also the complex conjugate of $\vec{r}_1$. You are right! (well, at least for real-valued matrices)

Let $a\pm bi$ be eigenvalues. Define $\lambda_+ = a+bi$ and $\lambda_- = a-bi$, and let $\hat{r}_+$, $\hat{r}_-$ be their corresponding right eigenvectors and $\hat{l}_+$, $\hat{l}_-$ be their corresponding left eigenvectors. Then, $(\hat{r}_\pm)^* = \hat{r}_\mp$ and $(\hat{l}_\pm)^* = \hat{l}_\mp$.
}

Also note that any vector $\vec{b} = \sum\limits_{j=1}^n b_{rj} \hat{r}_j$ that has complex eigenvectors $\hat{r}_j$ will return a real-valued vector $\hat{b}$. This is due to the fact that left and right eigenvectors corresponding to the same eigenvalue ($\hat{r}_j, \hat{l}_j$) are complex conjugates of each other; additionally, the eigenvectors $\hat{r}_j$ and $\hat{l}_j$ for $j=1, \dots, n$ come in conjugate pairs. 

Note that the observations above do not hold true if the matrix has complex components and our vectors are complex-valued.

Recall the Taylor series expansion $$f(M) =\sum\limits_{j=0}^\infty f_j \left(\sum\limits_{k=0}^n \mu_k^j (\hat{r}_k \otimes \hat{l}_k) \right)$$ that we use to calculate functions of matrices.

\ex{}{
	Let $$M = \begin{pmatrix} 1 & 2 \\ -4 & 3\end{pmatrix}.$$ What is $e^M$?
	
	The eigenvalues of this matrix are $\lambda_1=2+i$ and $\lambda_2=2-i$ respectively. We'll skip the calculation steps, but note that the right eigenvectors for $\lambda_1$ and $\lambda_2$ are $\begin{pmatrix} 1 \\ \frac{1+i}{2} \end{pmatrix}$ and  $\begin{pmatrix} 1 \\ \frac{1-i}{2} \end{pmatrix}$ respectively. Meanwhile, the left eigenvectors are $\begin{pmatrix} 1 \\ \frac{1-i}{2} \end{pmatrix}$ and $\begin{pmatrix} 1 \\ \frac{1+i}{2} \end{pmatrix}$ respectively. Observe that the Taylor series for $e^M$ is $$e^M = 1+M+\frac{M^2}{2}+\frac{M^3}{6}+\dots.$$
	
	Therefore the first few terms of the Taylor expansion are \begin{align*} f(M) &= \begin{pmatrix} 1 & \frac{1-i}{2} \\ \frac{1+i}{2} & \frac{1}{2} \end{pmatrix} +\begin{pmatrix} 1 & \frac{1+i}{2} \\ \frac{1-i}{2} & \frac{1}{2} \end{pmatrix} + (2+i) \begin{pmatrix} 1 & \frac{1-i}{2} \\ \frac{1+i}{2} & \frac{1}{2} \end{pmatrix} + (2-i) \begin{pmatrix} 1 & \frac{1+i}{2} \\ \frac{1-i}{2} & \frac{1}{2} \end{pmatrix} \\ &+ \frac{(2+i)^2}{2}  \begin{pmatrix} 1 & \frac{1-i}{2} \\ \frac{1+i}{2} & \frac{1}{2} \end{pmatrix} + \frac{(2-i)^2}{2}  \begin{pmatrix} 1 & \frac{1+i}{2} \\ \frac{1-i}{2} & \frac{1}{2} \end{pmatrix}\\ &+ \frac{(2+i)^3}{6} \begin{pmatrix} 1 & \frac{1-i}{2} \\ \frac{1+i}{2} & \frac{1}{2} \end{pmatrix}+ \frac{(2-i)^3}{6} \begin{pmatrix} 1 & \frac{1+i}{2} \\ \frac{1-i}{2} & \frac{1}{2} \end{pmatrix} +\dots \end{align*}
	
	which gets very messy as more terms are being introduced. Evidently, the better method is to use the fact that $$e^{M} = \sum\limits_{j=1}^n  e^{\mu_j} (\hat{m}_j \otimes \hat{m}_j).$$ In this case, this is $$e^{2+i}\begin{pmatrix} 1 & \frac{1-i}{2} \\ \frac{1+i}{2} & \frac{1}{2} \end{pmatrix} + e^{2-i}\begin{pmatrix} 1 & \frac{1+i}{2} \\ \frac{1-i}{2} & \frac{1}{2} \end{pmatrix} .$$
}

\chapter{Week 17}

This week will be dedicated towards integration in $\bbR^n$, or integration in multiple variables. In Week 5, we saw that the Gaussian integral $$I=\int_{-\infty}^\infty e^{-x^2}\ dx$$ could be solved by considering $$I^2 = \int \limits_{-\infty}^\infty \int \limits_{-\infty}^\infty e^{-(x^2+y^2)}\ dA$$ where $dA$ is an infinitesimally small surface area $dx \ dy$. Now we employ the strategic tactic by changing our coordinates from Cartesian coordinates $dx\ dy$ to \emph{polar coordinates} $dr \ d\theta$, noting the inclusion of the Jacobian determinant such that we change from $dx\ dy$ to $r\ dr \ d\theta$.

This is an example of a \tbf{volume integral} as by having one integral in the $x$-direction and one integral in the $y$-direction, we create a surface that looks like: 

\includegraphics[scale=0.3]{gaussint.jpg}

Note the legend - we "superimpose" $e^{-x^2}$ and $e^{-y^2}$ in three dimensional space such that a three-dimensional solid is created. The integral $I^2$ is therefore the \tbf{volume} between the plane $z=0$ and the solid, explaining the name volume integral.

In this week, we will encounter three types of integrals. These are line, volume, and surface integrals. The ideas behind these three types of integrals are conceptually challenging but turn out to be very similar to the one-variable integration that we encountered before.

\section{Line integrals}

In the integration of a singular variable, we compute definite integrals by integrating over an interval $[a, b]$. In line integration, we integrate a function along a curve $C$. When performing $\int_b^a f(x) \ dx$, for example, we integrate a function over the interval $[x=a, x=b]$, with $x$ as our input. 

In line integrals, we integrate a function $f(x, y)$ of two variables, with $x$ and $y$ being inputs of coordinates on the curve $C$. Though instead of catering to both $x$ and $y$ simultaneously, which may cause some practical difficulty, we use parametrisation to compute integrals evaluated on a curve. 

By parametrising the curve $C$ as $$\vec{r}(t) = g(t) \hat{i}+h(t) \hat{j}$$ given a specified domain $t \in [a, b]$, we can find the distance between a certain two-variable function $f(x, y)$ and a curve $C$.

\defn{Curve}{
	A curve $C$ is a map $$\vec{r}: [0,1] \to \bbR^3$$ which underscores the parametrisation of a function $f(x, y)$. Explicitly, we have that the parametrisation is given by $$\vec{r}(t) = (x(t), y(t), z(t))$$ for $0\leq1$.
	
	(n.b.) We can use other intervals as our domain. Usually, for simplicity's sake, we use the unit-interval $[0,1]$. This means of parametrisation is also known as \tbf{arc-length parametrisation}; we will explore this later.
}

\ex{Examples of curves}{
	Here are some examples of curves we can integrate upon, and their respective parameterisations.
	
	\begin{enumerate}
		\item An ellipse has the general equation $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$. It can be parameterised as $x=a\cos t, y=b \sin t$ for $0 \leq t \leq 2\pi$ (where we fix a value of $z$) in the \tbf{counter-clockwise direction}, and $x=a\cos t, y=-b \sin t$ in the \tbf{clockwise direction}. We differentiate between "tracing out the curve" in these two separate directions as the sign of the integral changes when you change the orientation. 
		
		We can normalise the parametrisation (similar to how we normalised vectors earlier in this course) to obtain a function from $[0,1] \to\bbR^2$.
		\item A line segment from $(x_0, y_0, z_0)$ to $(x_1, y_1, z_1)$ can be parametrised as \begin{align*} x&=(1-t)x_0+tx_1 \\ y&=(1-t)y_0+ty_1 \\ z&=(1-t)z_0+tz_1 \end{align*} for some $0\leq t \leq 1$. In short, this is $$\vec{x} = (1-t) \vec{x}_0 + t\vec{x}_1.$$
		\item A circle has the equation $x^2+y^2=r^2$ where $r$ is the radius of the circle. In the counter-clockwise direction, the parameterisation is $x=r\cos t, y=r \sin t$; in the clockwise direction, the parameterisation is $x = r\cos t, y=-r\sin t$.
	\end{enumerate}
}

The parameterisation of a line segment can be defined differently. Note that if we have $t \in [0, t_f]$ instead, we can parameterise the curve as $$\vec{x}(t) = t \vec{v}_0 + \vec{x}_0$$ where $\vec{x}_0$ is the starting point $(x_0, y_0, z_0)$ and $\vec{v}_0$ is the direction vector $\vec{v}(s) = \frac{d}{ds} \vec{x}_s$ which indicates the direction of the line. 

If instead we considered the arc-length parameterisation, we would have $$\vec{x}(t) = t \hat{v}_0 + \vec{x}_0$$ where $t \in [0, 1]$ where $\hat{v}_0$ is the normalised direction vector. 

Since $\vec{v}(s)=\vec{v}_0$ is constant, this indicates that the velocity vector is constant (as expected for a straight line), adhering to a straight-line path.

\defn{Line integral}{
	The line integral of a two-variable function $f(x, y)$ over a curve $C$ is given by $$\int_C f(x, y)\ ds,$$ where $ds$ denotes that we are integrating along the curve $C$ instead of integrating along the $x$ or $y$-axis.
}

\coro{}{
	Our ordinary integral in one dimension $$\int_a^b f(x)\ dx$$ has the curve $C$ as the $x$-axis. Therefore, $ds=dx$. This means that we've seen line integrals before - just a very simplistic type of line integral.
}

The simplest form of line integration is to evaluate the length of a curve given a certain start or endpoint. You might have learned in secondary school that the arc length of a curve $f(x)$ in one dimension is given by $$L = \int_a^b \ dS,$$ where $$dS = \sqrt{1+ \left(\frac{dy}{dx}\right)^2}.$$ 

\coro{Regular parameterisation v.s. arc-length parameterisation}{
	Recall that in arc-length parameterisation, we parameterise a curve such that when one integrates upon said curve, they integrate from $t=0$ to $t=1$. In regular parameterisation, we integrate from $0$ to a final point $t_f$ whereas in arc-length parameterisation, we integrate from $0$ to $1$.
	
	What differentiates the two types of parameterisation? When we parameterise by arc length, the infinitesimal arc-length distance $dl$ and the infinitesimal distance in our original parameterisation $ds$ are linked by $$dl = \sqrt{\vec{v}_0^2}\ ds.$$
	
	Don't overthink this. This is merely normalisation. This just seems conceptually unfamiliar in our new multivariable integration context.
}

\ex{}{
	Let's find the arc length of $f(x) = \sin x$ from $x=0$ to $x=2$. To do this, we have that $$L = \int_0^2 \sqrt{1 + \cos^2x}\ dx.$$
}

\thrm{Arc length of a curve}{
	To calculate the arc length of a curve parameterised by $t$, we simply evaluate $$\int_C f(x, y)\ ds = \int_a^b f(g(t), h(t)) \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2}.$$ If we let $$\sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2}=||\vec{r}'(t)||\ dt,$$ then we simply have that $$\int_C f(x, y)\ ds = \int_a^b f(g(t), h(t)) ||\vec{r}'(t)||\ dt.$$
}

\subsection{Vector fields, gradients, path independence}

Let's say that a group of people started off with zero assets at time $t=0$ (meaning $\vec{x}(0)=0$) and achieved, after some time $t=T$, a utility level equal to $\vec{x}(T)$. With their choices to expand their consumption bundle over time, they face a bundle of marginal utilities $\vec{\mu}(\vec{x}(t))$ such that $$dU = \vec{\mu}(\vec{x}(t)) \cdot \left(\frac{d}{dt} \vec{x}(t)\right)\ dt.$$ This is a classic dynamic optimisation problem where the objective would be to maximise $$U(C) = \int_C \vec{\mu}(\vec{x})\ dx$$ with respect to $C$, where $C$ is the curve from which this integral is evaluated upon. 

Though does the path that they chose to achieve the bundle $\vec{x}(t)$ matter? Can't we just index each $\vec{x}(t)$ for $t =0, ..., T$ with a utility function?

\thrm{}{
	If the \tbf{Jacobian of the vector field is symmetric}, then the underlying vector field $\vec{\mu}(\vec{x}(t))$ is a \emph{gradient}, and we can index each $\vec{x}(t)$ with a utility function, suggesting that the path of increasing assets does not matter.

	Let $J_\mu$ be the Jacobian of the vector field $\mu{\vec{x}}$. Essentially, we want $$J_\mu - J_\mu^T=\mbf{0}$$ where $\mbf{0}$ is the zero matrix.
}

\coro{}{A vector field which arises from the gradient of some function is known as a \emph{conservative vector field}. Conservative vector fields, as discussed just now, are path-independent.

When the vector field is conservative and the integration path is closed (meaning that the start and end points are the same), the integral along the loop is zero.

An intuitive explanation is that since the integral only \tbf{depends on the start and end points} for conservative vector fields and not the specific path taken, there is a net change of zero when we integrate along a closed loop. 

When we perform a line integral over a closed loop of a conservative vector field, we use the notation $\oint$ instead of $\int_C$. 
}

Before we move on to concrete examples, I'd like to introduce a formal piece of rationale behind this theory - the Fundamental Theorem of Line Integrals.

\thrm{Fundamental Theorem of Line Integrals}{
	Let a curve $C$ be defined by the vector function $\vec{r}(t)$. Assume that the curve starts at a point $\vec{a}=\vec{r}(a)$ and ends at a point $\vec{b}=\vec{r}(b)$. Suppose that the vector field arises from the gradient of a function $f$; this means that the vector field is conservative, and is path-independent. 
	
	The theorem says that $$\int_C \nabla\ f \cdot d\vec{r} =f(\vec{b})-f(\vec{a}).$$
}

\subsection{Line integrals of functions}

\ex{Integration along a line}{
	Consider the integral $$\int_C x^2y^2\ ds$$ evaluated along the line segment $(-1, 0)$ to $(1, 4)$. 
	
	First we parameterise the line segment. We know that $$\vec{r}(t) = (1-t) \begin{pmatrix} -1 \\ 0 \end{pmatrix} + t  \begin{pmatrix} 1 \\ 4 \end{pmatrix},$$ or $$\vec{r}(t) = \begin{pmatrix} 2t-1 \\ 4t \end{pmatrix} $$ for $0\leq t \leq 1.$ Here, the individual parametric equations are $x=2t-1$ and $y=4t$, so the integral now becomes $$\int_C x^2y^2\ ds = \int_0^1 (2t-1)^2 (4t)^2 \sqrt{2^2+4^2}\ dt.$$
	
	Notice how our parameterisation from $t=0$ to $t=1$ translates to the bounds of integration. This parameterisation has effectively enabled us to evaluate an ordinary integral in one dimension. The factor $\sqrt{2^2+4^2}$ is a scaling factor that changes according to the bounds of parameterisation. 
}

\ex{Integration along a curve}{
	Consider again the integral $$\int_C x^2y^2\ ds$$ evaluated along the curve $C:y=x^4-1$ where $0\leq x \leq 1$.
	
	To parameterise $C$, we simply use the parameterisation $x=t, y=t^4-1$. Since $x=t$, our bounds of integration are $0\leq t\leq 1$. The integral effectively becomes $$\int_0^1 t^2 (t^4-1)^2 \sqrt{1^2+(4t^3)^2}\ dt.$$
}

\ex{}{
	Let's consider an integral where the curve to be evaluated upon is a circle. Therefore, we must take caution in distinguishing between integration in the clockwise and integration in the anti-clockwise direction. 
	
	Consider the integral $$\int_C x^2+y^2x\ ds$$ evaluated along the curve $C:x^2+y^2=9$ in the anti-clockwise direction.
	
	We begin by parameterising the circle. This is $x=3\cos t, y = 3 \sin t$. Notice that to trace out the whole circle, we need to integrate from 0 to $2\pi$. Therefore, our integral is $$\int_0^{2\pi}(4\cos t)^2 + (4 \sin t)^2(4\cos t) \sqrt{(-3\sin t)^2 + (3\cos t)^2}\ dt,$$ which simplifies to $$9\int_0^{2\pi}(4\cos t)^2 + (4 \sin t)^2(4\cos t)\ dt.$$
}

\subsection{Line integrals of vector fields}

\thrm{}{
	Let $\vec{F}$ be a vector field evaluated upon a smooth curve $C:\vec{r}(t)$ where $t$ is parameterised in the range $[a, b]$. The line integral of $\vec{F}$ along the curve $C$ is $$\int_C \vec{F} \ d\vec{r} = \int_a^b \vec{F}(\vec{r}(t))\cdot \vec{r}\ '(t)\ dt.$$
}

This concept is confusing but works much better with ample examples. Though we can prove the Fundamental Theorem of Line Integrals from this. Recall that in our statement of the Fundamental Theorem, we have a curve that starts from $\vec{a}=\vec{r}(a)$ and has endpoint $\vec{b}=\vec{r}(b)$.

Observe that \begin{align*} \int_C \nabla\ f(\vec{r}) \ d\vec{r} &= \int_a^b \nabla\ f(\vec{r}) \cdot \vec{r}\ '(t)\ dt \\ &= \int_a^b \frac{d}{dt} f(\vec{r}(t))\ dt \\ &= f(\vec{r}(b)) -  f(\vec{r}(a)) \\ &= f(\vec{b})-f(\vec{a}), \end{align*}

which proves the statement. Note that after using the definition of a line integral, we used the chain rule to prepare ourselves to be in a position where we could employ the single-variable Fundamental Theorem of Calculus in line 3.

Let's work with a two-dimensional case before working with a three-dimensional case. 

\ex{Two-dimensional case}{
	Consider the two-dimensional vector field $$\vec{F}(x, y) = \begin{pmatrix} \frac{x}{\sqrt{y}} \\ \frac{y}{\sqrt{x}} \end{pmatrix}$$ evaluated upon the line segment from $(-2, 4)$ to $(-1, 2)$. 
	
	We first begin by parameterising the line segment. This is $$(1-t) \begin{pmatrix} -2 \\ 4 \end{pmatrix} + t \begin{pmatrix} -1 \\ 2 \end{pmatrix}=\begin{pmatrix} t-2 \\ 4-2t \end{pmatrix}.$$
	
	Therefore, the vector field evaluated along the curve is $$\vec{F}(\vec{r}(t))= \begin{pmatrix} \frac{t-2}{\sqrt{4-2t}} \\ \frac{4-2t}{\sqrt{t-2}} \end{pmatrix}$$ and thus \begin{align*} \int_C \vec{F}\ dr &= \int_0^1 \begin{pmatrix} \frac{t-2}{\sqrt{4-2t}} \\ \frac{4-2t}{\sqrt{t-2}} \end{pmatrix} \cdot \begin{pmatrix} 1 \\ -2 \end{pmatrix} \ dt \\ &= \int_0^1 \frac{t-2}{\sqrt{4-2t}}- \frac{8-4t}{\sqrt{t-2}} \ dt.\end{align*}
}	

\ex{Three-dimensional case}{
	Now consider the three-dimensional vector field $$\vec{F}(x, y, z) = 4x^2 \hat{i}+3yz^2 \hat{j} + 2z \hat{k}.$$ Let the vector field be evaluated along the curve $C:\vec{r}(t) = t^2 \hat{i} + 2t \hat{j} + t^3 \hat{k}$ for $0\leq t \leq 1$. The parameterisation is $$\vec{F}(\vec{r}(t)) \cdot \vec{r}\ '(t)= (4t^4 \hat{i} + 6t^7 \hat{j} + 2t^3 \hat{k}) \cdot (2t \hat{i}+2\hat{j} + 3t^2 \hat{k}) = 8t^5+12t^7+6t^5=14t^5+12t^7.$$ Therefore, the line integral is $$\int_C \vec{F} \cdot d\vec{r} = \int_0^1 14t^5+12t^7\ dt.$$
}

Recall in our previous discussion of path independence that the integral does not depend on the path if the Jacobian of the vector is symmetric. This means that insofar as conservative vector fields are at play, we can \tbf{compute the line integral without even computing the integral itself.}

\ex{}{
	Consider the vector field $\vec{F}(x, y) = \begin{pmatrix} \frac{y}{\sqrt{xy}} \\ \frac{x}{\sqrt{xy}} \end{pmatrix}$. The Jacobian of this vector field is \begin{align*} J_{\vec{F}} &= \begin{pmatrix} \frac{\partial}{\partial x} \frac{y}{\sqrt{xy}} & \frac{\partial}{\partial x} \frac{x}{\sqrt{xy}} \\ \frac{\partial}{\partial y} \frac{y}{\sqrt{xy}} & \frac{\partial}{\partial y} \frac{x}{\sqrt{xy}} \end{pmatrix} \\ &=  \begin{pmatrix} -\frac{y}{4\sqrt{x^3y}} & \frac{1}{4\sqrt{xy}} \\ \frac{1}{4\sqrt{xy}} & -\frac{x}{4\sqrt{xy^3}} \end{pmatrix} \end{align*} 
	
	which is a gradient. Therefore, \begin{align*} \int_C \vec{F}(\vec{x})\ d\vec{r} &= \int_0^1 \vec{F}(\vec{x}) \cdot \frac{d}{dt} \vec{x}(t)\ dt \\ &=  \int_0^1 \frac{1}{\sqrt{xy}} \begin{pmatrix} y\\ x \end{pmatrix} \cdot \begin{pmatrix} \frac{dx}{dt} \\ \frac{dy}{dt} \end{pmatrix}\ dt \\ &= \int_0^1 \frac{1}{\sqrt{xy}} y\cdot \frac{dx}{dt} + x \cdot \frac{dy}{dt}\ dt \\ &= \int_0^1 \left(\frac{y}{\sqrt{xy}} \frac{d}{dt} x(t) + \frac{x}{\sqrt{xy}} \frac{d}{dt} y(t)  \right)\ dt   \end{align*}
	
	Here we use the fact that the Jacobian is symmetric as the vector field is a gradient. From this, note that \begin{align*} \int_0^1 \left(\frac{y}{\sqrt{xy}} \frac{d}{dt} x(t) + \frac{x}{\sqrt{xy}} \frac{d}{dt} y(t)  \right)\ dt &= \int_0^1 2\frac{\partial}{\partial x} \sqrt{xy} \frac{dx}{dt} + 2\frac{\partial}{\partial y} \sqrt{xy} \frac{dy}{dt}\ dt \\ &= \int_0^1 2\frac{d}{dt} \sqrt{xy}\ dt \end{align*}
	
	which equals $\sqrt{x(1)y(1)}-\sqrt{x(0)y(0)}$ by the Fundamental Theorem of Calculus. This suggests that in the gradient case, we were able to compute the line integral without computing the integral. Notice that due to this path independence, we did not specify what path we were integrating upon.
}

There are some \emph{distinct} curves that, when integrated upon, yield the same result. We just have to ensure that the value $$\frac{y}{\sqrt{xy}} \frac{d}{dt} x(t) + \frac{x}{\sqrt{xy}} \frac{d}{dt} y(t)$$ is invariant under integration over the curves $C_1$ and $C_2$. 

\section{Volume integrals}

We've seen volume integrals in the evaluation of the Gaussian integral, where we evaluated an integral over $dx\ dy$. In a sense, if we integrate over a subset $\mcR \subset \bbR^n$ as $$\int_\mcR f(\vec{x}) dx_1 \ \dots \ dx_n,$$ we split up the function into infinitesimally small parts $dx_1 \ \dots\ dx_n$ like we do with normal single-variable integration. 

How do we interpret the $dx_1 \ \dots\ dx_n$? Consider the simpler case $dx\ dy$ like the one we encountered in the Gaussian integral. Note that $dx\ dy$ describes the volume of an infinitesimally small cube with magnitude $\det(V)$, where $$V = \begin{pmatrix} dx & 0 \\ 0 & dy \end{pmatrix}$$ and evidently $\det(V) = dx dy$. 

It is relatively simple, empirically, to compute volume integrals. Or at least when compared to single-variable integration. For example, if we wanted to compute the volume enclosed by the function $f(x, y) = 2x^2+xy$ within the region $\mcR = \{x, y | x \in [-1, 1], y \in [-1, 1]\}$, we would write $$\int_\mcR f(x, y) dx\ dy = \int_{-1}^1 \int_{-1}^1  2x^2+xy\ dx\ dy.$$

Like in partial differentiation, one would evaluate $\int_{-1}^1  2x^2+xy\ dx$ first, treating $y$ like a constant, then evaluating the integral with respect to $y$. The inner integral evaluates to $\frac{4}{3}$; thus, we have $\int_{-1}^1 \frac{4}{3}\ dy=\frac{8}{3}.$

\subsection{Changing coordinates}

We saw in Week 5 that we could utilise a change of coordinates and the Jacobian matrix to evaluate \emph{non-elementary integrals} such as $\int_{-\infty}^\infty e^{-x^2}$. We can also utilise the Jacobian matrix and a change of coordinates to evaluate other types of integrals. First recall that the change between Cartesian coordinates and Polar coordinates is given by \begin{align*} x &= r \cos \theta \\ y &= r \sin \theta, \end{align*} and the Jacobian determinant is given by $\det\begin{pmatrix} \cos \theta & -r\sin \theta \\ \sin \theta & r\cos\theta \end{pmatrix}=r\cos^2\theta-(-r\sin^2\theta) = r.$

\ex{}{
	Consider the integral $$I=\int_D \frac{1}{\sqrt{x^2+y^2}} dx\ dy$$ where $D$ is the disk $x^2+y^2 \leq r^2$. The coordinate change is easy. We have that $$I= \int_0^a \int_0^{2\pi} \frac{1}{r}\cdot r\ dr\ d\theta = \int_0^a \int_0^{2\pi}\ dr\ d\theta=2\pi a.$$
	
	The key is to note that the bounds of integration for $\theta$ is $\theta \in [0, 2\pi]$. 
}

\coro{}{
	A way to interpret the factor $r$ when changing from the Cartesian coordinates $dx\ dy$ to the polar coordinates $dr \ d\theta$ is the fact that infinitesimal changes to the length and width ($dx$ and $dy$) correspond to infinitesimal changes to the modulus $dr$ and argument $d\theta$ - though by making this change, the area increases by a factor of $r$.
}

Are there coordinate changes in $\bbR^3$? Yes - the spherical coordinates and cylindrical coordinates are the most well-known ones. 

\defn{Cylindrical coordinates}{
	Cylindrical coordinates are an extension of polar coordinates in three dimensions. The conversion is \begin{align*} x &= r\cos\theta \\ y &= r \sin\theta \\ z&=z. \end{align*} Instead of the conversion $dA =  r\ dr\ d\theta$ seen in polar coordinates, the conversion is now $dV =r\  dr\ d\theta\ dz$.
	
	Observe that the Jacobian of this transformation is $r$.
	
	We need great care in determining the region over which we integrate. The key here is to note that $$\int\int\int f(x, y, z)\ dV = \int_a^b \int_{f_1(\theta)}^{f_2(\theta)} \int_{g_1(r \cos \theta, r\sin\theta)}^{g_2(r \cos\theta, r\sin\theta)} r f(r\cos\theta, r\sin\theta, z)\ dz\ dr\ d\theta.$$	
	
	Notice that by performing this conversion, we represent $r$ in terms of $\theta$ and $z$ in terms of both $r$ and $\theta$.
}

\ex{Cylindrical coordinates}{
	Let $f(x, y, z) = 2x+yz^2$. What is the integral of $f$ between the region $z=x^2+y^2-5$ and $z=1$?
	
	We first observe that the limits for $z$ are $x^2+y^2-5\leq z \leq 1$, or $x^2+y^2 \leq 6$. Thus, our bounds of integration are: \begin{align*} 0 \leq \theta \leq 2\pi \\ 0 \leq r \leq \sqrt{6} \\ r^2-5\leq z \leq 1\end{align*}
	
	Thus, our integral becomes $$\int_0^{2\pi} \int_0^{\sqrt{6}} \int_{r^2-5}^1 2r\cos\theta+ r\sin\theta z^2 \ dz\  dr\ d\theta.$$
	
	You'll notice that the most conceptually tricky part is to evaluate the bounds of integration. The rest is rather methodical.
}

\defn{Spherical coordinates}{
	Spherical coordinates are a triple $(\rho, \theta, \varphi)$ that are transformed from the regular Cartesian coordinates $(x, y, z)$. They are conceptually more tricky than the cylindrical coordinates.
	\begin{itemize}
		\item The value $\rho$ merely measures the distance from the origin to the point, similar to the function of $r$ in 	polar and cylindrical coordinates. Since it is a measure of distance, it satisfies the identity $$\rho^2=x^2+y^2+z^2.$$
		\item The value $\theta$ is identical in function to $\theta$ in polar and cylindrical coordinates, or the modulus. It measures the angle between the positive $x$-axis and the line $r$, where $r$ is the projection of the line $\rho$ onto the $x$-$y$ axis.
		\item The value $\varphi$ is the angle between the positive $z$-axis and the line $\rho$. Note that by defnition, $0\leq \varphi \leq \pi$, while the value of $\theta$ has no bounds but is typically set to be $0\leq \theta \leq 2\pi$. 
	\end{itemize}
	
	The conversion between three-dimensional Cartesian coordinates and spherical coordinates are as follows:
	\begin{align*} x &= \rho \sin\varphi\cos\theta \\ y &= \rho \sin\varphi\sin\theta \\ z&=\rho \cos\varphi \end{align*} This variable change has Jacobian determinant $\rho^2 \sin\varphi$. 
}

\ex{Volume of sphere}{
	I will now use volume integrals to prove that the volume of a sphere with radius $R$ is $V=\frac{4}{3}\pi R^3$. 
	
	First, we observe the bounds of integration. We have $\rho \in [0, R]$, $\theta \in [0, 2\pi]$, and $\varphi = [0, \pi]$. So, the volume is \begin{align*} \iiint 1 \ dV  &= \int_0^R \int_0^{2\pi} \int_0^{\pi} r^2 \sin (\varphi)\ d\varphi\ d\theta\ dr \\ &= \int_0^R \int_0^{2\pi} r^2 (-\cos(\pi)+\cos(0))\ d\theta\ dr \\ &= \int_0^R \int_0^{2\pi} 2r^2 \ d\theta\ dr \\ &= \int_0^R 4\pi r^2 \ dr \\ &= \frac{4}{3} \pi R^3.\end{align*}
}

\coro{Calculation tip}{
	Instead of peeling the triple integral like an onion - solving the inner integral before moving to the outer ones, we can expedite the integration process for some integrals by observing that $$\int_0^R \int_0^{2\pi} \int_0^{\pi} r^2 \sin (\varphi)\ d\varphi\ d\theta\ dr$$ is actually $$\int_0^R r^2\ dr \int_0^{2\pi} \ d\theta \int_0^{\pi} \sin (\varphi)\ d\varphi.$$
	
	Though one must take caution before they split the integrals up as such. A double integral such as $$\int_0^1 \int_0^{\cos\theta} \frac{r^2+1}{r^3-1} \ dr \ d\theta$$ is not the same as $$\int_0^{\cos\theta} \frac{r^2+1}{r^3-1} \ dr \int_0^1\ d\theta.$$
}

\section{Surface integrals}

Before we discuss surface integrals, we have to explore how to parameterise surfaces. Recall that when we parameterised a curve, we represented $$\vec{r}(t) = x(t) \hat{i} + y(t) \hat{j} + z(t) \hat{k},$$ or $$\vec{r}(t) = \begin{pmatrix} x(t) \\ y(t) \\ z(t) \end{pmatrix}.$$ When we parameterise surfaces, we instead write $$\vec{r}(u, v) = x(u, v) \hat{i} + y(u, v) \hat{j} + z(u, v) \hat{k}.$$

For example, the surface $x^2+z^2=16$ can be parameterised by \begin{align*} x &= 4\cos\theta \\ y&= y \\ z&=4 \sin\theta \end{align*} where $\theta \in [0, 2\pi]$. 

The underlying theory for a surface integral is easy. Suppose that $dS$ is comprised of $\vec{du}, \vec{dv}$. At each point $(u, v)$, the infinitesimal vectors $\vec{du}=\frac{d\vec{x}}{du}$ and $\vec{dv}=\frac{d\vec{x}}{dv}$ span the parallelogram $$dS= \det(J) du\ dv \cdot \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix},$$ where $J$ is the Jacobian of $(x, y)$ with respect to $(u, v).$

This should remind you of the means of evaluating a volume integral. But why the matrix $\begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}$? Notice that if we consider two-dimensional figures in $\bbR^n$ for $n\geq 3$, the \emph{orientation} of said two-dimensional figure matters. For example, if we have vectors $\vec{a}$ and $\vec{b}$ that span a parallelogram, it is important whether to write the determinant as $$\det\begin{pmatrix} a_1 & a_2 \\ b_1 & b_2 \end{pmatrix}$$ or $$\det\begin{pmatrix} b_1 & b_2 \\ a_1 & a_2 \end{pmatrix},$$ as they return $a_1b_2-a_2b_1$ and $a_2b_1-a_1b_2$ respectively.

To preserve the orientation of surfaces, we observe that the surface can be written as $\vec{a} \otimes \vec{b} - \vec{b} \otimes \vec{a} = (a_1b_2-a_2b_1) \begin{pmatrix} 0&1 \\-1&0 \end{pmatrix}.$

Since there is only one orientation of a 2-dimensional surface in $\bbR^2$, we can ignore the matrix $ \begin{pmatrix} 0&1 \\-1&0 \end{pmatrix}$ in $\bbR^2$. Though in future, our means of integration should account for this orientation change.

Therefore, a change of variables $$\iint_D f(x, y) \ dx\ dy = \iint f(x(u, v), y(u, v)) \cdot |J|\ du\ dv$$ where $J$ is the Jacobian matrix $$J= \begin{pmatrix}\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{pmatrix}.$$

\thrm{}{
	Given a parameterisation $\vec{r}(u, v) = x(u, v) \hat{i} + y(u, v) \hat{j} + z(u, v) \hat{k},$, we have that $$\iint f(x,y, z)\ dS = \iint f(\vec{r}(u, v)) ||\vec{r}_u \times \vec{r}_v ||\ dA$$
	
	In the case that the curve is written as $z=f(x, y)$, we can write $$\iint f(x,y, z)\ dS = \iint f(\vec{r}(u, v)) \sqrt{\left(\frac{\partial g}{\partial x} \right)^2+\left(\frac{\partial g}{\partial y} \right)^2 +1}dA.$$

}

\ex{Surface area of a sphere}{
	Recall that the conversion between Cartesian coordinates in $\bbR^3$ and spherical coordinates is given by \begin{align*} x &= \rho \sin\varphi\cos\theta \\ y &= \rho \sin\varphi\sin\theta \\ z&=\rho \cos\varphi. \end{align*}
	
	Therefore, the parameterised surface $x(u, v) \hat{i} + y(u, v) \hat{j} + z(u, v) \hat{k}$ becomes $$\vec{r}(\theta, \varphi) = \rho \sin\varphi\cos\theta \hat{i} +\rho \sin\varphi\sin\theta \hat{j} +  \rho \cos\varphi \hat{k}.$$ The first-order conditions for $\vec{r}$ are \begin{align*} \frac{\partial\vec{r}}{\partial \varphi} &= \rho \cos\varphi \cos\theta \hat{i} + \rho\cos\varphi\sin\theta \hat{j} -\rho \sin\varphi \hat{k} \\ \frac{\partial\vec{r}}{\partial \theta} &= -\rho\sin\varphi\sin\theta \hat{i} + \rho\sin\varphi\cos\theta \hat{j} \end{align*}
	
	The cross product of these two first-order conditions is \begin{align*}\left\|\frac{\partial\vec{r}}{\partial \varphi} \times \frac{\partial\vec{r}}{\partial \rho}\right\| &=  \det \begin{pmatrix} \hat{i} & \hat{j} & \hat{k} \\ \rho\cos\varphi \cos\theta & \rho\cos\varphi\sin\theta & -\rho\sin\varphi \\ -\rho\sin\varphi\sin\theta & \rho\sin\varphi\cos\theta & 0 \end{pmatrix} \\ &= \rho^2\sin\varphi(\sin^2 \varphi cos^2\theta +\sin^2\varphi \sin^2 \theta+\cos^2 \varphi)^{1/2}\end{align*} which simplifies to $\rho^2 \sin\varphi$, something you've seen before in our previous section on spherical coordinates. Therefore, $$dS = \rho^2\sin\varphi d\varphi\ d\theta.$$ The sphere merely has area \begin{align*}  \iint_S 1 \ dS &=\int_0^{2\pi} \int_0^\pi  \rho^2\sin\varphi d\varphi\ d\theta \\ &= \rho^2 \int_0^{2\pi} d\theta \int_0^\pi \sin\varphi d\varphi \\ &= 4\pi \rho^2.\end{align*} 
}

The processes of computing, volume, and surface integrals are rather methodical in simple cases. They involve procuring the right parameterisation, finding the Jacobian determinant if a coordinate change is necessary, and finding appropriate boundary conditions. This then evolves to a mere calculation of a multiple integral - the mechanics of which you have learned in high school. 

I will briefly discuss the computation of surface integrals in $n$ dimensions.

In $n$ dimensions, a change of coordinates is described as $$d^nS = dx_1\ \dots\ dx_n = \left(\frac{\partial}{\partial u} \vec{x}(u, v) \otimes \frac{\partial}{\partial v} \vec{x}(u, v)\right)-\left(\frac{\partial}{\partial v} \vec{x}(u, v) \otimes \frac{\partial}{\partial u} \vec{x}(u, v)\right).$$ Do not be confused by the tensor product! It is simply a wider application of $\vec{a}\otimes \vec{b}-\vec{b}\otimes\vec{a}$ that we saw in the two-dimensional case.

Recall that if the two-dimensional surface is parameterised by $\vec{r}(u, v)$, then we can use \begin{align*} dS &= \left\| \frac{\partial \vec{r}}{\partial u} \times \frac{\partial \vec{r}}{\partial v} \right\| \ du\ dv  \\ &= \left\| \det \begin{pmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{pmatrix}\right\| \ du\ dv\end{align*}

Let $M = \begin{pmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{pmatrix}$.

with an integral $$\int_S f\ dS = f(\vec{r}(u, v))\cdot \sqrt{\det(M(u, v))}\ du\ dv.$$

But suppose that $S \subset \bbR^4$. This integral would then be $$\int_S f\ dS = f(\vec{r}(u, v, w))\cdot \sqrt{\det(M(u, v, w))}\ du\ dv\ dw$$ where in this case, $M = \begin{pmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} & \frac{\partial x}{\partial w} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} & \frac{\partial y}{\partial w} \\ \frac{\partial z}{\partial u} & \frac{\partial z}{\partial v} & \frac{\partial z}{\partial w} \end{pmatrix}$,

and one can simply involve more variables when we generalise to $n$ dimensions.

\chapter{Week 18}

We discuss the Fourier transform and its relation to function spaces. After discussing applications of the Fourier transform, we take a more pure maths-esque detour to discuss the mathematical context by which the Fourier transform arises.

\section{Eigenfunctions of the derivative operator}

Recall from Week 13 that we can view the derivative operator as a linear map as $\frac{d}{dx} f(x)+g(x) = \frac{d}{dx} f(x) + \frac{d}{dx} g(x)$, and $\frac{d}{dx} kf(x) = k\frac{d}{dx} f(x).$ 

If you're not convinced as to how this linear map arises in practice, consider the case where the function space is the space of all quadratic functions. We covered this in Week 13. 

Consider the function space of all infinitely-differentiable functions from $\bbR$ to $\bbR$ with period $T$. If we define $$f \cdot g = \int_0^T f(x)g(x)\ dx$$ for all functions $f, g$, then \begin{align*} g \cdot \frac{d}{dx} f &= \int_0^T g(x) \frac{df}{dx} \ dx \\ &= g(T)f(T)-g(0)f(0) - \int_0^T f(x) \frac{dg}{dx}\ dx \\ &=- \int_0^T f(x) \frac{dg}{dx}\ dx \\ &= -(\frac{d}{dx}g) \cdot f, \end{align*}

where we used the fact that due to periodicity, $g(T)f(T)-g(0)f(0)=0$. We can also relinquish the periodicity condition and use an integral from $-\infty$ to $\infty$ instead, employing the fact that $g(-\infty)=g(\infty)=f(-\infty)=f(\infty)=0$. 

Note that this implies that the \tbf{differential operator is antisymmetric}, meaning that the differential operator $D$ satisfies $$g \cdot D(f)  = -D(g) \cdot f.$$

But let us take a detour back to Week 13, where we discussed eigenfunctions of the differential operator. Recall that the eigenvalue problem for the differential operator is given by $$\frac{dy}{dx} = \lambda y.$$ This is a first-order differential equation which solutions are the exponential functions $y=ce^{\lambda x}$ that we know and love. 

Since we are working with non-symmetric matrices, we have to deal with the right-eigenfunction and left-eigenfunction case as instantiated in Week 16. Simply put, if we let $\lambda=ik$, then the eigenvalue equation for right eigenfunctions $$\frac{d}{dx} r_k(x) = ik r_k(x)$$ implies that $r_k(x) = r_k(0) e^{ikx}$. Similarly, the eigenvalue equation for left eigenfunctions $$l_k(x) \frac{d}{dx}= ik l_k(x)$$ implies that $$-\frac{d}{dx} l_k(x) = ik l_k(x),$$ which yields the solution $l_k(x) = l_k(0) e^{-ikx}$. 

This is something similar to what we've seen before! In the end of Week 16's content, we learned that the eigenvalues of the left and right eigenfunctions are conjugates of each other. Here we see that the right and left-eigenfunctions have the eigenvalues $ik$ and $-ik$ respectively. 

\section{The Fourier transform}

Why did we let $\lambda=ik$? This seems pretty arbitrary. These left and right eigenfunctions are the \emph{basis functions} for the Fourier transform. The Fourier transform expresses a function as a linear combination of $e^{ikx}$ basis functions. Differentiating this linear combination multiplies each term by its eigenvalue $ik$.

(n.b.) For the purposes of this course, we only work with continuous Fourier transforms. Therefore, the Fourier transform is a continuous summation of the $e^{ikx}$ basis functions.

\defn{Fourier transform}{
	Let $f(x)$ be smooth and infinitely differentiable. Then, the Fourier transform $\mcF(f(x))$ is defined as $$\mcF(f) = \int_{-\infty}^\infty f(x) e^{-ikx}\ dx.$$
	
	While $\mcF$ is the operator that produces said transform, we generally refer to $\mcF(f(x))$ as $\hat{f}(k)$ as we make it clear that the operator $\mcF$ makes $\hat{f}$ a function in $k$. So, $$\hat{f}(k)= \int_{-\infty}^\infty f(x) e^{-ikx}\ dx.$$
}

\coro{Existence of the Fourier transform}{
	The Fourier transform of a function $f(x)$ exists if and only if $$\int_{-\infty}^\infty |f(x)|\ dx < \infty,$$ meaning that it converges. 
}

An interesting property of the Fourier transform is that we can recover the function $f(x)$ from $\hat{f}(k)$.

\thrm{Fourier inversion}{
	Let $f$ and its Fourier transform $\hat{f}$ be infinitely differentiable. Then, $$f(x)=\frac{1}{2\pi} \int_{-\infty}^\infty \hat{f}(k) e^{ikx}\ dk.$$
	
	Using the Fourier transform operator $\mcF^{-1}$, we have that $$\mcF^{-1}(\hat{f}) = f.$$
}

\tbf{Proof:} Since $\int_{-\infty}^\infty |f(x)|\ dx < \infty$ (we call functions that satisfy this property functions of $L^1$ type), we have that \begin{align} I=\frac{1}{2\pi} \int_{-\infty}^\infty \hat{f}(k) e^{ikx}\ dk &= \lim\limits_{\varepsilon \to 0} \frac{1}{2\pi} \int_{-\infty}^\infty \hat{f}(k) e^{-\varepsilon k^2} e^{ikx}\ dk \\ &= \lim\limits_{\varepsilon \to 0} \frac{1}{2\pi} \int_{-\infty}^\infty \int_{-\infty}^\infty f(y) e^{-\varepsilon k^2} e^{ik(x-y)}\ dy\ dk, \end{align} where in the final step, we used that $\hat{f}(k)= \int_{-\infty}^\infty f(y) e^{-iky}\ dy$.

Using integration by parts with the fact that $\int_{-\infty}^\infty e^{-k^2}\ dk = \sqrt{\pi}$, we get that $$\int_{-\infty}^\infty e^{-\varepsilon k^2} e^{ik(x-y)}\ dk = \sqrt{\frac{\pi}{\varepsilon}} e^{-\frac{(x-y)^2}{4\varepsilon}}.$$

Substituting back into (18.2) yields us \begin{align*} I &= \lim\limits_{\varepsilon \to 0} \frac{1}{2\sqrt{\varepsilon \pi}} \int_{-\infty}^\infty f(y) e^{-\frac{(x-y)^2}{4\varepsilon}}\ dy \end{align*}

Now we substitute $t=\frac{x-y}{2\sqrt{\varepsilon}}$ and $dt = -\frac{1}{2\varepsilon} dy$ such that $-\frac{(x-y)^2}{4\varepsilon} = -t^2.$ Therefore, $y=x-2\sqrt{\varepsilon} t$, and the integral becomes $$I=\lim\limits_{\varepsilon \to 0} \frac{1}{\sqrt{\pi}} f(x-2\sqrt{\varepsilon} t) e^{-t^2}\ dt.$$ But note that $-2\sqrt{\varepsilon} t$ approaches zero in the limit; thus, the integral is just $$I=\frac{f(x)}{\sqrt{\pi}} \int_{-\infty}^\infty e^{-t^2}\ dt,$$ which evaluates to $f(x)$ by employing the Gaussian integral.

\coro{$2\pi$}{
	You will see in Frank's lecture notes that the Fourier transform is defined slightly differently. He defines $$\hat{f}(k) = \mcF(f(x)) = \int_{-\infty}^\infty e^{-2\pi i kx} f(x)\ dx.$$ 
	
	Where does the $2\pi$ come from? The constant $2\pi$ makes the Fourier operator $\mcF$ a \emph{unitary operator} on $L^2$ space. This means that for any two functions $u(x), v(x)$, we have that $$\int_{-\infty}^\infty u(x) \bar{v}(x)\ dx = \int_{-\infty}^\infty \mcF(u(x)) \cdot \mcF(\bar{v}(x))\ dx.$$
	
	It, however, does not matter in our case since the meanings of any calculations henceforth are the same. To demonstrate the properties of the unitary operator, note that if we put $e^{-2\pi i kx}$ instead of $e^{-ikx}$, we have that $$\hat{f}(k) = \int_{-\infty}^\infty e^{-2\pi i kx} f(x)\ dx$$ and $$f(x) = \int_{-\infty}^\infty e^{-2\pi i kx} \hat{f}(k)\ dk$$ instead of including the $\frac{1}{2\pi}$ factor when performing Fourier inversion.
}

Here are some properties of the Fourier transform and its inversion.

\begin{enumerate}
	\item Let $a \in \bbC$. Then, $$\mcF(af) = a\mcF(f).$$
	\item $$\mcF\left(\frac{df}{dx}\right) = -ik \hat{f}(k).$$ This comes directly from the definition of the Fourier transform; simply represent $\mcF(\frac{df}{dx}) = \int_{-\infty}^\infty \frac{df}{dx} e^{ikx}\ dx$.
	\item In fact, we can generalise this result to higher-order derivatives. We have that $$\mcF\left(\frac{d^nf}{dx^n}\right) = (-ik)^n \hat{f}(k).$$ This is the result of a recursive process.
	\item Let $a \in \bbC$. Then, $$e^{ika} \hat{f}(k) = \mcF(f(x-a)).$$ 
	\item The result above is analogous to the \emph{shift property} $$\mcF(f(x-a)) = \int_{-\infty}^\infty f(x-a) e^{ikx}\ dx.$$
\end{enumerate}

The Fourier transform of the Dirac delta function is 1. By definition, $$\int_{-\infty}^\infty \delta(x)\ dx =1,$$ and that the Fourier transform is actually $$\int_{-\infty}^\infty \delta(x) e^{-ikx}\ dx;$$ the result follows by using integration by parts. 

\ex{}{
	Let $f(x) = e^{-a|x|}$. Then, $$\hat{f}(k) = \mcF(f(x)) = \int_{-\infty}^\infty e^{-a|x|} e^{-ikx}\ dx.$$ Here we take care in splitting up the $|x|$ into its $f(x)=x$ and $f(x)=-x$ branches; explicitly, we have \begin{align*} \int_{-\infty}^\infty e^{-a|x|} e^{-ikx}\ dx &= \int_{-\infty}^0 e^{ax}e^{ikx} \ dx + \int_{-\infty}^0 e^{-ax}e^{ikx} \ dx \\ &= \frac{1}{a-ik}+\frac{1}{a+ik} \\ \hat{f}(k)&= \frac{2a}{a^2+k^2}.   \end{align*}
}

\section{Applications of the Fourier transform}

\subsection{Convolution}

I first give an intuitive outlook towards convolution before providing a more mathematically rigorous description. 

Consider two continuous functions $f(x)$ and $g(x)$ defined over a particular range on the real axis (e.g. in the range $[-2, 2]$. Fix $f(x)$ and slide $g(x)$ over the real axis. In what follows, we consider the areas under the curves $f(x)$ and $g(x)$

The \emph{convolution} of $f(x)$ and $g(x)$ is a function denoted $$f *g(x).$$ It is defined as the overlap between the areas under $f(x)$ and $g(-x)$ as $g$ is shifted along the $x$-axis. Intuitively, you're sliding the function $g$ across $f$ and measuring the overlap at each point - the resulting function $f *g(x)$ is precisely a measure of this overlap.

We won't go into much detail into explaining why we flip $g(x)$ across the $y$-axis to obtain $g(-x)$ before sliding it across the $x$-axis to obtain our convolution function. For those interested, the cross-correlation of two functions $f$ and $g$ is the vocabulary for when we don't flip $g$ before sliding it over $f$. 

\ex{Discrete convolution}{Let's consider the discrete case as the continuous case may prove confusing. Consider two vectors $\vec{f}=\{1, 2, 3\}$ and $\vec{g} = \{3, 7, 5, 1\}$. For motivational purposes, you can consider these values to be the functions $f$ and $g$ evaluated at certain values - e.g. $f(1)=1, g(1)=3, g(2)=7$. 

We fix $f$ and slide $g$ over $f$, and compute the dot product of the overlapping values as it is the discrete analogue of the area under the curve. At the beginning, we have \begin{align*} &1\ 2\ 3  \\ 3\ 7\ 5\ &1 \end{align*} and the overlap is simply $1\cdot1=1$. Then we slide and obtain \begin{align*} &1\ 2\ 3  \\ 3\ 7\ &5\ 1 \end{align*} which yields an overlap of $1\cdot 5 + 2\cdot 1 =7$. Sliding again yields \begin{align*} &1\ 2\ 3  \\ 3\ &7\ 5\ 1 \end{align*} which gives $1\cdot7+2\cdot5+3\cdot1=7+10+3=20$. Then we have \begin{align*} &1\ 2\ 3  \\ &3\ 7\ 5\ 1 \end{align*} which yields a sum of 32. Sliding twice more yields sums of 27 and 9 respectively. Thus, the convolution of $\vec{f}$ and $\vec{g}$ is $1+7+20+32+27+9=96$.
}

Now that we've built intuition, let's formally define convolution in the continuous case.

\defn{Convolution}{
	Let $f(x)$ and $g(x)$ be continuous. The convolution of $f$ and $g$ is $$(f*g)(x) = \int_{-\infty}^\infty f(t)g(x-t)\ dt.$$
} 

Before linking the convolution to Fourier transforms, we discuss one special application of the convolution. This is the convolution of probability distributions.

\thrm{Convolution of probability distributions}{
	Let $X$ and $Y$ be two random variables, and let $Z=X+Y$. The probability distribution of $Z$ is the convolution of the probability distributions of $X$ and $Y$. 
	
	Formally, if we let $F_Z(z), F_Y(y)$, and $F_X(x)$ denote the cumulative distribution functions of $Z, Y$, and $X$ respectively and $f_Z(z), f_Y(y)$, and $f_X(x)$ denote their probability distribution function counterparts, then $$f_Z(z) = \int_{\Omega_X} f_Y(z-x) f_X(x)\ dx,$$ where $\Omega_X$ is the probability space by which $x \in X$ is defined on.
}

Note that I had not specified the order in which $X$ and $Y$ appear. This is because the convolution operator is commutative. To show this, simply note that for two functions $f$ and $g$, we have that with $$\int_{-\infty}^\infty f(t)g(t-x)\ dt,$$ we can simply substitute $u=x-t, du=-dt$ to obtain $$\int_{-\infty}^\infty f(x-u) g(u)\ du.$$

\tbf{Proof:} We first note that $F_Z(z) = P(X+Y \leq z)$. We first invoke a theorem.

\thrm{Law of Total Probability}{
	In the discrete case, the Law of Total Probability states that for a set of mutually exclusive and collectively exhaustive events $B_i$, then any event $A$ satisfies $$P(A) = \sum\limits_i P(A \cap B_i),$$ or, using conditional probabilities, $$P(A) = \sum\limits_n P(A \mid B_n) P(B_n).$$
	
	The continuous case is analogous to the discrete case in the sense that $$P(A) = \int_{x \in \Omega_X} P(A \mid X=x) f_X(x)\ dx.$$
}

Therefore, by computing and applying partial derivatives, \begin{align*}F_Z(z) &= P(X+Y \leq z) \\ &= \int_{x \in \Omega_X} P(X+Y \leq Z \mid X=x) f_X(x)\ dx \\ &= \int_{x \in \Omega_X} P(x+Y \leq z) f_X(x)\ dx \\ &= \int_{x \in \Omega_X} P(Y \leq z-x) f_X(x)\ dx \\  &=   \int_{x \in \Omega_X} F_Y  (z-x) f_X(x)\ dx \\ \frac{\partial}{\partial z} F_Z(z) &= \frac{\partial}{\partial z} \int_{x \in \Omega_X} F_Y  (z-x) f_X(x)\ dx \\ f_Z(z) &= \int_{x \in \Omega_X} f_Y  (z-x) f_X(x)\ dx \end{align*} which demonstrates that the PDF of $Z=X+Y$ is the convolution of the PDFs of $X$ and $Y$.

The discrete case is similar. We have that $$p_Z(z)= \sum\limits_{x \in \Omega_X} p_X(x) p_Y(z-x).$$

\ex{}{
	Suppose that $X$ and $Y$ are independent and identically distributed uniform distributions between 0 and 1. Let $Z=X+Y$. Then to compute the probability density function of $Z$, we first observe that for $X \sim \text{Unif}(0, 1)$, we have that $f_X(x) = 1$ for all $0 \leq x \leq 1$ and $f_X(x)=0$ otherwise. Then note, by convolution on probability distributions, that \begin{align*} f_Z(z) &= \int_{x \in \Omega_X} f_X(x) f_Y(z-x)\ dx  \\ &= \int_0^1 f_Y(z-x)\ dx \end{align*} since the probability space $\Omega_X$ is evidently $[0, 1]$. 
	
	To ensure that $z-x$ is in the density function, we have to ensure that $z-x \in [0, 1]$. We casework on whether $z \in [0,1]$ or $z \in [1,2]$ as each of those cases have different specifications as to whether $z-x \in [0, 1]$.
	
	\tbf{Case 1:} $z\in [0,1]$. In this case, $z-x\leq 1$ since $z\leq 1.$ We just have to ensure that $z\leq x$ in order for the probability density to be non-zero; otherwise, the integral evaluates zero. Thus, the integral is \begin{align*} f_Z(z) &= \int_0^z f_Y(z-x)\ dx + \int_z^1 f_Y(z-x)\ dx \\ &= \int_0^z 1 \ dz + \int_0^z 0\ dz \\ &= z. \end{align*}
	
	\tbf{Case 2:} $z\in [1, 2]$. In this case, we have that $z-x \leq 0$ since $x \in [0,1]$. We just need the condition that $z-x\leq 1$. This suggests that $x\geq z-1$, meaning that we henceforth evaluate the integral \begin{align*} f_Z(z) &= \int_0^{z-1} f_Y(z-x) \ dx + \int_{z-1}^1 f_Y(z-x)\ dx  \\ &= \int_0^{z-1} 0\ dx + \int_{z-1}^1 1\ dx \\ &= 1-(z-1)\\ &= 2-z. \end{align*}
	
	Thus, the probability distribution $f_Z(z)$ is $$f_Z(z) = \begin{cases} z & 0\leq z \leq 1 \\ 2-z & 1 \leq z \leq 2 \\ 0 & \text{otherwise}. \end{cases}$$
}

\tbf{Exercise:} Draw out the probability density function graphs of $f_X(x)$ and $f_Y(y)$, and slide the PDF of $Y$ and over $X$. What do you notice?

Though perhaps the most interesting application of convolution is the \emph{convolution theorem}. 

\thrm{}{
	Let $f(x)$ and $g(x)$ be two continuous functions. Then, $$\mcF((f*g)(x)) = \mcF(f) \cdot \mcF(g),$$ where $\mcF$ denotes the Fourier transform.
}

This suggests that we merely need to multiply the Fourier transforms of the functions $f$ and $g$ to find the Fourier transform of its convolution. 

\subsection{Differential equations}

Fourier transforms are a means of solving differential equations. 

\ex{}{Consider the equation $$\frac{df}{dx}+af(x) = 0.$$ The Fourier transform entails that $$\mcF(\frac{df}{dx}) + a\mcF(f(x))=0.$$ Performing each of the transforms yields $$ik \hat{f}(k) + a \hat{f}(k)=0,$$ or $$(ik+a)\hat{f}(k)=0.$$ This means that $$\hat{f}(k) = \alpha \delta(ik+a).$$

Simply perform the inverse Fourier transform to obtain \begin{align*} \mcF^{-1}(\mcF(f(x))) &= \frac{1}{2\pi}\int_{-\infty}^\infty \alpha \delta(ik+a) e^{ikx}\ dk \\ &= \alpha e^{-ax} \end{align*} which aligns with the general solution for a differential equation.

}

If instead we consider a second-order differential equation, we have the following:

\ex{}{
	Consider the differential equation $$\frac{d^2f}{dx^2} + \alpha \frac{df}{dx} + \beta f = 0.$$ Performing a Fourier transform on every term yields $$\mcF\left(\frac{d^2f}{dx^2}\right) + \alpha \mcF\left(\frac{df}{dx}\right) + \beta \mcF(f)=0,$$ or $$(-k^2+ik\alpha+\beta)\hat{f}(k) =0,$$ or $$\hat{f}(k) = c\delta(-k^2+ik\alpha+\beta)$$ for some constant $c$. Using the properties of the Dirac delta function, we can split the function $\delta(-k^2+ik\alpha+\beta)$ up into two constituent functions. Since the roots of $\delta(-k^2+ik\alpha+\beta)$ are $k_{1, 2} = \frac{i\alpha \pm \sqrt{-\alpha^2+4\beta}}{2}$, we can write
	
	$$\hat{f}(k) = c_1 \delta\left(k-\frac{i\alpha + \sqrt{-\alpha^2+4\beta}}{2}\right) + c_2 \delta\left(k-\frac{i\alpha - \sqrt{-\alpha^2+4\beta}}{2}\right)$$ and we can perform inverse Fourier transforms to obtain $$f(x) = \frac{1}{2\pi} c_1 e^{\frac{i\alpha + \sqrt{-\alpha^2+4\beta}}{2} x} + c_2 e^{\frac{i\alpha - \sqrt{-\alpha^2+4\beta}}{2} x}.$$
}

\subsection{Characteristic functions}

\defn{Characteristic function of a probability distribution}{
	Let $X$ be a random variable. Its characteristic function $\varphi_t(X)$ is the expected value of the random variable $e^{itX}.$ So, $$\varphi_X(t) = \bbE(e^{itX}).$$ Why is it related to the Fourier transform? Well, simply note that $$\bbE(e^{itX}) = \int_{-\infty}^\infty e^{itx} f_X(x)\ dx,$$ which is the Fourier transform of the probability density function $f_X(x)$. 
}

\ex{Binomial distribution}{
	The probability distribution function of the Binomial distribution is $$p_X(x) = \binom{n}{x}p^x (1-p)^{n-x}.$$ Since the Binomial distribution is a discrete distribution, we must use the discrete form $$\varphi_X(t) = \sum\limits_{x \in \Omega_X} e^{itx}\cdot p_X(x).$$ This is \begin{align*} \varphi_X(t) &= \sum\limits_{x=0}^n e^{itx} \binom{n}{x}p^x (1-p)^{n-x} \\ &= \sum\limits_{x=0}^n \binom{n}{x}(pe^{it})^x (1-p)^{n-x} \\ &= (1-p + pe^{it})^n  \end{align*} in which in the final step, we note that $(a+b)^n = \sum\limits_0^n \binom{n}{x}a^x b^{n-x}$.
}

In your statistics course you should have learned that the Binomial distribution converges in limit to the Poisson distribution. We can show this holds in the case of the characteristic function.

\ex{Poisson distribution vs Binomial distribution}{
	We begin by observing that in the case of the Binomial distribution, that \begin{align*} \phi_{X_n}(t) &= (1-p_n+p_n e^{it})^n  \\ &= \left(1+ \frac{np_n (e^{it}-1)}{n}\right)^n \\ &= e^{n \ln \left(1+ \frac{np_n (e^{it}-1)}{n}\right)}. \end{align*}
	
	Taking logarithms on both sides yields $$\ln \phi_{X_n}(t) = n \ln \left(1+ \frac{np_n (e^{it}-1)}{n}\right).$$ We can then perform a Taylor expansion on the logarithmic function on the right-hand side to yield $$\ln \phi_{X_n}(t) = n\cdot \frac{np_n(e^{it}-1)}{n} + \dots,$$ where the $\dots$ represent higher-order terms. Substituting $\lambda=np_n$ as is typical in the case of transforming from the Binomial to the Poisson, we have that $\ln \phi_{X_n}(t) = \lambda_n (e^{it}-1)$, or $$\phi_{X_n}(t) = e^{\lambda(e^{it}-1)}.$$
	
	This is the characteristic function of the Poisson distribution. One can verify this by checking that the characteristic function of the Poisson's PDF $\frac{\lambda^k e^{-\lambda}}{k!}$ is indeed $e^{\lambda(e^{it}-1)}$.
}

\subsection{Economic applications of Fourier analysis}

Fourier analysis is widely used in physics and engineering. It also has applications in economics and finance. Financial markets and business cycles often exhibit cyclical behaviour. The Fourier series identifies said cycles by decomposing said financial data into its constituent frequencies. By this, we can focus on particular market dynamics and observe seasonal effects.

\defn{Fourier series}{
	For a periodic function, define the Fourier series to be $$f(t) = a_0 + \sum\limits_{n=1}^\infty \left(a_n \cos\left(\frac{2\pi nt}{T}\right) + b_n \sin\left(\frac{2\pi nt}{T}\right)\right).$$
}

One can use this Fourier series to decompose GDP data or other indicators of business cycle performance, or earnings data for a company to decompose this data into its constituent frequencies. One can use this to observe and find seasonal trends. 

In this week's EconZone, we demonstrate how Fourier series is built into the evaluation of data with annual, biannual, or generally periodic trends.

\coro{Fourier series vs transform}{
	The Fourier series and the Fourier transform both capture the idea of writing functions as either sums (in the discrete case) or integrals (in the continuous case) of some class of functions. In this case, the class of functions at hand are complex exponential functions. 
	
	The Fourier transform can be viewed as the limit of the Fourier series as the period $T$ approaches infinity such that we enable the limits of integration to be $(-\infty, \infty)$. 
	
	Another distinction between the two are the types of functions that are acted on. The Fourier series only represents periodic functions as a sum of complex exponential functions. Meanwhile, the Fourier transform can convert both periodic functions (by representation through the Fourier series) and non-periodic functions (by performing the transform and converting it into its continuous frequency domain). 
}

\chapter{Week 19}

In this week, we discuss dynamic optimisation, where we introduce the notion of time into optimisation problems. This is another application of difference equations. 

\section{Dynamic optimisation}

\subsection{Discrete-time dynamic optimisation}

In past weeks, all optimisation problems have been \emph{static optimisation} problems, where the objective function and constraint does not vary over time. For example, in the one-variable case, we looked at optimisation problems of the form $\max\limits_x u(x)$ by checking $u'(x)=0$ and seeing if $u''(x) <0$. 

In dynamic optimisation, we introduce the notion of time.

Recall that when performing static optimisation, we established a way of finding optima using the iterative process of gradient ascent and descent, or \emph{gradient learning}. This came in the form of a difference equation $$\vec{y}_{t+1}-\vec{y}_t = \alpha_t \nabla g(\vec{y}_t).$$ 

You may notice that instead of the subscripts $n$ in the gradient learning equation, we used the time-subscript $t$. Let's suppose we considered the one-variable version of the gradient learning equation. This is $$y_{t+1}-y_t= \alpha g_t(x_t, y_t, t)$$ in which the $x_t$ are choice variables chosen for optimisation purposes. Also suppose that at each period $t=0, 1, \dots, T$, the reward of choosing $x_t$ and $y_t$ is $f_t(x_t, y_t)$. Therefore, the reward of choosing $x_0, \dots, x_T, y_1, \dots, y_T$ is $$\sum\limits_{t=0}^T f_t(x_t, y_t)$$ though we work with respect to the gradient-learning constraint. 

\coro{}{
	It is customary to add the $t$ as a parameter in the gradient-learning difference equation. This is not seen in the general gradient-learning optimisation case; we must make distinctions here. 
}

How do the control variables $x_t$ work? In any given period $t$, the agent chooses $x_t$ and $y_t$ is given by previous history. When we go from period $t$ to $t+1$, note that $y_{t+1}$ is subsequently determined by the gradient learning equation. We then choose $x_{t+1}$ and so on.

This is the dynamic optimisation problem in the general sense. Later we will define the dynamic optimisation problem in the case of utility-maximisation with respect to time-specific statistical weights.

\coro{}{
	Here, we assume that the endpoints $y_0$ and $y_{T+1}$ are given. These are our \emph{endpoint conditions}.
	
	It is important to distinguish between the fixed-endpoint problem and the free-endpoint problem. In the fixed-endpoint problem, both $y_0$ and $y_{T+1}$ are given; in the free-endpoint problem, only $y_0$ is given. 
}

\ex{}{
	A typical example of $f_t$ would be utility optimisation with respect to time-weights. Here, we are poised to optimise by choosing options $x_0, x_1, \dots, x_T$ such that $$\sum\limits_{t=0}^T u(y_t) \cdot w_t$$ is maximised with respect to $y_t$. Here, the $w_t$ take the form of time-specific statistical weights, where at different points in time, one's utility may be augmented or diminished with regard to said weights with reference to how important said utility is at that point in time.
}

One problem arises. Can't the statistical weights be imbued into the utility function? Possibly, but here we distinguish between utility at a single point in time, and the weights being placed on it at that juncture.

\defn{Dynamic optimisation problem}{
	Thus, the dynamic optimisation problem is to maximise the function $$\sum\limits_{t=0}^T u(y_t) \cdot w_t$$ with respect to the constraint $y_{t+1}-y_t = \alpha f_t(w_t, y_t).$
	
	Here, the control, or choice variables $x_t$ take the form of statistical weights $w_t$.
}

Let's first consider the simple case where $w_t=1$. This suggests that the maximisation problem is merely $$\max\limits_{y_t} u(y_t),$$ which is a simple optimisation problem. 

\coro{Abuse of notation}{
	To shorten notation, we write $u(y_0, \dots, y_t)$ as $u(Y)$.
}

In this case, the first-order conditions are found by simply performing partial differentiation with respect to $y_0, y_1, \dots, y_T$. The first order conditions are: $$\begin{cases} \frac{\partial u(Y)}{\partial y_0}&=0 \\ \frac{\partial u(Y)}{\partial y_0}&=0 \\ &\vdots \\ \frac{\partial u(Y)}{\partial y_T}&=0.  \end{cases}$$

Now we introduce the constraint. The constraint is such that $y_{tot} = \sum\limits_{t=0}^T y_t$. Thus, the Lagrangian function is $$\mcL(Y) = u(y_0, \dots, y_t)- \lambda (y_0+\dots+y_T - y_{tot}).$$ 

Let's introduce time-weights that are not all unitary. In this case, the first-order conditions are $$\frac{\partial u(y(t))}{\partial y(t)} \cdot w(t) - \lambda=0$$ and $$y_0+\dots+y_T - y_{tot}=0$$ which is the constraint. Note this means that the marginal utility $\frac{\partial u(y_t)}{\partial y_t} \cdot w_t =\lambda$, meaning that it is constant over time and is also constant regardless of the value of $t$. 

This suggests that for any $t$, the identity $\frac{\partial u(y_t)}{\partial y_t} \cdot w_t = \frac{\partial u(y_{t+1})}{\partial y_{t+1}} \cdot w_{t+1}$ is satisfied, or $$\frac{\partial u(y_{t+1})}{\partial y_{t+1}} = \frac{w_t}{w_{t+1}} \cdot \frac{\partial u(y_t)}{\partial y_t}.$$ This is a recurrence relation!

Now suppose that we wanted to incorporate rates of change into our utility function. This theory will arise again when we discuss continuous-time dynamic optimisation, where the rate of change of $y$ is incorporated into the utility function.  

Define a new utility function which incorporates the rate-of-change as one of its parameters: $$\mcU(y_t) = u(y_t) - \frac{\mu}{2} (y_{t+1}-y_t)^2.$$ Again we aim to maximise $$U =\sum\limits_{t=0}^T \mcU(x_t).$$ 

What are the first-order conditions for $U$? Well, simply note that although $U$ is a sum from $t=0$ to $t=T$, only the terms $\mcU(y_t)$ and $\mcU(y_{t-1})$ are concerned when first-order conditions are at hand. This is because \begin{align*} \mcU(y_t) &= u(y_t) - \frac{\mu}{2} (y_{t+1}-y_t)^2 \\ \mcU(y_{t-1}) &= u(y_{t-1}) - \frac{\mu}{2} (y_{t}-y_{t-1})^2 \end{align*} are concerned.

The marginal utilities of these two terms \tbf{with regards to} $y_t$ are \begin{align*} \frac{\partial\mcU(y_t)}{\partial y_t} &= \frac{\partial u(y_t)}{\partial y_t} + \mu(y_{t+1}-y_t) \\ \frac{\partial \mcU(y_{t-1})}{\partial y_t} &= - \mu (y_{t}-y_{t-1}) \end{align*}

Note the disappearance of the $u(y_{t-1})$ term in the second first-order condition as we are differentiating with respect to $y_t$, not $y_{t-1}$. Thus, we conclude that \begin{align*} \frac{\partial U}{\partial y_t} &= \frac{\partial u(y_t)}{\partial y_t} + \mu(y_{t+1}-y_t) - \mu(y_t-y_{t-1}) \\ &= \frac{\partial u(y_t)}{\partial y_t} + \mu(y_{t+1}-2y_t+y_{t-1}) \\ &= \frac{\partial u(y_t)}{\partial y_t} + \mu((y_{t+1}-y_t)-(y_t-y_{t-1})) \\ &=0. \end{align*}

This is a second-order difference equation. If we truncate by writing $v_t = \Delta y_t = y_t-y_{t-1}$, we have that $$\frac{\partial u(y_t)}{\partial y_t} + \mu(v_{t+1}-v_t)=0,$$ which means that $$v_{t+1}-v_t = -\frac{1}{\mu} \frac{\partial u(y_t)}{\partial y_t}.$$

\subsection{Continuous-time dynamic optimisation}

In the previous section, we viewed a modified utility function $$\mcU(y_t) = u(y_t) - \frac{\mu}{2} (y_{t+1}-y_t)^2.$$ in which not only was the utility of $y_t$ (denoted $u(y_t)$) considered, but also the utility of $\Delta y_t = y_{t+1}-y_t$ in the form of the utility function $$\bar{u}(\Delta y_t) = \frac{\mu}{2} (\Delta y_t)^2.$$

When we work in continuous time instead of discrete time, our $\Delta y_t$ is now the derivative $\frac{dy(t)}{dt}$. Thus, our utility function requires us to choose $y(t)$ such that we maximise $$U(y(t)) = \int_0^T u\left(y(t), \frac{dy}{dt}, t \right).$$ Here we also have that $y(0)$ and $y(T)$ is given. 

Let us backtrack a bit. In the beginning of the discrete-time dynamic optimisation section, we had that $$\Delta y_t = \alpha g_t(x_t, y_t, t)$$ as our gradient learning equation. The continuous-time analogue is \begin{equation} \frac{dy}{dt} = g(x(t), y(t), t).\end{equation} Here, the agent chooses $x(t)$ such that their total utility $$\int_0^T u(x(t), y(t), t)\ dt$$ is maximised. 

To approach this problem, we use an approach similar to the method of Lagrangian multipliers - the Hamiltonian method.

\defn{}{
	Suppose we have the classic dynamic optimisation problem, where $\frac{dy}{dt} = g(x(t), y(t), t)$ was given in order to maximise $$\int_0^T u(x(t), y(t), t)\ dt.$$ We define the Hamiltonian $H(x, y, t, \lambda)$ as $$H(x, y, t, \lambda) = u(x, y, t) + \lambda g(x, y, t).$$
	
	The first-order conditions of the Hamiltonian function are \begin{align*} \frac{\partial H}{\partial x}&=0 \\ \frac{\partial H}{\partial y} &= -\frac{\partial \lambda}{\partial t} \\  \frac{\partial H}{\partial \lambda} &= \frac{\partial y}{\partial t} .\end{align*}
}

Let us apply the Hamiltonian back into our continuous-time problem. Here, we have that $x(t) = \frac{dy}{dt}$. Hence, $g(x, y, t) = x$ by substitution back into equation (19.1). Thus, the Hamiltonian function is $$H(x, y, t, \lambda) = u(x, y, t)+\lambda x.$$ The first-order conditions are \begin{align*} \frac{\partial u}{\partial x}&=-\lambda \\ \frac{\partial u}{\partial y} &= -\frac{\partial \lambda}{\partial t}. \end{align*}

Relating these first-order conditions, we have that $$\frac{d}{dt} \left(\frac{\partial u}{\partial x} \right) = \frac{\partial u}{\partial y},$$ or, using the fact that $x(t) = \frac{dy}{dt}$, we have the Euler equation:
\defn{Euler equation}{If we aim to seek the optimal path between $y(0)$ and $y(T)$, such a path is given by the equation $$\frac{d}{dt} \left(\frac{\partial u}{\left(\frac{\partial y}{\partial t} \right)} \right) = \frac{\partial u}{\partial y}.$$}

\coro{}{
	The official lecture notes use small deviations around the solution $x_t$ to obtain the Euler equations. The Hamiltonian method is an alternative method used in the Pemberton and Rau textbook.
}

\ex{}{
	Suppose that we chose $y(t)$, where $0\leq t\leq 1$, to maximise the objective function $$\int_0^1 e^t \left(\left(\frac{dy}{dt}\right)^2+6y\right)\ dt$$ subject to the initial conditions $y(0)=0, y(1)=1$. 
	
	The Euler equation is $$\frac{d}{dt} \left(2e^t \frac{dy}{dt} \right)= 6e^t.$$ Differentiating yields $$2e^t \frac{d^2y}{dt^2} + 2e^t \frac{dy}{dt} = 6e^t,$$ or $$\frac{d^2y}{dt^2} + \frac{dy}{dt} = 3.$$
	
	Methods explored in Week 14's appendix tells us that the general solution satisfies $$y(t) = C_1 + C_2 e^{-3t} + 3t.$$ Given the initial conditions, we have that $$y(t) = \frac{-2}{e-1} + \frac{2e}{e-1} e^{-3t} + 3t.$$
}

\section{Economic applications of dynamic optimisation}

Here are some economic applications of the Euler equation.

\ex{}{
	An agent supplies a stock of goods $S$. They supply goods to the market; the quantity supplied is the rate $r(t)$. Assume that the market clears. Thus, the price that they charge is $$p(t) = p_0-ar(t).$$ The total profit for the agent is \begin{align*} \pi_T &= \int_0^T p(t) \times r(t) \\ &= \int_0^T (p_0-ar(t)) r(t)\ dt. \end{align*} This, of course, is subject to the constraint $\int_0^T r(t)\ dt = S$, the total stock of goods.
	
	We now create the Lagrangian function given by $$\mcL = \int_0^T ((p_0-ar(t)) r(t) )\ dt- \int_0^T (\lambda \cdot r(t))\ dt-S=0.$$ Incorporating $S$ into the integral yields $$\mcL = \int_0^T (p_0-ar(t)) r(t) - \lambda \cdot \left(r(t)-\frac{S}{T}\right)=0.$$
	
	Since the total utility is represented in the form of an integral, the utility at a single point is in the form $$\mcU(r(t)) = (p_0-ar(t)) r(t) - \lambda \left(\cdot r(t)-\frac{S}{T}\right).$$
	
	Notice that there is no $\frac{dr}{dt}$ term in this Euler equation. The first-order conditions are $$\frac{\partial \mcU}{\partial r(t)} = p_0-2ar(t) -\lambda=0,$$ and, evidently, the constraint $$\frac{\partial \mcU}{\partial r(t)} = \int_0^T r(t)\ dt - S=0.$$
	
	The first first-order condition yields $$r(t) = \frac{p_0-\lambda}{2a}.$$
}

What if we wanted to include exponential discounting into our model? In this model, future costs and benefits are discounted at a consistent rate $e^{-rt}$ as this utility of future costs and benefits are viewed less strongly than current costs and benefits. 

In this case, the modified utility function is now $$\mcU(r(t)) = e^{-rt} ((p_0-ar(t)) r(t) )- \lambda \cdot \left(r(t)-\frac{S}{T}\right)$$ where we only discount the objective function but not the constraint. Taking partial derivatives with respect to $r(t)$ yields $$\frac{\partial \mcU}{\partial r(t)} = -e^{rt}\lambda - 2ar+p_0=0,$$ which yields $$r(t) = \frac{-e^{rt}\lambda+p_0}{2a}.$$ The constraint satisfies $$\int_0^T r(t)\ dt -S =0,$$ or $$\int_0^T \frac{-e^{rt}\lambda+p_0}{2a}\ dt -S=0,$$ suggesting $\lambda=\frac{r(Tp_0-2aS)}{e^{rT}-1}.$

\ex{Firm investment}{
	Consider a firm with capital stock $K_t$ at each period $t = 0, \dots, T$. Let its gross investment expenditure be $I_t$ at each period $t$, and assume that stock depreciation is $\delta$ and the firm's discount rate is $r$. Therefore, the firm's capital satisfies, in continuous time, the differential equation $$\frac{dK_t}{dt} = I-\delta K.$$ The revenue that the firm faces is $\pi(t)K(t)$, and it faces costs $C(I(t))$ that depend on investment expenditures $I(t)$. Therefore, the investment problem, given exponential discounting is to maximise $$\int_0^T e^{-rt} (\pi(t)K(t) - C(I(t))\ dt$$ subject to $$\frac{dK}{dt} = I-\delta K.$$
	
	This time, we use the Hamiltonian instead of the Lagrangian to analyse this problem. The Hamiltonian is $$H (K, \mu, t)=\pi(t)K - C(I(t))+\mu (I-\delta K).$$ We also have that $$\frac{\partial \mu}{\partial t} = (r+\delta)\mu-\pi.$$
	
	We can solve this differential equation using the integrating factor method. In this case, the integrating factor is $e^{-(r+\delta)t}$; we incorporate the fact that $\mu(T)=0$ to obtain $$\mu(t)=\int_t^T e^{(r+\delta)(t-s)}\pi(s)\ ds.$$
	
	Representations for $\mu(t)$ and $I(t)$ depend on the specific values of $C(I)$ and $\pi(t)$.
}

This week's EconZone discusses an example in resource economics. In said example, the Euler equation includes the term $\frac{dy}{dt}.$

\chapter{Week 20}

\section{Review of weeks 11-19}

When compared to Weeks 1-9, the content covered in Weeks 11-19 are extension material. More importantly, they primarily comprise extensions of material covered in Weeks 6-8, namely eigenvalue problems and matrix algebra, difference and differential equations, and constrained optimisation.

Constrained optimisation played a key role in Term 2's content. This came in the form of the envelope theorem, which provided a marginal interpretation of the Lagrange multiplier. When dealing with inequality constraints, the Kuhn-Tucker conditions proved useful. The Legendre transform also arose as a means of performing optimisation.

One must take care to observe that the Legendre transform transforms a function $f(x)$ with parameter $x$ into a function in the conjugate variable $y$. This is a transformation between sets of functions, which opens up our discussion into function spaces. This changes our view of linear algebra and eigenvalue problems to include \emph{eigenfunction problems}. 

To expand our toolkit in addressing eigenvalue problems, we also looked at addressing eigenvalue problems of the form $M\vec{v} = \lambda\vec{v}$ when the matrix $M$ is not symmetric. This allowed us to distinguish between left and right eigenvectors. 

Difference and differential equations played a major role given our newfound knowledge on function spaces. The function-space knowledge allowed us to find spaces of solutions to first-order differential equations by taking snapshots out of the difference map matrix. Note that when we are referring to an indefinitely long time series, the matrix representing the difference map is an \emph{infinite matrix}! We also looked at using the Fourier transform to solve differential equations; we perform the Fourier transform, then its inverse.

Lastly, we looked at dynamic optimisation (as opposed to static optimisation), and observed that our utility function could be altered to include $\Delta y_t$, or changing preferences over a period of time, as a penalty. This culminated in the Euler equation to seek an optimal path in dynamic optimisation.

\section{Two really long problems}

The following problems should cover many topics in this course, and will set you up to be ready for examination.

\subsection{Question 1} 

Consider two firms with production functions $$f_1(x, y) = 15x^{\frac{1}{4}}y^{\frac{1}{4}}$$ and $$f_2(x, y) = 8ax^{\frac{1}{3}}y^{\frac{1}{3}}$$ and cost functions $c_1(x, y)=2x+3y$ and $c_2(x, y)=ax+y$ respectively; both firms set prices to be £1 per unit of output. 

a) i. Find the profit functions for both firms, and the Lagrangians for both firms.

ii. Find the first-order conditions of said Lagrangians.

b) i. Consider the \tbf{first firm's} revenues and costs for now. Suppose that instead of the budget constraint being $2x+3y =B$, it was $2x+3y \leq B$. Does the optima change when you invoke the Kuhn-Tucker conditions?

ii. Suppose that $a=4.$ Calculate $\frac{df_2}{da}$. 

iii. From here, assume that $a=4.$ Construct the Jacobian matrix $J_{\vec{f}}$, where $\vec{f} = \begin{pmatrix} f_1(x, y) \\ f_2(x, y) \end{pmatrix}$. Find the eigenvalues of the matrix. Then find the left-eigenvectors and right-eigenvectors, and comment on their nature.

c) i. Suppose that the \tbf{first firm} finds optimum prices via gradient ascent. Use a first-order difference equation to represent how the firm learns its prices via gradient learning, and solve said difference equation.

ii. Now consider when both firms find optimum prices via gradient ascent. Therefore we have a system of difference equations of the form $$\vec{x}_{t+1}-\vec{x}_t = \vec{V}(\vec{x}) \vec{x}_t.$$ Find $\vec{V}(\vec{x})$, and thus find the two stationary states.

iii. If we have small deviations $\vec{x}_t = \vec{x}_s + \vec{\delta x_t}$ from the stationary state, find a linear approximation using the Jacobian matrix. Find the eigenvalues and eigenvectors of said Jacobian, and sketch a phase portrait of behaviour around the two stationary states. (You can use two separate graphs to represent the behaviour, or integrate it into one graph)

d) i. Suppose now that the \tbf{second firm} performs gradient learning via an infinitesimal process. Suppose that this process can be described using the form \begin{align*} \frac{d}{dt} \begin{pmatrix} x(t) \\ y(t) \end{pmatrix} &= \gamma \nabla_{x(t), y(t)} \pi(x, y) \\ &= \gamma \begin{pmatrix} \frac{\partial \pi(x, y)}{\partial x} \\ \frac{\partial \pi(x, y)}{\partial y} \end{pmatrix}.\end{align*}

Is the stationary state (optimal values of $x$ and $y$) stable? Use a differential equation involving a Jacobian to demonstrate this. 

ii. Now suppose that the second firm incurs additional administrative costs equal to $$\mcU(x, y)=3\left(\frac{df(x, y)}{dt}\right)^2 + \left(\frac{df(x, y)}{dt}\right).$$ Construct and solve (optional) the associated Euler equation. 

\subsection{Question 2}

a) Consider the matrix $$M=\begin{pmatrix}
\frac{11}{4} & -\frac{5}{4} & \frac{5}{4} \\
\frac{3}{4} & \frac{3}{4} & -\frac{3}{4} \\
2 & -2 & 2
\end{pmatrix}.$$

i. Find the eigenvalues and the eigenvectors of this matrix. 

ii. Thus, perform a spectral decomposition of this matrix, meaning you represent it in the form $$M = \sum\limits_{i=1}^3 \mu_i (\hat{m}_i \otimes \hat{m}_i),$$ where $\mu_i\ (i=1, 2, 3)$ are the eigenvalues and $\hat{m}_i\ (i=1, 2, 3)$ are the normalised eigenvectors. With this, find $M^{-1}$.

iii. Find $e^{Mx}$.

b) Suppose we have a function given by $$f_i(\vec{x}) = \vec{x}_i \cdot (\vec{x} \cdot M)_i + \vec{x}_{4-i} \cdot(M \cdot \vec{x})_{4-i}$$ where $\vec{x}_i$ refers to the $i$'th element of the vector $\vec{x}$. 

i. Let $\vec{x} = \{x_1, x_2, x_3\}$ (which should be quite obvious from the fact that we set one of the subscripts to be $4-i$). Find the first-order conditions.

ii. Let $\hat{f}_i(\vec{x})$ be the normalised version of $f_i(\vec{x})$. Find a matrix with eigenvectors $f_i(\vec{x})$ and eigenvalues being the eigenvalues of $M$.

c) Consider the difference equation $$\vec{x}_{t+1} = M \cdot \vec{x}_t + \begin{pmatrix} 1\\ -1\\ 1 \end{pmatrix}.$$ What is the homogeneous solution for the system of difference equations? What is the general solution (sum of homogeneous and particular solutions)?

d) Perform linearisation around the stationary states, and determine whether the stationary states are stable. Sketch a phase portrait demonstrating behaviour near the stationary states.

\chapter{Further reading and final remarks}

Work in progress! See later versions for more.

\section{Mathematics in economics}

\section{Mathematics in finance}

We begin by considering the \emph{lognormal distribuions} as the binomial model is based on a lognormal random walk. Recall that if the random variable $X$ is log-normally distributed, then $Y=\ln X$ has a normal distribution. 

First observe that $$E(S_t)=\exp(\mu+\frac{\sigma^2}{2})$$ and $$Var(S_t) = (\exp(\sigma^2)-1)\exp(2\mu+\sigma^2).$$

Then we invoke a corollary of Ito's lemma (Workshop 3). We have that $\ln S_t$ is normally distributed with mean $E(\ln S_t)=\ln S_0+(\mu-\frac{\sigma^2}{2}t)$ and variance $Var(\ln S_t)=\sigma^2 t$. Thus, \begin{align*} E(\ln S_{t+\delta t}) &= \ln S_t + (\mu-\frac{\sigma^2}{2}) \delta t \\ Var(\ln S_{t+\delta t}) &= \sigma^2 \delta t. \end{align*}

Substitution back into the expressions for the expected value and variance of the lognormal distribution yield $$E(S_{t+\delta t}) = S_t e^{\mu \delta t}$$ and \begin{align*} Var(S_{t+\delta t}) &= S_t^2 \sigma^2 \delta t+ S_t^2 (2\mu \delta t) \sigma^2 \delta t \\ &= S_t^2 \sigma^2 \delta t\end{align*} where the second term involves a $\delta t^2$ term so it converges to zero quicker than the first term; in the approximation we omit the second term. 

Risk-neutral expectations yield $$S_t(pu+(1-p)v) = S_t e^{\mu \delta t}$$ so $$pu+(1-p)v = e^{\mu \delta t}.$$ Thus, $$p=\frac{e^{\mu \delta t}-v}{u-v}.$$ 

Now we consider the variance. Note that \begin{align*} Var(S_{t+\delta t}) &= E(S_{t+\delta t}^2) - (E(S_{t+\delta t}))^2 \\ S_t \sigma^2 \delta t&= S_t(pu^2+(1-p)v^2-(pu+(1-p)v)^2) \\ \sigma^2 \delta t&= (p-p^2)(u-v)^2.\end{align*}

Substitute $p=\frac{e^{\mu \delta t}-v}{u-v}$ from the expression for the expected value. Thus, \begin{align*} \sigma^2 \delta t &= \left( \frac{e^{\mu \delta t}-v}{u-v} - \frac{e^{2\mu \delta t}-2e^{\mu \delta t}v+v^2}{(u-v)^2}\right) (u-v)^2 \\ &= e^{\mu \delta t}(u+v)-e^{2\mu \delta t} - uv. \end{align*}

Here we assume that $uv=1$ or $v=\frac{1}{u}$ - taking an upwards and a downwards path on a two-step binomial tree should yield the same result as taking a downwards then an upwards path. Thus, $$\sigma^2 \delta t = e^{\mu \delta t}(u+\frac{1}{u})-e^{2\mu \delta t} - 1,$$ or $$u+\frac{1}{u} = \frac{\exp({2\mu \delta t}+\sigma^2 \delta t +1)}{\exp({\mu \delta t})}$$

Simplifying the right-hand side and using the Taylor approximation $e^x = 1+x$ (which holds as we take the time interval $\delta t$ to be infinitesimally small), we get that \begin{align*}u+\frac{1}{u} &= e^{\mu\delta t} + e^{-\mu \delta t} \sigma^2 \delta t + e^{-\mu \delta t} \\ &= 1+\mu\delta t+  (1-\mu\delta t) \sigma^2\delta t + (1-\mu\delta t) \\ &= 2+\sigma^2\delta t - \mu\sigma^2 (\delta t)^2 \end{align*} in which the last term in the final line converges to zero quickly as it contains a $(\delta t)^2$ term. 

Therefore, $$u^2+1=(2+\sigma^2 \delta t)u,$$ yielding us $$u = \frac{2+\sigma^2 \delta t \pm \sqrt{\sigma^4 (\delta t)^2 +4\sigma^2 \delta t}}{2}$$ in which the $\sqrt{4\sigma^2 \delta t}$ term has quickest convergence out of all terms containing $\delta t$. Thus, \begin{align*} u &= 1 \pm \frac{\sqrt{4\sigma^2 \delta t}}{2} \\ &= 1 \pm \sigma \sqrt{\delta t}. \end{align*} 

Here we take $u=1+\sigma\sqrt{\delta t}$ to be the larger value, and $v=1-\sigma\sqrt{\delta t}$ to be the smaller one. Undoing the approximation yields $$u=\exp(\sigma \sqrt{\delta t})$$ and $$v=\exp(-\sigma \sqrt{\delta t}).$$ 

Employing a Taylor approximation on the expression for $p$ and using the approximations for $u$ and $v$ yields \begin{align*} p&=\frac{1+\mu \delta t-v}{u-v} \\ &= \frac{1}{2} + \frac{\mu}{2\sigma} \sqrt{\delta t}.\end{align*}

Consider a Black-Scholes portfolio with value $$\Pi = V-\Delta S.$$ At time $t$, the option has value $V=V(S, t)$. At time $t+\delta t$, the option either has value $$V^+=V(uS, t+\delta t)$$ or $$V^- = V(vs, t+\delta t).$$ Thus, the portfolio has value $V^+-\Delta us$ or $V^- -\Delta vs$. Delta hedging requires $$V^+-\Delta us = V^- -\Delta vs,$$ or $$\Delta = \frac{V^+-V^-}{(u-v)S}.$$

Performing Taylor expansions around $V^+$ and $V^-$ yield \begin{align*}V^+ &= V(S, t) + \frac{\partial V}{\partial S}(uS-S)+\frac{\partial V}{\partial t} \delta t+\dots \\ &= V+\frac{\partial V}{\partial S} S\sigma \sqrt{\delta t} + \frac{\partial V}{\partial t} \delta t + \frac{1}{2} \cdot \frac{\partial^2 V}{\partial S^2} S^2\sigma^2 \delta t+\dots  \end{align*} and  \begin{align*} V^- &= V(S, t) + \frac{\partial V}{\partial S}(vS-S)+\frac{\partial V}{\partial t} \delta t+\dots  \\ &= V-\frac{\partial V}{\partial S} S\sigma \sqrt{\delta t} + \frac{\partial V}{\partial t} \delta t + \frac{1}{2} \cdot \frac{\partial^2 V}{\partial S^2} S^2\sigma^2 \delta t+\dots \end{align*}

Substituting $V^+$ and $V^-$ into the expression for $\Delta$ yields $\Delta=\frac{\partial V}{\partial S}$.

No-arbitrage requires the condition that $$V = \frac{1}{e^{-r\delta t}} (p' V^+ (1-p')V^-),$$ where $p' = \frac{1}{2} + \frac{r \sqrt{\delta t}}{2\sigma}$ as detailed previously. This simplifies to $$V = \frac{1}{1+r \delta t} (p' V^+ (1-p')V^-)$$ given a Taylor approximation, or $$V + rV\delta t = p' V^+ (1-p')V^-.$$

Consider the right-hand side. This is \begin{align*}p' V^+ (1-p')V^- &= p' (V+\frac{\partial V}{\partial S} S\sigma \sqrt{\delta t} + \dots) + (1-p') (V-\frac{\partial V}{\partial S} S\sigma \sqrt{\delta t} + \dots) \\ &= (2p'-1) \frac{\partial V}{\partial S} S\sigma \sqrt{\delta t} + V+ \frac{\partial V}{\partial t} \delta t + \frac{1}{2} \cdot \frac{\partial^2 V}{\partial S^2} S^2 \sigma^2 \delta t \\ &= \frac{r \sqrt{\delta t}}{\sigma} \frac{\partial V}{\partial S} S\sigma \sqrt{\delta t} + V+ \frac{\partial V}{\partial t} \delta t + \frac{1}{2} \cdot \frac{\partial^2 V}{\partial S^2} \\ &= rS \frac{\partial V}{\partial S} \delta t + V+ \frac{\partial V}{\partial t} \delta t + \frac{1}{2} \cdot \frac{\partial^2 V}{\partial S^2}  \end{align*}

The left-hand side is just $V+rV\delta t$ so equating the two and dividing by $\delta t$ yields us $$\frac{\partial V}{\partial t} + \frac{1}{2} \cdot \frac{\partial^2 V}{\partial S^2} S^2\sigma^2 + \frac{\partial V}{\partial S}rS -rV=0.$$

\section{Final remarks}

\chapter{EconZones}

The last section of each week is on a real-life application of the week's material. This comes in the form of economic or financial theories, numerical examples, or models. EconZones hopefully give some context to the mathematics used in economic or financial contexts, ensuring that the mathematics is not too theoretical, and admits real-life use cases. 

There will be at least one EconZone per non-review week. For some weeks there will be two.

\section{Term 1}

\subsection{Week 1: Utility functions}

A utility function is a measure of the satisfaction that an economic agent gains from consuming a certain quantity of a certain good or service. Denote it as $u(x)$. Let $u(x) = \frac{1}{3}x^3-x.$ Then, $$u'(x) = x^2-1,$$ meaning that $u'(x)$ is maximised when $x=1$ or $x=-1$. Since quantity of consumption is always positive, one concludes that the \emph{optimal quantity of consumption} is 1 unit. 

However, does this accurately depict an utility function? Note that as $x \to \infty$, $u'(x) \to \infty$, meaning that our marginal utility increases to infinity? This isn't an accurate depiction of economic behaviour.

What are some of the assumptions of utility functions? Firstly, $u'(x) >0$ since more of one good is always preferred to less of that good. Additionally, $u''(x) <0$ due to the \tbf{law of diminishing marginal utility}.

A more pertinent utility function would be $u(x) = \frac{1}{3}x^{0.8}+x^{0.5}$, which has derivative $$u'(x) = \frac{4}{15}x^{-0.2}+\frac{1}{2}x^{-0.5}$$ and second-order derivative $$u''(x)-\frac{4}{75}x^{-1.2}-\frac{1}{4}x^{-1.5}.$$ Note that for all values $x>0$, we have that $u'(x)>0$ and $u''(x)<0$. 

\subsection{Week 2: Agricultural production} 

A farmer produces corn, potatoes, and radishes in two separate plots of farmland. Denote the quantity of corn produced by $a_1$, the quantity of potatoes produced by $a_2$, and the quantity of radishes produced by $a_3$. Let the two separate plots of farmland be $b_1$ and $b_2$. Consider the quantities of each vegetable produced in each plot of farmland.

\begin{center}
    \begin{tabular}{|l|ccc|} 
        \hline
        & $a_1$ & $a_2$ & $a_3$ \\
        \hline
        $b_1$& 200 & 300 & 250 \\
        $b_2$& 150 & 350 & 300  \\

        \hline
    \end{tabular} 
\end{center}

Let the selling prices for $a_1, a_2, a_3$ be $P=\begin{pmatrix} a_1\\a_2\\a_3 \end{pmatrix} = \begin{pmatrix} 5\\4\\ 6\end{pmatrix}$ and the costs be $C=\begin{pmatrix} a_1\\a_2\\a_3 \end{pmatrix} = \begin{pmatrix} 4.5\\3\\ 5\end{pmatrix}$.

We can represent the quantities of crops as $$Q=\begin{pmatrix} 200&300&250 \\ 150&350&300\end{pmatrix}.$$ The total revenue is $$PQ = \begin{pmatrix} 200&300&250 \\ 150&350&300\end{pmatrix} \begin{pmatrix} 5\\4\\6 \end{pmatrix}= \begin{pmatrix} 200\cdot5+300\cdot 4+250\cdot 6 \\ 150\cdot5+350\cdot 4 + 300\cdot 6 \end{pmatrix} = \begin{pmatrix} 3700 \\ 3950 \end{pmatrix},$$ meaning that the total revenue for plot $b_1$ is 3700 and the total revenue for plot $b_2$ is 3950. 

The total cost is $$PC = \begin{pmatrix} 200&300&250 \\ 150&350&300\end{pmatrix} \begin{pmatrix} 4.5\\3\\5 \end{pmatrix}= \begin{pmatrix} 200\cdot4.5+300\cdot 3+250\cdot 5 \\ 150\cdot4.5+350\cdot 3 + 300\cdot 5 \end{pmatrix} = \begin{pmatrix} 3050 \\ 3225 \end{pmatrix},$$ thus meaning that the total profit for $\begin{pmatrix} b_1 \\ b_2 \end{pmatrix}$ is $$\begin{pmatrix} 3700 \\ 3950 \end{pmatrix}-\begin{pmatrix} 3050 \\ 3225 \end{pmatrix}= \begin{pmatrix} 650 \\ 725 \end{pmatrix}.$$ 

\subsection{Week 3: CES production function}

A special type of production function is the \tbf{constant elasticity of substitution (CES)} production function. It is $$Q = A(\delta K^\gamma + (1-\delta) L^\gamma)^{1/\gamma},$$ where $A, \gamma, \delta$ are constants such that $A>0$, $0<\delta<1$, and $\gamma<1, \gamma \neq 0$.

The marginal product of capital is \begin{align*}\frac{\partial Q}{\partial K} &= \frac{A}{\gamma} (\delta K^\gamma + (1-\delta) L^\gamma)^{\frac{1}{\gamma}-1} (\delta\gamma K^{\gamma-1})\\ &= A(\delta K^\gamma + (1-\delta) L^\gamma)^{\frac{1}{\gamma}(1-\gamma)}(\delta K^{\gamma-1})\\ &= \delta A^\gamma A^{1-\gamma} (\delta K^\gamma+(1-\delta) L^\gamma)^{\frac{1}{\gamma}(1-\gamma)}(K^{\gamma-1})\\ &= \delta A^\gamma (Q/K)^{1-\gamma} \end{align*}  and similarly, the marginal product of labour is $$\frac{\partial Q}{\partial L} = (1-\delta) A^\gamma (Q/L)^{1-\gamma}.$$

Thus, the marginal rate of technical substitution (MRTS) is \begin{align*} MRTS_{K, L} = \frac{MP_L}{MP_K} &= \frac{(1-\delta) A^\gamma (Q/L)^{1-\gamma}}{\delta A^\gamma (Q/K)^{1-\gamma}} \\ &= \frac{1-\delta}{\delta} \left(\frac{K}{L}\right)^{1-\gamma}\end{align*}

\defn{Elasticity of substitution}{
	Let $x_1$ and $x_2$ be two goods. The elasticity of substitution of $x_1$ and $x_2$ is $$ \varepsilon_{x_1, x_2}=\frac{d \ln(\frac{x_1}{x_2})}{d \ln (MRTS_{K, L})} .$$ 
}

Let's calculate the elasticity of substitution of the CES production function. This is $$\frac{d \ln(\frac{K}{L})}{d \ln (MRTS_{K, L})} =\frac{d \ln(\frac{K}{L})}{d \ln \alpha}= \frac{d\frac{K}{L}}{\frac{K}{L}}\frac{\alpha}{d\alpha} = \frac{d\frac{K}{L}}{d\alpha}\frac{\alpha}{\frac{K}{L}}$$

Let's try to write $\frac{K}{L}$ in terms of $\alpha$. Note that $\alpha = \frac{1-\delta}{\delta} \left(\frac{K}{L}\right)^{1-\gamma}$, meaning that $$\frac{K}{L} = \left( \frac{\alpha\delta}{1-\delta} \right)^{\frac{1}{1-\gamma}}=\left( \frac{\delta}{1-\delta} \right)^{\frac{1}{1-\gamma}}\alpha^{\frac{1}{1-\gamma}}.$$

Therefore, \begin{align*} \frac{d\frac{K}{L}}{d\alpha}\frac{\alpha}{\frac{K}{L}} &= \frac{1}{1-\gamma} \left( \frac{\delta}{1-\delta} \right)^{\frac{1}{1-\gamma}}\alpha^{\frac{\gamma}{1-\gamma}} \cdot \alpha \cdot \left( \frac{\delta}{1-\delta} \right)^{-\frac{1}{1-\gamma}} \alpha^{-\frac{1}{1-\gamma}}\\ &= \frac{1}{1-\gamma} \cdot \alpha^{\frac{\gamma}{1-\gamma}+1-\frac{1}{1-\gamma}} \\ &= \frac{1}{1-\gamma} \cdot \alpha^0 \\ &= \frac{1}{1-\gamma},\end{align*} which is constant. Now you know why $Q$ is called the constant elasticity of substitution production function.

\subsection{Week 4: National income model}

The national income model is a way to measure a country's aggregate economic activity within a given time period. We assume that investment and government spending are fixed at levels $I_0$ and $G_0$ respectively. Let $Y, C,$ and $T$ be national income, consumption spending, and taxation amount respectively. 

The model is governed by the equations \begin{align*} Y &= C+I_0+G_0 \\ C&= \alpha+\beta(Y-T) \\ T&=\delta+ tY\end{align*}

Rearrange the equations to get \begin{align*} Y-C &= I_0+G_0 \\ -\beta Y + C+\beta T&= \alpha \\ -tY+T&=\delta.\end{align*}

This means that $$\begin{pmatrix} 1 & -1 & 0 \\ -\beta & 1 & \beta \\ -t & 0 & 1\end{pmatrix} \begin{pmatrix} Y\\C\\T \end{pmatrix} = \begin{pmatrix} I_0+G_0 \\ \alpha \\ \delta \end{pmatrix}.$$ 

Note $\begin{pmatrix} 1 & -1 & 0 \\ -\beta & 1 & \beta \\ -t & 0 & 1\end{pmatrix}^{-1}  = \begin{pmatrix} 0 & \frac{1}{\beta t-\beta} & \frac{1}{1-t} \\ -1 & \frac{1}{\beta t-\beta} & \frac{1}{1-t} \\ 0 & \frac{1}{\beta t-\beta} & \frac{1}{1-t}\end{pmatrix}$, so  $$\begin{pmatrix} Y\\C\\T \end{pmatrix} = \begin{pmatrix} 0 & \frac{1}{\beta t-\beta} & \frac{1}{1-t} \\ -1 & \frac{1}{\beta t-\beta} & \frac{1}{1-t} \\ 0 & \frac{1}{\beta t-\beta} & \frac{1}{1-t}\end{pmatrix} \begin{pmatrix} I_0+G_0 \\ \alpha \\ \delta \end{pmatrix}.$$

\subsection{Week 5: Comparative statics}

Let's look at an economic application of the Jacobian matrix - comparative statics. Comparative statics compares two different economic outcomes, for example market equilibriums, before and after some exogenous event occurred. Consider a simple goods market with linear supply and demand schedules. Denote the price of a good by $p$ and income by $y$, and let quantity demanded and quantity supplied be $q_d$ and $q_s$ respectively. Then we can write $$q_d = \alpha - \beta p + \gamma y$$ and $$q_s = \delta + \varep p.$$ The market clears when $q_d=q_s$, so $$q=\alpha - \beta p + \gamma y=\delta + \varep p.$$

Solving for $p$, we have that $$p=\frac{\alpha-\delta+\gamma y}{\delta +\varep},$$ and thus $$q = \delta+\varep p = \frac{\beta\delta+\varep(\alpha+\gamma y)}{\beta+\varep}.$$ 

In general, we can write $$q_d=D(p, y)\ \text{and}\ q_s=S(p).$$

Comparative statics, for example, deals with what occurs when there is a change in price $\Delta p$ that causes price to change from $p_0$ to $p_1$. Note that $$\Delta p = p_1-p_0 = \frac{\gamma}{\beta+\varep}(y_1-y_0).$$

Now, consider when we have $n$ goods and $m$ consumers. Then, consider the \tbf{excess supply function} in one variable $E(p, y) = S(p)-D(p, y).$ Now let's consider it in vector form: $$\vec{E}(\vec{p}, \vec{y}) = \vec{S}(p)-\vec{D}(\vec{p}, \vec{y}).$$ When the market clears, we aim to have $$\vec{E}(\vec{p}, \vec{y}) =0,$$ which means that $$\nabla \vec{E} +J\cdot \nabla \vec{p^{*}}=\mbf{0},$$ where $\vec{p^{*}}$ denotes the vector of equilibrium prices. In other words, $$\nabla \vec{p^{*}} =-J^{-1}\cdot \nabla \vec{E},$$ where the Jacobian $$J= \begin{pmatrix} \frac{\partial E_1}{\partial p_1^{*}} & \dots & \frac{\partial E_1}{\partial p_n^{*}} \\ \vdots & \ddots & \vdots \\ \frac{\partial E_n}{\partial p_1^{*}} & \dots & \frac{\partial E_n}{\partial p_n^{*}} \end{pmatrix}.$$

\subsection{Week 6: Leontief matrix}

Consider a producer which divides their work up to three sectors, in which each sector produces an unique product. Denote the three products by $p_1, p_2$, and $p_3$ respectively. In order to produce said product, each sector requires inputs that comes from all three sectors. Suppose that:

\begin{itemize}
	\item To produce one unit of Product 1, 0.4 units of Product 1, 0.15 units of Product 2, and 0.1 units of Product 3 are required.
	\item To produce one unit of Product 2, 0.3 units of Product 1, 0.2 units of Product 2, and 0.25 units of Product 3 are required.
	\item To produce one unit of Product 3, 0.25 units of Product 1, 0.3 units of Product 2, and 0.15 units of Product 3 are required.
\end{itemize}  

Therefore, \begin{align*} p_1&=0.4p_1+0.15p_2+0.1p_3 \\ p_2&=0.3p_1+0.2p_2+0.25p_3 \\ p_3&=0.25p_1+0.3p_2+0.15p_3.\end{align*}\

Suppose that there is external demand of quantity $\begin{pmatrix} d_1 \\ d_2 \\ d_3 \end{pmatrix}$. Our matrix system is therefore $$\begin{pmatrix} p_1 \\ p_2 \\ p_3 \end{pmatrix}= \begin{pmatrix} 0.4&0.15&0.1 \\ 0.3&0.2&0.25 \\ 0.25&0.3&0.15 \end{pmatrix} \begin{pmatrix} p_1 \\ p_2 \\ p_3 \end{pmatrix}+\begin{pmatrix} d_1 \\ d_2 \\ d_3 \end{pmatrix}.$$

Let $M=\begin{pmatrix} 0.4&0.15&0.1 \\ 0.3&0.2&0.25 \\ 0.25&0.3&0.15 \end{pmatrix}$. Then, $$\vec{p} =M\vec{p}+\vec{d},$$ meaning that $\vec{p}(\bbI-M)=\vec{d}$, or $$\vec{p} = (\bbI-M)^{-1} \vec{d}.$$

We can, of course, use matrix inversion to evaluate the value of the matrix. Now consider when $M$ is non-invertible. Observe that $$(\bbI-M)^{-1} = \frac{\bbI}{\bbI-M} = \bbI+M + M^2+M^3+\dots.$$We can represent $M = SDS^{-1}$ (performing eigendecomposition), and compute the values of this infinite sum.

Admittedly, there are not many \emph{pure} economic applications of eigenvalues. They come up more often in studies of difference and differential equations.

\subsection{Week 7: Malthusian economics}

The famous Malthusian model of population growth details the number of individuals in a population at time $t$. This model can be represented by the difference equation $$N(t+1)= N(t) + b N(t) - d N(t),$$ where $b$ and $d$ represent the number of births and deaths in the population respectively. 

Let's try to construct the analogous differential equation. Instead of using time $t+1$, we can use time $t+\Delta t$ instead, where $\Delta t$ is an infinitesimally small timeframe. By this $N(t+\Delta t)= N(t) + b\Delta t N(t) - d\Delta t N(t) = (b-d) \Delta t N(t)$. Thus, $$\frac{N(t+\Delta t) - N(t)}{\Delta t} = (b-d) N(t).$$ In the limit $\Delta t \to 0$, we have that $$\frac{dN}{dt} = (b-d)N(t),$$ which yields the solution $$N(t) = N(0) e^{b-d}t.$$

\subsection{Week 8: Utility functions and budget constraints}

In Week 1's EconZone, we worked with utility functions of one variable. Now, we work with utility functions of two variables, subject to a budget constraint. We are confined to the number of products $x$ and $y$ we can consume by our budget.

Let our utility function be $u(x, y)= 2x-y^{-1}$ and let the prices of product $x$ and $y$ be $\$8$ and $\$9$ respectively. Assume that our budget is $\$14$. Thus, our budget set is $8x+9y=14$, or $g(x, y) = 8x+9y-14=0$. Our Lagrangian is $$\mcL(x, y, \lambda) = 2x-y^{-1}+\lambda(14-8x-9y)=0.$$ The first-order conditions for the Lagrangian are $$\frac{\partial \mcL}{\partial x} = 2-8\lambda=0, \frac{\partial \mcL}{\partial y} = y^{-2}-9\lambda=0.$$ 

These first-order conditions yield $$\lambda=\frac{1}{4}, \lambda = \frac{1}{9y^2},$$ which yield $y=\frac{2}{3}, y=-\frac{2}{3}.$ (we obviously disallow the negative solution)

Substitution back into the budget set yields $14-8x-6=0$, meaning that $x=1$. The utility-maximising quantities of $x$ and $y$ are $(x, y) = (1, \frac{2}{3}).$ 

\subsection{Week 9: Agents with opinions} 

We defer to Frank's lecture notes for this week's EconZone as most applications of entropy in economics deal with \tbf{decision-making under uncertainty}, which is what Frank's lecture notes cover. 

Let an agent have an opinion $\sigma$ about some policy. Suppose they are unsure of their opinion. They know, with probability $P(\sigma)$, that their opinion lies in some range infinitesimally small range $(\sigma, \sigma+d\sigma)$. Denote the utility of their opinion by $u(\sigma)$.

The entropy function is therefore $$S(P) = -\int_{-\infty}^\infty P(\sigma) \log(P(\sigma)) d\sigma.$$ The expected value is $$E(u, P) = \int_{-\infty}^\infty P(\sigma)u(\sigma) d\sigma.$$ 

Suppose that there is a per-unit cost of $c$ for the uncertainty of information. 

Therefore, the agent tries to maximise $$\mcL(u, P, c) = E(u, P) -(-cS(P))=E(u, P)+cS(P)$$ subject to the condition $$\int_{-\infty}^\infty P(\sigma) d\sigma=1.$$

We can perform constrained maximisation by finding the first-order conditions of $\mcL(u, P, c)$ and using Lagrange multipliers.

\section{Term 2}

\subsection{Week 11: Short-run and long-run cost curves}

An interesting topic in the theory of the firm pertains to the relationship between the short-run and long-run cost curves. In microeconomics, the short-run refers to a production period where there is at least one fixed factor of production. The long-run refers to production wherein all factors of production are variable. 

\defn{Short-run and long-run cost curves}{Consider the costs of producing a certain amount of output $Q$. This depends on the quantity of a fixed factor such as capital $K$. Thus, we can write $C = f(K, Q)$. 

For any fixed $K$, the plot of $C=f(K, Q)$ against $Q$ gives us the \tbf{short-run cost curve} corresponding to a certain level of $K$.

The \tbf{long-run cost curve}, meanwhile, corresponds to the relationship between $f(K, Q)$ and $Q$ obtained by choosing the cost-minimising level of $K$ at every output level $Q$. Explicitly, the equation of the long-run cost curve is $$g(Q) = \min\limits_{K} f(K, Q).$$

In short, the short-run cost curve is $f(K, Q)$, while the long-run cost curve is $g(Q)=\min\limits_{K} f(K, Q)$. 
}

The diagram above demonstrates the relationship between short-run and long-run cost curves, where the many short-run cost curves are "enveloped" within the long-run cost curves. Perhaps this gives a visual aid as to why the envelope theorem is termed the envelope theorem; the \tbf{long-run cost curve is the envelope of the family of short-run cost curves.}

By the envelope theorem, we have that $$g'(Q)=\frac{\partial f(K, Q)}{\partial Q}.$$ This means that each of the short-run cost curves meet the long-run cost curve at the point where the curves are tangential to each other. This tangential relationship at the point of intersection creates the effect of the long-run cost curve enveloping the short-run cost curves.

% example from Pemberton? 18.2.1

\ex{Long and short-run cost curves}{
	This is an exercise from Pemberton and Rau's \emph{Mathematics for Economists: An Introductory Textbook}.
	
	Suppose a firm's short-run cost curve, fixing capital $K$, is $$C(K, Q) = bK^2(1+\left(\frac{Q}{K}\right)^4).$$ The equation of the long-run cost curve is found by minimising $C(K, Q)$ with respect to $K$. Setting $\frac{\partial C}{\partial K}=0$, we have that $$\frac{2b(K^4-Q^4)}{K^3}=0,$$ or $K=Q$. In this case, the long-run cost curve is $$C(Q)=2bQ^2.$$ 
	
	We can verify the envelope theorem in this case. Using the implicit function theorem, the slope of the short-run cost curve is $4bK^{-2}Q^3$, which equals $4bQ$ at the optimum $K=Q$. Differentiating the long-run cost curve yields $C'(Q) = 4bQ$, which is the same as the slope of the short-run cost curve.
}

\subsection{Week 12: Inequality constraints}

Instead of using budget constraints and utility functions, we switch to the producer side and work with isocosts and isoquants. Consider when one maximises the production quantity $xy$ subject to the cost constraint $x^2+(y+2)^2 \leq 10.$ Let's first work with this constraint and not impose the additional constraints that $x, y\geq0$. 

The Lagrangian function is now $$\mcL(x, y, \lambda) = xy - \lambda (x^2+(y+2)^2-10)$$ and therefore the first-order conditions are \begin{align*} \frac{\partial \mcL}{\partial x} = y -2x\lambda&=0 \\ \frac{\partial \mcL}{\partial y} = x-2\lambda(y+2)&=0 \\ \lambda (x^2+(y+2)^2-10)&=0 \\ \lambda &\geq0 \end{align*}

The first two conditions yield $y=2\lambda x$ and $x=2\lambda(y+2)$. First consider when the constraint is inactive, meaning $\lambda=0$. In this case, $y=0, x=0$, which is evidently not a maxima. Therefore we have to consider when $\lambda \neq 0$. In this case, the solutions for $\lambda$ are $$\lambda = \pm \sqrt{\frac{3}{10}-\frac{\sqrt{21}}{20}}, \pm \sqrt{\frac{3}{10}+\frac{\sqrt{21}}{20}}.$$

Substituting back into $x$ and $y$, we find that the value that maximises $xy$ is when $x=-2.606, y=-3.791$, yielding $xy=9.875$. I used numerical approximations here because the exact form of $x$ and $y$ is really tedious to write out.

If the condition that $x\geq0, y\geq0$ was given, the value that maximises $xy$ occurs when $x=1.486, y=0.791$, which yields $xy=1.175$. 

Additionally, we also could have constructed a Lagrangian function that incorporates both positivity constraints. The modified Lagrangian function is $$\mcL(x, y, \lambda, \mu_1, \mu_2) = xy - \lambda(x^2+(y+2)^2-10) - \mu_1 x - \mu_2y.$$

The Kuhn-Tucker conditions are now \begin{align*} \frac{\partial \mcL}{\partial x} = y -2x\lambda-\mu_1&=0 \\ \frac{\partial \mcL}{\partial y} = x-2\lambda(y+2)-\mu_2&=0 \\ \lambda (x^2+(y+2)^2-10)&=0 \\ \mu_1x&=0 \\ \mu_2y&=0 \\ \lambda &\geq0 \end{align*} Though tedious, one can realise that an optimum requires $\mu_1, \mu_2=0$ to allow $x, y\neq 0$. Our optimum $x=1.486, y=0.791$, which yields $xy=1.175$ is then obtained.

\subsection{Week 13: Cobweb model of price fluctuations}

In the previous section, we explored the difference map $$\Delta=\begin{pmatrix} \hdots & \vdots & \ddots & \ddots & \ddots \\ \ddots & -1 & 1 & \ddots & \ddots \\ \ddots & \ddots & -1 & 1 & \ddots \\ \ddots & \ddots & \ddots & -1 & \vdots \\ \ddots&\ddots&\ddots&\ddots&\hdots \end{pmatrix}$$

as a representation of the difference between the terms $x_{t+1}$ and $x_t$ of a difference equation. 

We will apply this map to the cobweb model, a model that describes why there are fluctuations in prices in various markets. It is undergirded by a time lag between supply and demand, causing a cycle of rising and falling prices. Consider an agricultural market.

Assume that the market for corn clears. If there is a good harvest of corn, then the supply of corn will increase. Thus, the equilibrium price will decrease and the equilibrium quantity will increase. However, the fall in corn prices will cause some farmers to go out of business, or to produce something else. Therefore, farmers will gradually decrease their supply of corn, and the price will rise again. When the price of corn rises, farmers will gradually increase their supply of corn as the product is more profitable, creating a cyclical relationship of prices fluctuating between low and high prices.

The cobweb model is described in the diagram below.

\includegraphics[scale=0.5]{cobweb.png}

Let's try to create a set of difference equations that describe the cyclical relationship between supply and demand. Increases or decreases in quantity supplied at time $t$ are determined as functions of price at the time $t-1$. Therefore, we have that $D_t = f(P_t)$ and $S_t = f(P_{t-1})$.

Assume that the supply and demand functions are linear. Then, we have that \begin{align*}D_t &= a-bP_t \\ S_t &= -c+dP_{t-1} \end{align*} where $a, b, -c, d>0$ are constants. Evidently, we have that $D_t=S_t$ at equilibrium.

The difference equation describing equilibrium satisfies \begin{align*}a-bP_t &= -c+dP_{t-1} \\ bP_t+dP_{t-1} &= a+c \\ P_t + \frac{d}{b}P_{t-1} &= \frac{a+c}{b} \end{align*} or \begin{equation}P_t - P_{t-1} = \left(-\frac{d}{b}-1\right) P_{t-1} + \left(\frac{a+c}{b}\right). \end{equation}

The difference equation satisfies \begin{equation}D\cdot \vec{P} = \left(-\frac{d}{b}-1\right)\bbI \cdot \vec{P} + \left(\frac{a+c}{b}\right)\vec{\bbI},\end{equation} where $\Delta$ is the difference map described above, $\vec{\bbI}$ is the identity vector $\begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}$, $\bbI$ is the identity matrix, and $\vec{P}$ is the vector containing all the values of $P_t$.

(n.b.) Using methods of solving first-order difference equations as detailed in Week 7, the solution to equation 13.3 is $$P_t = \left( P_0 - \frac{a+c}{b+d}\right)\left(-\frac{d}{b}\right)^t+\left(\frac{a+c}{b+d}\right)$$ where $P_0$ is the initial price.

\subsection{Week 14: Multiplier-accelerator model} 

Let $\{C_t\}, \{I_t\}$, and $\{Y_t\}$ be three separate time series representing consumption, investment, and income in Hicks' multiplier-accelerator model. In the model, we have that consumption in the current period is the product of income in the previous period and the marginal propensity to consume. In short, we have that $$C_t=mY_{t-1}.$$ Also, $$C_t = (1-s)Y_{t-1},$$ where $s$ is the marginal propensity to save.

Meanwhile, investment in the current period $I_t$ satisfies the relationship $$I_t=a(C_t-C_{t-1}),$$ where $a$ is the accelerator. This yields \begin{align*}I_t&=n(aY_{t-1}-aY_{t-2}) \\ &= na(Y_{t-1}-Y_{t-2}).\end{align*} Putting it all together, letting $Y_t=C_t+I_t$ in our closed economy, we have that \begin{align*}Y_t &= \alpha Y_{t-1}+\beta(Y_{t-1}-Y_{t-2})\\ &= (\alpha+\beta)Y_{t-1}-\beta Y_{t-2}.\end{align*} and \begin{align*}Y_t &= (1-\gamma)Y_{t-1}+\beta(Y_{t-1}-Y_{t-2}) \\ &= (1-\gamma+\beta)Y_{t-1}-\beta Y_{t-2}\end{align*} which are two second-order difference equations.

Let's work with the first differential equation $Y_t=(\alpha+\beta)Y_{t-1}-\beta Y_{t-2}$. The second differential equation is very similar so we will omit the calculations. The first differential equation isn't really in the form we want. Note that in Section 13.1, we represented the difference equation in terms of the differences between successive terms, or $(x_{t+2}-x_{t+1})-(x_{t+1}-x_t)=\Delta x_{t+1}-\Delta x_t$. Observe that \begin{align*} Y_{t+2}&=(\alpha+\beta)Y_{t+1}-\beta Y_t \\ (Y_{t+2}-Y_{t+1})-(Y_{t+1}-Y_t)&=(\alpha+\beta-2)Y_{t+1}-(\beta-1) Y_t. \end{align*}

Wait, wasn't the right-hand side supposed to be a function of $Y_t$, and not a function of $Y_{t+1}$? To correct for this, note that $$(Y_{t+2}-Y_{t+1})-(Y_{t+1}-Y_t)=(\alpha+\beta-2)Y_{t+1}-(\alpha+\beta-2)Y_t-(\beta-1) Y_t+(\alpha+\beta-2)Y_t$$ and thus $$(Y_{t+2}-Y_{t+1})-(Y_{t+1}-Y_t) - ((\alpha+\beta-2)Y_{t+1}-(\alpha+\beta-2)Y_t)=-(\beta-1) Y_t+(\alpha+\beta-2)Y_t. $$ 

This means that $$(\Delta^2_Y - (\alpha+\beta-2) \Delta_Y)\cdot \vec{X} = (\alpha-1)Y_t \cdot \vec{X},$$ where $\Delta^2_Y=\begin{pmatrix}\vdots&\hdots&\vdots&\ddots&\ddots \\ \ddots&1&-2&1&\ddots \\ \ddots&\ddots&1&-2&\vdots \\ \ddots&\ddots&\ddots&1&\hdots \\ \ddots&\ddots&\ddots&\ddots&\vdots \end{pmatrix}$ and $\Delta_Y=\begin{pmatrix}\hdots&\vdots&\ddots&\ddots&\ddots \\\ddots&-1&1&\ddots&\ddots \\ \ddots&\ddots&-1&1&\ddots \\ \ddots&\ddots&\ddots&-1&\vdots \\ \ddots&\ddots&\ddots&\ddots&\hdots \end{pmatrix}$ are our familiar difference maps for first-order and second-order differences.

Notice that the matrix corresponding to $\alpha-1$ is merely $(\alpha-1)\bbI$ as the coefficient is not dependent on time. But note that just like the difference map matrices, this $\bbI$ is infinitely large.

\subsection{Week 15: Exchange rate dynamics}

A paper of Dornbusch outlines a system of two differential equations that outline the relationship between log-price levels and log-exchange rates. (We use log-variables as it is a well-known econometric fact that log-variables can be approximated by percentage changes for small values of said variables)

Consider a country (named Country A) with interest rate $r$. Assume that the interest rate in the rest of the world is homogeneous, and is determined $r'$. Also assume that the rest of the world has a common currency distinct from Country A, and that the average price level for the rest of the world is constant. 

We acknowledge that the difference between $r$ and $r'$ reflect changes in the log-exchange rate $q$. Specifically, $$r=r'+\frac{dq}{dt}.$$ Now let the price levels in Country A and the rest of the world be $P$ and $\bar{P}$ respectively, and define $p =\log(P)$ and $\bar{p} = \log(\bar{P})$. 

After a disturbance, the price level $P$ tends towards $Q\bar{P}$. This means that $\log(P)$ tends towards $\log(Q\bar{P})$, or $p$ tends towards $q+\bar{p}.$

Therefore, $$\frac{dp}{dt} = \gamma(q+\bar{p}-p)$$ where $\gamma$ represents the speed of adjustment.

Let $M$ be the total stock of money in Country A, and let $$\frac{M}{P}=ae^{-\beta r}.$$ Taking natural logarithms and letting $m=\log(M), \alpha = \log(a)$, we have that $$m-p = \alpha-\beta \left(\bar{r}+\frac{dq}{dt}\right).$$ Thus, we have the set of differential equations \begin{align*} \frac{dp}{dt} &= \gamma (-p+q+\bar{p}) \\ \frac{dq}{dt} &= \frac{1}{\beta}(p-m+\alpha-\beta \bar{r}) \end{align*}

This can be simplified to produce the matrix system $$\frac{d}{dt} \begin{pmatrix} p\\q \end{pmatrix} = \begin{pmatrix} -\gamma & \gamma \\ \frac{1}{\beta} & 0 \end{pmatrix} \cdot \begin{pmatrix} p\\q \end{pmatrix} + \begin{pmatrix} \gamma \bar{p} \\ \frac{1}{\beta}(\alpha -m-\beta r) \end{pmatrix}$$

Let $\lambda_1, \lambda_2$ be the eigenvalues of $\begin{pmatrix} -\gamma & \gamma \\ \frac{1}{\beta} & 0 \end{pmatrix}$, and let the corresponding eigenvectors be $\vec{v}_1, \vec{v}_2$. Then, the solution to our system is $$\begin{pmatrix} p\\q \end{pmatrix} = c_1 e^{\lambda_1 t} \vec{v}_1 + c_2 e^{\lambda_2 t} \vec{v}_2 + \begin{pmatrix} m-\alpha+\beta\bar{r} \\ m-\alpha+\beta\bar{r}-\bar{p} \end{pmatrix}.$$

\subsection{Week 16: Markov chains and stochastic behaviour}

A Markov chain is a series of possible events in which the probability of one event happening depends on the state in the previous event. In short, what happens next is induced by the state of affairs now. For example, inflation next year has non-zero covariance with inflation this year, and is thus somewhat dictated by this year's inflation rate.

Suppose that the incomes of a group of people could be characterised by lower-class, middle-class, or upper-class. Also suppose that after one period of unspecified length, the following holds:

\begin{enumerate}
	\item The probability of a person from the lower-class remaining poor is 0.9, while the probability of that person reaching the middle-class is 0.1. The probability that they reach the upper-class is zero.
	\item The probability of a middle-class individual remaining in the middle-class is 0.5, while the probabilities of  them reaching the upper-class and the lower-class are 0.1 and 0.4 respectively.
	\item The probability of an upper-class individual remaining rich is 0.8, with the probability of retreating to the middle and lower-class respectively being 0.1 and 0.1 respectively.
\end{enumerate}

\defn{Transition matrix}{
	The transition matrix describes how likely it is to move from one state to another. Let $P$ be the transition matrix. The probability that a person begins in the lower class and stays in it is $M_{11}=0.9$. 
	
	In the example above, it is $$P=\begin{pmatrix} 0.9 & 0.1 & 0 \\ 0.4 & 0.5 & 0.1 \\ 0.1 & 0.1 & 0.8\end{pmatrix}.$$
	
	In short, the transition matrix of a Markov chain is defined as $$P(X_{t+1}=i \mid X_t = j) = P_{ij},$$ where $X_t$ is the state of an arbitrary person in the group at time $t$. 
}

Since probabilities sum up to one and each row dictates the probability that a person will enter the lower, middle, or upper-class, the \tbf{sum of each row in the transition matrix is one}, or $\sum_j P_{ij}=1$.

Considering the right-eigenvalue problem for this transition matrix, we have that $$P \vec{x} = \lambda \vec{x}$$ yields the solution $\lambda=1$, and therefore 1 is an eigenvalue with right-eigenvector $\vec{x}= \begin{pmatrix} 1\\ \vdots \\ 1 \end{pmatrix}$.

In fact, the eigenvalue $\lambda^*=1$ is known as the \emph{leading eigenvalue}, where $|\lambda| \leq |\lambda^*|$ for all other values of this matrix. Note that $\lambda^*=1$ must also be an eigenvalue of the transpose $P^T$. 

If we take the transpose $\vec{x}^T$ of the eigenvector of $P^T$, we obtain a left eigenvector of $P$ that satisfies $\vec{x} P=\vec{x}$. To show this, simply note that $\vec{x}^T P^T = (P\vec{x})^T$. In conclusion, transition probability matrices have an eigenvalue $\lambda=1$; all eigenvalues of the transition matrix $\leq1.$ Furthermore, they have at least one left eigenvector corresponding to the eigenvalue $\lambda=1$.

It is known by the \emph{Perron-Frobenius theorem} that such an eigenvector $\vec{x} = \{x_i \}$ will invariably be a probability distribution, where $\sum_i x_i = 1.$ We won't discuss this theorem in detail here; for completeness, this is the Perron-Frobenius theorem. 

\thrm{Perron-Frobenius}{
	Let $M$ be a square, non-negative matrix. Let the eigenvalue of $M$ with the largest absolute value be denoted the \tbf{leading eigenvalue}, or the \tbf{dominant eigenvalue}. Then, the dominant eigenvalue $\lambda$ is real-valued and non-negative. 
	
	Additionally, the eigenvalue problem for $M$, namely $$M \vec{v} = \lambda \vec{v}$$ yields a $\vec{v}$ that is strictly positive. 
} 

Furthermore, note that $$\vec{x}_{t+1} = P \vec{x}_t,$$ since the state at time $t+1$ corresponds to the mapping of the state at time $t$ under the transition matrix. Therefore, the left-eigenvector satisfies \begin{align*} \vec{x}_{t+1} &= P \vec{x}_t \\ &= 1\vec{x}_t \\ &= \vec{x}_t .\end{align*}

This is similar to a stationary state in a difference equation, where $\vec{x}_t = \vec{x}_s$ for all $t\geq s$. We call this left-eigenvector that satisfies this condition the stationary distribution of the Markov chain, where $\vec{x}_t = \vec{x}_{t+1} = \vec{x}_{t+n}$ for all $n\geq0$. 

\subsection{Week 17: Transforming constraints into polar coordinates}

Consider the production function $$f(x_1, x_2) = \ln(x_1) + \ln(2x_2)$$ subject to the constraint $x_1^2+x_2^2-4=R^2$. If we introduce a polar coordinate system centered around $x_1, x_2=0$, this suggests that $f(x_1,x_2)$ becomes $$f(R, \theta)=\ln(\sqrt{R^2+4} \cos\theta)+ \ln(2\sqrt{R^2+4}\sin\theta).$$ Note that we do not have to include any Lagrangian multiplier as the constraint is already "built into" the objective function. This is an easy two-variable optimisation problem; finding $\frac{df}{dR}$ and $\frac{df}{d\theta}$ is left as an exercise.

\subsection{Week 18: Time series decomposition with Fourier series}

Consider a time series of length $n$ which goes from $y_0$ to $y_{n-1}$. If the time series demonstrates periodic behaviour, we can try to fit said time-series with a sinusoidal model. For example, $$y_t = \beta_0 + \beta_1 \cos (\alpha\pi t)+\varepsilon_t,$$ where $\alpha$ depends on the periodicity of the data. 

If the data showed signs of recurring every year, then we could model it as $$y_t = \beta_0 + \beta_1 \cos \left(\frac{2}{365}\pi t\right)+\varepsilon_t.$$

If there was a general data trend that recurred every year and a sub-trend that also recurred twice a year, we could use the model $$y_t = \beta_0 + \beta_1 \cos \left(\frac{1}{365}\cdot 2\pi t\right)+ \beta_2 \cos \left(\frac{2}{365}\cdot 2\pi t\right)+\varepsilon_t.$$

The Fourier series of a rather complex signal decomposes it into its constituent sinusoidal elements. Formally, we include terms containing sines, and terms containing cosines. This comes in the form of a Fourier series; namely, $$y_t = a_0 + \sum\limits_{k=1}^{n/2} a_k \cos\left(\frac{2k\pi t}{n}\right)+ b_k \sin \left(\frac{2k\pi t}{n}\right).$$

Note that we can remove the term containing $b_{n/2}$ because $\sin (\frac{2(n/2)\pi t}{n})=\sin(\pi t)=0.$ So, we can rectify this representation by writing $$y_t = a_0 + \left(\sum\limits_{k=1}^{n/2-1} a_k \cos\left(\frac{2k\pi t}{n}\right)+ b_k \sin \left(\frac{2k\pi t}{n}\right)\right) + a_{n/2} \cos(\pi t).$$

We finish off by attempting to find closed-form representations for estimators of the regression parameters $a_k$ and $b_k$. Let's attempt finding estimators for $a_k$ first. To do this, we minimise residuals by observing that $$\varepsilon_t= \left(y_t - a_k \cos\left(\frac{2k\pi t}{n}\right)\right)^2,$$ and noting that $$\sum\limits_{t=1}^n \varepsilon_t = \left(y_t - a_k \cos\left(\frac{2k\pi t}{n}\right)\right)^2.$$ Setting $\frac{d}{da_k} \varepsilon_t=0$ yields the expression (note that we write estimators as $\hat{a_k}$ instead of $a_k$ $$ \sum\limits_{t=1}^n 2\left(y_t - \hat{a}_k \cos\left(\frac{2k\pi t}{n}\right)\right)\left(-\cos \left(\frac{2k\pi t}{n}\right)\right)=0,$$ or 

\begin{align*} \sum\limits_{t=1}^n y_t \cos\left(\frac{2k\pi t}{n}\right) &= \hat{a}_k \sum\limits_{t=1}^n \cos^2\left(\frac{2k\pi t}{n}\right)  \\ \hat{a}_k &= \frac{\sum\limits_{t=1}^n y_t \cos\left(\frac{2k\pi t}{n}\right)}{\sum\limits_{t=1}^n \cos^2\left(\frac{2k\pi t}{n}\right)} \\ &= \frac{2}{n} \sum\limits_{t=1}^n y_t \cos\left(\frac{2k\pi t}{n}\right).\end{align*}

Similarly, the estimator $\hat{b}_k$ takes the form $$\hat{b}_k = \frac{2}{n} \sum\limits_{t=1}^n y_t \sin\left(\frac{2k\pi t}{n}\right).$$

\subsection{Week 19: A problem in resource economics}

Consider a firm that has a stock $S(t)$ of resources in any given time $t$. Every period they extract said stock at a rate $r(t)$. Therefore, we have the differential equation $$r(t) = -\frac{d}{dt} S(t).$$

We assume that there are $T+1$ time periods: from $t=0$ to $t=T$. We also assume that the firm faces a linear demand curve.  Therefore, the firm charges a price $$p(t) = p_0-ar(t).$$ The Lagrangian for this problem is thus $$\mcL(S(t), r(t), \lambda(t)) = ( p_0-ar(t)) r(t) - \lambda(t) (r(t)+\frac{d}{dt} S(t)).$$ The Euler equation $$\frac{\partial \mcL}{\partial S(t)} - \frac{d}{dt} \frac{\partial \mcL}{\partial\frac{dS}{dt}}$$ becomes \begin{align*} 0-\frac{d}{dt}(-p_0-2a\frac{dS}{dt})&=0 \\ 2a \frac{d^2S}{dt^2} =0 \end{align*}

If we include the discount factor, then the new Lagrangian is $$\mcL(S(t), r(t), \lambda(t)) = e^{-rt}( p_0-ar(t)) r(t) - \lambda(t) (r(t)+\frac{d}{dt} S(t)).$$

In this case, the Euler equation satisfies $$rp_0 + 2ar \frac{dS}{dt}-2a \frac{d^2S}{dt^2}=0$$ which, given the boundary conditions $S(0)=S$ and $S(T)=0$, yields the particularly tedious solution $$S(t) = \frac{-2a(e^{rt}-e^{rT})S + (t-te^{rt}+(e^{rt}-1)T)p_0}{2a(e^{rT}-1)}.$$

\end{document}